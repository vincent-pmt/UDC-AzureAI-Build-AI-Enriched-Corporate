Query - Course Indexes - Request URL
https://search-udc-dev-eastus-01.search.windows.net/indexes/courses-index/docs?api-version=2023-07-01-Preview&search=Machine%20Learning
Results:
{
  "@odata.context": "https://search-udc-dev-eastus-01.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "@search.nextPageParameters": {
    "search": "Machine Learning",
    "top": null,
    "skip": 50
  },
  "value": [
    {
      "@search.score": 34.912865,
      "Timestamp": "2023-08-07T21:51:00.618Z",
      "Key": "ms-learn9047ec1d-3190-4c8b-bcb4-0818a69ef813",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/explain-machine-learning-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning models",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 32.659386,
      "Timestamp": "2023-08-07T21:50:54.133Z",
      "Key": "ms-learn458c8cc3-1d5f-4f3b-a713-25c7b62055d6",
      "description": "Use automated machine learning in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 3459,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Use automated machine learning in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-automated-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "automated machine learning",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 32.659386,
      "Timestamp": "2023-08-07T21:50:54.132Z",
      "Key": "ms-learncf8b55b7-23d8-4763-ae3d-36966bc00e32",
      "description": "Use automated machine learning in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 3459,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Use automated machine learning in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-automated-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "automated machine learning",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 30.687607,
      "Timestamp": "2023-08-07T21:51:00.618Z",
      "Key": "ms-learn79e7e990-c902-459e-8639-56d9646520fc",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": null,
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/explain-machine-learning-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning models",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 30.687607,
      "Timestamp": "2023-08-07T21:51:00.618Z",
      "Key": "ms-learnb141649a-e509-4f0f-b84b-a437b6f04af7",
      "description": "Explain machine learning models with Azure Machine Learning",
      "duration": 47,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.76,
      "rating_count": 522,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Explain machine learning models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/explain-machine-learning-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning models",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 29.242064,
      "Timestamp": "2023-08-07T21:51:00.616Z",
      "Key": "ms-learn4ffb7ba0-0771-4a01-9da7-915635fb6ba4",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "models"
      ]
    },
    {
      "@search.score": 28.738686,
      "Timestamp": "2023-08-07T21:50:59.314Z",
      "Key": "ms-learn2b0ef195-b399-4e30-a61d-07b2bcf90857",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-data-drift-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "data drift"
      ]
    },
    {
      "@search.score": 28.738686,
      "Timestamp": "2023-08-07T21:51:00.629Z",
      "Key": "ms-learn2eb64953-5f7a-4985-8632-314014c8a8c2",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning SDK",
        "Introduction"
      ]
    },
    {
      "@search.score": 28.738686,
      "Timestamp": "2023-08-07T21:51:00.63Z",
      "Key": "ms-learna7182168-4625-4f3f-8348-bd0ddc542f63",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "student",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning SDK",
        "Introduction"
      ]
    },
    {
      "@search.score": 28.738686,
      "Timestamp": "2023-08-07T21:50:54.124Z",
      "Key": "ms-learnc3e839d4-2913-4d44-843d-9ed5250afe83",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/tune-hyperparameters-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Tune hyperparameters"
      ]
    },
    {
      "@search.score": 27.303646,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learn98b24c93-11ba-45fb-be8b-07fc40bebd19",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "student",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model selection",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 27.303646,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learnafe14147-e05a-4cfe-b086-93b4843fe5f0",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/train-local-model-with-azure-mls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 27.303646,
      "Timestamp": "2023-08-07T21:51:00.627Z",
      "Key": "ms-learnca22fe05-2abc-47d4-a275-28f33ffbf932",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model selection",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 27.303646,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learnf5492a10-074e-4436-bdee-971c285e5811",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "student",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/train-local-model-with-azure-mls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 27.249817,
      "Timestamp": "2023-08-07T21:50:56.709Z",
      "Key": "ms-learnca0b5b35-2276-44a7-b704-c519ce6c8c1a",
      "description": "Detect and mitigate unfairness in models with Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 429,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Detect and mitigate unfairness in models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/detect-mitigate-unfairness-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "unfairness",
        "models"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:51:04.483Z",
      "Key": "ms-learn2b94446f-a61b-4fbc-a20e-4f6c6306722d",
      "description": "Create a Clustering Model with Azure Machine Learning designer",
      "duration": 49,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 2435,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a Clustering Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-clustering-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "Clustering Model"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:50:55.437Z",
      "Key": "ms-learn5556e411-eadf-4582-a906-35746136caa4",
      "description": "Create a classification model with Azure Machine Learning designer",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 1848,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a classification model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-classification-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "classification model"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:51:04.483Z",
      "Key": "ms-learn5eb8b628-5bd4-444a-a8f2-268baff04eb0",
      "description": "Create a Clustering Model with Azure Machine Learning designer",
      "duration": 49,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 2435,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a Clustering Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-clustering-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "Clustering Model"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:51:04.493Z",
      "Key": "ms-learn62aa6a44-9129-4636-8c70-894e2311be72",
      "description": "Deploy batch inference pipelines with Azure Machine Learning",
      "duration": 44,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-machine-learning",
      "rating_average": 4.71,
      "rating_count": 572,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy batch inference pipelines with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-batch-inference-pipelines-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "batch inference pipelines",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:50:55.437Z",
      "Key": "ms-learn8860465a-b002-41bc-9025-f54d5ea1376d",
      "description": "Create a classification model with Azure Machine Learning designer",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.76,
      "rating_count": 1848,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a classification model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-classification-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "classification model"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:51:04.484Z",
      "Key": "ms-learnbbe6ffcb-3a56-4ea2-8715-e81e5f441a05",
      "description": "Create a Regression Model with Azure Machine Learning designer",
      "duration": 55,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 2352,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Create a Regression Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-regression-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "Regression Model"
      ]
    },
    {
      "@search.score": 26.606014,
      "Timestamp": "2023-08-07T21:51:04.484Z",
      "Key": "ms-learnfac8a485-321e-4777-bf59-b61d6d02d25d",
      "description": "Create a Regression Model with Azure Machine Learning designer",
      "duration": 55,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.75,
      "rating_count": 2352,
      "role": "ai-engineer",
      "source": "MS Learn",
      "title": "Create a Regression Model with Azure Machine Learning designer",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-regression-model-azure-machine-learning-designer/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning designer",
        "Regression Model"
      ]
    },
    {
      "@search.score": 26.510063,
      "Timestamp": "2023-08-07T21:50:54.119Z",
      "Key": "ms-learnc9ef57db-07ca-404f-986b-e9aa144fd562",
      "description": "Deploy real-time machine learning services with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.65,
      "rating_count": 1340,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy real-time machine learning services with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "real-time machine learning services",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 25.016806,
      "Timestamp": "2023-08-07T21:51:00.616Z",
      "Key": "ms-learn181bbcf0-6713-4ee7-b858-3cf54674ed18",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "models"
      ]
    },
    {
      "@search.score": 25.016806,
      "Timestamp": "2023-08-07T21:51:00.615Z",
      "Key": "ms-learn3f5306a1-2460-4628-9ada-8f0628b20bbd",
      "description": "Monitor models with Azure Machine Learning",
      "duration": 39,
      "instructor": null,
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.75,
      "rating_count": 504,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor models with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-models-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "models"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:50:54.124Z",
      "Key": "ms-learn16681c04-f885-49d8-ab7e-e477936a6bdc",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/tune-hyperparameters-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Tune hyperparameters"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:51:00.629Z",
      "Key": "ms-learn59c58efa-d830-4c65-8ac4-5432bb0ec73e",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "student",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning SDK",
        "Introduction"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:51:00.629Z",
      "Key": "ms-learn6456b6d8-0af1-4dfd-b895-37ecfb3df42b",
      "description": "Introduction to the Azure Machine Learning SDK",
      "duration": 60,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 2869,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Introduction to the Azure Machine Learning SDK",
      "url": "https://docs.microsoft.com/en-us/learn/modules/intro-to-azure-machine-learning-service/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning SDK",
        "Introduction"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:50:59.313Z",
      "Key": "ms-learn94a8abf2-6e37-40ed-8fed-6c23d50a7424",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": null,
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-data-drift-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "data drift"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:50:54.124Z",
      "Key": "ms-learn9e2bd51e-0f26-487b-b828-4bd6fd850743",
      "description": "Tune hyperparameters with Azure Machine Learning",
      "duration": 46,
      "instructor": null,
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.76,
      "rating_count": 544,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Tune hyperparameters with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/tune-hyperparameters-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Tune hyperparameters"
      ]
    },
    {
      "@search.score": 24.513428,
      "Timestamp": "2023-08-07T21:50:59.314Z",
      "Key": "ms-learna5ffe016-69df-4be9-be9a-76bdfc4092c2",
      "description": "Monitor data drift with Azure Machine Learning",
      "duration": 42,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.8,
      "rating_count": 814,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Monitor data drift with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/monitor-data-drift-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "data drift"
      ]
    },
    {
      "@search.score": 23.886328,
      "Timestamp": "2023-08-07T21:50:56.708Z",
      "Key": "ms-learn6b32e048-d4a8-426c-abf7-956369670549",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-compute-contexts-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Compute"
      ]
    },
    {
      "@search.score": 23.886328,
      "Timestamp": "2023-08-07T21:50:58Z",
      "Key": "ms-learna5b5e7fc-04c9-44a6-8f6d-2ffe764516c9",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/work-with-data-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Data"
      ]
    },
    {
      "@search.score": 23.886328,
      "Timestamp": "2023-08-07T21:50:58Z",
      "Key": "ms-learnacbba05d-493a-4548-a5df-ecf3ba1c7a05",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/work-with-data-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Data"
      ]
    },
    {
      "@search.score": 23.886328,
      "Timestamp": "2023-08-07T21:50:56.708Z",
      "Key": "ms-learnbf5af1d1-1929-4291-ad51-1e521d19255d",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure-machine-learning",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-compute-contexts-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Compute"
      ]
    },
    {
      "@search.score": 23.07839,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learn15f02c60-301b-4418-9368-044c0bbcd36f",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/train-local-model-with-azure-mls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 23.07839,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learn3f048030-d800-4cbe-9261-024a642ecbf7",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "student",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model selection",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 23.07839,
      "Timestamp": "2023-08-07T21:51:00.627Z",
      "Key": "ms-learn868b0ff7-95e5-49f4-b490-0eb51b136d87",
      "description": "Automate machine learning model selection with Azure Machine Learning",
      "duration": 25,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1289,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Automate machine learning model selection with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model selection",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 23.07839,
      "Timestamp": "2023-08-07T21:51:00.628Z",
      "Key": "ms-learnc1f7d0eb-8786-4f99-8eb9-db55ef8f789b",
      "description": "Train a machine learning model with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.63,
      "rating_count": 1998,
      "role": "student",
      "source": "MS Learn",
      "title": "Train a machine learning model with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/train-local-model-with-azure-mls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "machine learning model",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 23.024559,
      "Timestamp": "2023-08-07T21:51:00.613Z",
      "Key": "ms-learn3348b913-81d8-42c8-a511-36b9d7004fa9",
      "description": "Work with Azure Machine Learning to deploy serving models",
      "duration": 23,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-databricks",
      "rating_average": 4.67,
      "rating_count": 49,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Azure Machine Learning to deploy serving models",
      "url": "https://docs.microsoft.com/en-us/learn/modules/work-with-azure-machine-learning-deploy-serving-models/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "serving models"
      ]
    },
    {
      "@search.score": 22.380756,
      "Timestamp": "2023-08-07T21:51:04.493Z",
      "Key": "ms-learnd6a8fd2e-d3a5-449e-8d07-804b61d20a60",
      "description": "Deploy batch inference pipelines with Azure Machine Learning",
      "duration": 44,
      "instructor": null,
      "level": "intermediate",
      "product": "azure",
      "rating_average": 4.71,
      "rating_count": 572,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy batch inference pipelines with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-batch-inference-pipelines-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "batch inference pipelines",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 22.380756,
      "Timestamp": "2023-08-07T21:51:04.493Z",
      "Key": "ms-learnef8a8f33-680a-475d-8f84-a646ab5d1614",
      "description": "Deploy batch inference pipelines with Azure Machine Learning",
      "duration": 44,
      "instructor": null,
      "level": "intermediate",
      "product": "azure-portal",
      "rating_average": 4.71,
      "rating_count": 572,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy batch inference pipelines with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-batch-inference-pipelines-with-azure-machine-learning/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "batch inference pipelines",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 22.284805,
      "Timestamp": "2023-08-07T21:50:54.118Z",
      "Key": "ms-learndd2b8b0a-0c34-493d-aecb-ee38f7711e9d",
      "description": "Deploy real-time machine learning services with Azure Machine Learning",
      "duration": 40,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.65,
      "rating_count": 1340,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Deploy real-time machine learning services with Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "real-time machine learning services",
        "Azure Machine Learning"
      ]
    },
    {
      "@search.score": 19.661072,
      "Timestamp": "2023-08-07T21:50:58Z",
      "Key": "ms-learn5de4bb8a-9063-4a38-adb6-4d8f6f0c12b1",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/work-with-data-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Data"
      ]
    },
    {
      "@search.score": 19.661072,
      "Timestamp": "2023-08-07T21:50:56.708Z",
      "Key": "ms-learn7dfc1bb4-abe2-4277-a732-162f1b38b412",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "student",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-compute-contexts-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Compute"
      ]
    },
    {
      "@search.score": 19.661072,
      "Timestamp": "2023-08-07T21:50:56.708Z",
      "Key": "ms-learn81d1ec7d-958f-4e0f-9607-9ec1344b67ab",
      "description": "Work with Compute in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.68,
      "rating_count": 992,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Compute in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/use-compute-contexts-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Compute"
      ]
    },
    {
      "@search.score": 19.661072,
      "Timestamp": "2023-08-07T21:50:57.999Z",
      "Key": "ms-learna274e897-5b2e-4ff3-aec9-2066e94d9a51",
      "description": "Work with Data in Azure Machine Learning",
      "duration": 45,
      "instructor": null,
      "level": "beginner",
      "product": "azure",
      "rating_average": 4.66,
      "rating_count": 1119,
      "role": "data-scientist",
      "source": "MS Learn",
      "title": "Work with Data in Azure Machine Learning",
      "url": "https://docs.microsoft.com/en-us/learn/modules/work-with-data-in-aml/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "Azure Machine Learning",
        "Data"
      ]
    },
    {
      "@search.score": 16.128819,
      "Timestamp": "2023-08-07T21:50:55.434Z",
      "Key": "ms-learn0b0b920b-a1d7-4ee3-8d8c-5670b2144508",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": null,
      "level": "intermediate",
      "product": "m365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "free open online course",
        "enterprise AI management",
        "machine learning"
      ]
    },
    {
      "@search.score": 16.128819,
      "Timestamp": "2023-08-07T21:50:55.434Z",
      "Key": "ms-learn145bf0eb-edf3-4518-b858-4ad1a2ffb08d",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": null,
      "level": "intermediate",
      "product": "m365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "business-user",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "free open online course",
        "enterprise AI management",
        "machine learning"
      ]
    },
    {
      "@search.score": 16.128819,
      "Timestamp": "2023-08-07T21:50:55.434Z",
      "Key": "ms-learn18ee7c59-95d0-4709-a5a3-6d13672918a3",
      "description": "Learn enterprise AI management with our free open online course, including machine learning.",
      "duration": 30,
      "instructor": null,
      "level": "intermediate",
      "product": "dynamics-365",
      "rating_average": 4.75,
      "rating_count": 68,
      "role": "functional-consultant",
      "source": "MS Learn",
      "title": "Start the machine learning lifecycle with MLOps",
      "url": "https://docs.microsoft.com/en-us/learn/modules/start-ml-lifecycle-mlops/?WT.mc_id=api_CatalogApi",
      "keyphrases": [
        "free open online course",
        "enterprise AI management",
        "machine learning"
      ]
    }
  ],
  "@odata.nextLink": "https://search-udc-dev-eastus-01.search.windows.net/indexes/courses-index/docs/search?api-version=2023-07-01-Preview"
}

==================================================
Query - Library Indexes - Request URL
https://search-udc-dev-eastus-01.search.windows.net/indexes/paperlibrary-index/docs?api-version=2023-07-01-Preview&search=IoT%20Service
Query Results:
{
  "@odata.context": "https://search-udc-dev-eastus-01.search.windows.net/indexes('paperlibrary-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 14.886934,
      "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker  J Big Data            (2019) 6:95  \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence:   \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker  J Big Data            (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\n\n\nPage 3 of 25Sarker  J Big Data            (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker  J Big Data            (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker  J Big Data            (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker  J Big Data            (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker  J Big Data            (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker  J Big Data            (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1  = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker  J Big Data            (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker  J Big Data            (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsiv",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNTM3LTAxOS0wMjU4LTQucGRm0",
      "metadata_author": "Iqbal H. Sarker ",
      "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions",
      "people": [
        "Iqbal H. Sarker",
        "Sarker  J",
        "J",
        "larity",
        "Alice",
        "J Big",
        "Schilit",
        "Brown",
        "Simi",
        "Ryan",
        "Franklin",
        "Ward",
        "Hull",
        "Rodden",
        "Dey",
        "mation",
        "Song",
        "Rawassizadeh",
        "Mukherji",
        "Bayir",
        "Paireekreng",
        "Jayarajah",
        "Xu",
        "Mehrotra",
        "Zhu",
        "Oulasvirta",
        "Yu",
        "Naboulsi",
        "Dashdorj",
        "Shin",
        "Farrahi",
        "Ozer",
        "Do",
        "Karatzoglou",
        "Phithakkitnukoon",
        "Khail",
        "Dekel",
        "Zulkernain",
        "Seo",
        "Sarker",
        "Halvey",
        "Zhang",
        "Shokoohi",
        "Hartono",
        "Keogh",
        "Lu",
        "Kandasamy"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "intelligent decision making strategies",
        "advanced data analytical techniques",
        "25Sarker  J Big Data",
        "Intelligent systems Open Access",
        "machine learning based techniques",
        "Creative Commons license",
        "corresponding context-aware rule learning",
        "Iqbal H. Sarker",
        "intelligent context-aware applications",
        "User behavior modeling",
        "data-driven context-aware systems",
        "raw contextual data",
        "corresponding search interests",
        "original author(s",
        "phone data analytics",
        "corresponding data processing",
        "multifunctional cell phones",
        "users’ surrounding environment",
        "smartphone data",
        "context-aware rules",
        "Data science",
        "author information",
        "future directions",
        "recent days",
        "daily life",
        "important IoT",
        "next generation",
        "wireless connectivity",
        "work coverage",
        "developed countries",
        "recent statistics",
        "Google Trends",
        "users’ interest",
        "other platforms",
        "Desktop Computer",
        "Laptop Computer",
        "current world",
        "recent developments",
        "behavioral activities",
        "wide attention",
        "high precision",
        "traditional approaches",
        "efficient results",
        "previous work",
        "rule-based automated",
        "IoT services",
        "iveco mmons",
        "unrestricted use",
        "appropriate credit",
        "1 Swinburne University",
        "Full list",
        "Tablet Computer",
        "last 5 years",
        "timestamp information",
        "particular date",
        "highest point",
        "personal devices",
        "Things) devices",
        "essential part",
        "world population",
        "Mobile Phones",
        "particular term",
        "SURVEY PAPER",
        "peak popularity",
        "challenges",
        "Introduction",
        "smartphones",
        "individuals",
        "Internet",
        "capabilities",
        "number",
        "Fig.",
        "Abstract",
        "ogy",
        "academia",
        "industry",
        "order",
        "set",
        "key",
        "contexts",
        "comparison",
        "effective",
        "article",
        "area",
        "discussion",
        "Clustering",
        "Classification",
        "Association",
        "Personalization",
        "Time-series",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "msarker",
        "Melbourne",
        "Australia",
        "Page",
        "Figure",
        "x-axis",
        "range",
        "chart",
        "instance",
        "value",
        "y-axis",
        "context-aware mobile notification management system",
        "other broad application areas",
        "corresponding rule-based prediction model",
        "different context-aware test cases",
        "data-driven intelligent context-aware systems",
        "optimal time-based context-aware rules",
        "rule-based intelligent context-aware systems",
        "context-aware smart city services",
        "smartphone data analytic solutions",
        "users’ behavioral activity patterns",
        "intelligent data-driven decisions",
        "THEN” logical structure",
        "Users’ interest trends",
        "intelligent eHealth services",
        "aware behavioral rules",
        "sponding behavioral activities",
        "relevant context-aware rules",
        "relevant contextual data",
        "contextual data patterns",
        "contextual data clustering",
        "machine learning researchers",
        "machine learning techniques",
        "different surrounding contexts",
        "machine learning rules",
        "relevant contextual information",
        "data mining techniques",
        "advanced data analysis",
        "corresponding data",
        "rule-based classification",
        "raw data",
        "different contexts",
        "mobile notifications",
        "context-aware  software",
        "hidden patterns",
        "data science",
        "time-series data",
        "advanced features",
        "mental learning",
        "personalized services",
        "rity services",
        "deeper analysis",
        "association analysis",
        "detailed analysis",
        "daily basis",
        "great source",
        "useful information",
        "two parts",
        "temporal context",
        "tial context",
        "social contexts",
        "family members",
        "specific exceptions",
        "particular domain",
        "prac- titioners",
        "many reasons",
        "various purposes",
        "particular timestamp",
        "numeric values",
        "world-wide popularity",
        "ture optimization",
        "dynamic updating",
        "The reason",
        "huge amount",
        "unknown correlations",
        "diverse behaviors",
        "quent part",
        "individuals’ usage",
        "phone user",
        "devices",
        "result",
        "antecedent",
        "others",
        "example",
        "Alice",
        "work",
        "evening",
        "general",
        "preferences",
        "addition",
        "cable",
        "study",
        "applications",
        "building",
        "Several",
        "selection",
        "aim",
        "machine learning based context-aware rule learning framework",
        "Suggested machine learning based framework",
        "Context-aware rule learning strategies",
        "rule discovery techniques",
        "decision making capability",
        "real world applications",
        "rule-based intelligent systems",
        "Human Computer Interaction",
        "corresponding context-aware rules",
        "smartphone data analytics",
        "contextual smartphone data",
        "incremental learning",
        "context-aware computing",
        "updation techniques",
        "smrtphone data",
        "several perspectives",
        "time-series modeling",
        "various layers",
        "various perspectives",
        "different meanings",
        "different purposes",
        "numerous areas",
        "Computer-Supported Collaborative",
        "Ambient Intelligence",
        "recent works",
        "broader collection",
        "social aspects",
        "little work",
        "earlier work",
        "brief discussion",
        "background information",
        "main characteristics",
        "existing research",
        "brief survey",
        "first article",
        "multi-dimensional contexts",
        "sion” section",
        "Computing area",
        "profit",
        "analysis",
        "discretization",
        "issues",
        "contributions",
        "paper",
        "shortcomings",
        "role",
        "knowledge",
        "remainder",
        "variety",
        "notion",
        "Pervasive",
        "mobile",
        "Ubiquitous",
        "context-awareness",
        "location",
        "people",
        "objects",
        "factors",
        "physical",
        "entity",
        "activities",
        "users",
        "definitions",
        "categories",
        "daily life activities",
        "temporal informa- tion",
        "user diverse activities",
        "rent social situation",
        "data source",
        "smartphone usage",
        "temporal information",
        "defi- nitions",
        "dis- cussion",
        "Several studies",
        "surrounding people",
        "Other definitions",
        "other hand",
        "current situation",
        "nearby people",
        "available processors",
        "network capacity",
        "noise level",
        "lighting level",
        "day situations",
        "cellular phones",
        "computing community",
        "computing area",
        "location information",
        "locational information",
        "environmental information",
        "surrounding information",
        "changing environment",
        "physical environment",
        "other researchers",
        "important aspects",
        "alternative view",
        "con- text",
        "computing environment",
        "social context",
        "following definition",
        "user location",
        "user input",
        "relevant contexts",
        "spatial context",
        "user environment",
        "pervasive",
        "section",
        "scope",
        "different",
        "perspectives",
        "Schilit",
        "Brown",
        "temperature",
        "Simi",
        "identity",
        "account",
        "Ryan",
        "synonyms",
        "computer",
        "Franklin",
        "Ward",
        "state",
        "Hull",
        "entire",
        "settings",
        "Rodden",
        "resources",
        "connectivity",
        "display",
        "costs",
        "examples",
        "Dey",
        "survey",
        "person",
        "place",
        "interaction",
        "influence",
        "decisions",
        "everything",
        "lives",
        "Mobile",
        "personalized mobile game recommendation system",
        "vidual mobile phone user behavior",
        "mobile notification management systems",
        "relevant contextual informa- tion",
        "related contextual infor- mation",
        "various smart phone applications",
        "individual mobile phone users",
        "personalized context-aware recommendation",
        "user mobile web navigation",
        "smart mobile applications",
        "smart mobile phone",
        "phone usage behavior",
        "standardized communications rules",
        "International Telecommunication Union",
        "sive commercial industry",
        "accurate context-aware predictions",
        "text communication service",
        "contextual behavioral patterns",
        "automatic spam filtering",
        "smartphone notification logs",
        "phone call activities",
        "short text messages",
        "contextual usage patterns",
        "short message service",
        "spam text messages",
        "contextual patterns",
        "unique phone",
        "individual users",
        "User navigation",
        "short messages",
        "game log",
        "voice communication",
        "major activities",
        "spam messages",
        "various types",
        "various kinds",
        "various real",
        "web log",
        "web searching",
        "particular user",
        "meta data",
        "log data",
        "outgoing calls",
        "incoming calls",
        "other dimensions",
        "day situation",
        "social relationship",
        "numerous growth",
        "dramatic increasing",
        "good time",
        "bad time",
        "rapid development",
        "smartphone apps",
        "Such logs",
        "social networks",
        "predictive suggestions",
        "portal structure",
        "computing capabilities",
        "life-logging device",
        "technical sense",
        "context data",
        "tual information",
        "context source",
        "SMS log",
        "real world",
        "life purposes",
        "contact number",
        "ability",
        "caller",
        "callee",
        "exchange",
        "protocols",
        "81 billion",
        "task",
        "Multimedia",
        "Facebook",
        "Gmail",
        "Youtube",
        "Skype",
        "notifications",
        "events",
        "news",
        "reminders",
        "alerts",
        "games",
        "promotional",
        "emails",
        "Twitter",
        "LinkedIN",
        "WhatsApp",
        "Viver",
        "intelligent",
        "entertainment",
        "chat",
        "misc",
        "TV",
        "netting",
        "travel",
        "sport",
        "banking",
        "needs",
        "action",
        "adventure",
        "puzzle",
        "RPG",
        "strategy",
        "opportunity",
        "memories",
        "Time interval type Number",
        "existing time-series segmentation approaches",
        "time‑series smartphone data",
        "temporal context based rules",
        "mobile phone data",
        "Google play API",
        "geographical loca- tion",
        "segment details References",
        "personalized log data",
        "user phone calls",
        "time-based behavior modeling",
        "smart mobile phones",
        "life log data",
        "mobile user behavior",
        "behavioral data clusters",
        "A time series",
        "context-aware rule learning",
        "related contextual information",
        "static time segments",
        "existing rule",
        "IoT data",
        "data points",
        "existing strategies",
        "important context",
        "contextual rule",
        "learning rules",
        "temporal, spatial",
        "effective modeling",
        "behavioral rules",
        "exact time",
        "user experience",
        "surrounding environment",
        "core components",
        "life- logs",
        "SMS headers",
        "App use",
        "physical activities",
        "Bluetooth devices",
        "open challenges",
        "diverse activities",
        "open problem",
        "Various types",
        "different applications",
        "remaining hours",
        "Unequal 3 Morning",
        "Unequal 4 Morning",
        "Unequal 5 Morning",
        "late morning",
        "Equal 4 Night",
        "6:00 a",
        "sensors",
        "content",
        "sapp",
        "WiFi",
        "proximity",
        "opportunities",
        "future",
        "directions",
        "discovery",
        "behaviors",
        "sequence",
        "research",
        "Table",
        "afternoon",
        "Song",
        "Rawassizadeh",
        "Mukherji",
        "Bayir",
        "Paireekreng",
        "day",
        "Jayarajah",
        "Do",
        "Xu",
        "Mehrotra",
        "Zhu",
        "Oulasvirta",
        "Next",
        "Yu",
        "midnight",
        "0:00",
        "dynamic segmenta- tion technique",
        "high confidence temporal rules",
        "unequal interval based segmentation",
        "high confidence rules",
        "four different time segments",
        "various mobile applications",
        "dynamic segmen- tation",
        "mobile phone users",
        "individual smartphone users",
        "four time slots",
        "same interval length",
        "unequal time interval",
        "two broad categories",
        "users’ behavioral activities",
        "dynamic segmentation techniques",
        "Such static segmentation",
        "various time intervals",
        "temporal coverage",
        "different lengths",
        "temporal contexts",
        "arbitrary categories",
        "two types",
        "behavioral characteristics",
        "corresponding segmentation",
        "population behavior",
        "ioral evidence",
        "usage records",
        "seg- ments",
        "single parameter",
        "division result",
        "base period",
        "time period",
        "static generation",
        "meaningful results",
        "variable number",
        "one case",
        "N1 number",
        "N2 number",
        "Naboulsi",
        "Dashdorj",
        "Shin",
        "11:00 a",
        "S5",
        "S8",
        "Farrahi",
        "ii",
        "researchers",
        "periods",
        "1 h",
        "works",
        "approaches",
        "goal",
        "change",
        "patterns",
        "authors",
        "Tmax",
        "24-h",
        "BP",
        "5 min",
        "2:00",
        "5:00",
        "7:00",
        "9:00",
        "important machine learning tech- niques",
        "time- series data Lu",
        "Base technique Description References",
        "time-series log data Halvey",
        "data shape based approaches",
        "squared error-based clustering algorithm",
        "time-series data analysis",
        "time-series data Hartono",
        "time-series data Das",
        "similar K-medoids method",
        "mean value calculation",
        "temporal context Khail",
        "different initial points",
        "base time period",
        "dynamic time segments",
        "Various calendar schedules",
        "corresponding time boundaries",
        "user behavior patterns",
        "large time segments",
        "large value",
        "initial partitions",
        "different value",
        "time segmentation",
        "time interval",
        "different contexts",
        "predefined value",
        "finest granularity",
        "segments decreases",
        "iable length",
        "particular event",
        "personal calendars",
        "multiple thresholds",
        "sliding window",
        "several applications",
        "specific application",
        "convergence centroids",
        "segments Ozer",
        "segmentation Do",
        "upper boundary",
        "particular segment",
        "genetic algorithm",
        "users’ behavior",
        "users’ preferences",
        "one user",
        "users’ behave",
        "clustering algorithms",
        "Single parameter",
        "K-means algorithm",
        "predefined number",
        "individual",
        "assumptions",
        "problems",
        "sense",
        "clusters",
        "K.",
        "outliers",
        "30 min",
        "Karatzoglou",
        "small",
        "Phithakkitnukoon",
        "Dekel",
        "Zulkernain",
        "Seo",
        "Multi-thresholds",
        "lower",
        "Zhang",
        "Shokoohi",
        "Keogh",
        "Kandasamy",
        "characteristic",
        "reason",
        "1",
        "2:30",
        "single linkage agglomerative clustering algorithm",
        "bottom-up hierarchical data processing",
        "popular agglomerative clustering",
        "other characteristic mismatches",
        "device clustering method",
        "agglomerative methods",
        "complete linkage",
        "device methods",
        "other clusters",
        "hierarchical algorithms",
        "time segments",
        "behavioral clusters",
        "nearest neighbor",
        "proximity matrix",
        "new cluster",
        "matrix value",
        "outlier problem",
        "presence",
        "medoid",
        "K-means",
        "size",
        "user",
        "practice",
        "simplest",
        "distance"
      ],
      "merged_content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker  J Big Data            (2019) 6:95  \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence:   \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker  J Big Data            (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n 100 30 10 90 40 20 50 60 70 80 0 13-Apr-2014 + 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 - 13-Aug-2015 Desktop 13-Oct-2015 13-Dec-2015 1 13-Feb-2016 -Tablet 13-Apr-2016 13-Jun-2016 13-Aug-2016 - 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 -Mobile Phone 13-Jun-2017 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019 \n\n\n\nPage 3 of 25Sarker  J Big Data            (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker  J Big Data            (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker  J Big Data            (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker  J Big Data            (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker  J Big Data            (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker  J Big Data            (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1  = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker  J Big Data            (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker  J Big Data            (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsiv",
      "text": [
        "100 30 10 90 40 20 50 60 70 80 0 13-Apr-2014 + 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 - 13-Aug-2015 Desktop 13-Oct-2015 13-Dec-2015 1 13-Feb-2016 -Tablet 13-Apr-2016 13-Jun-2016 13-Aug-2016 - 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 -Mobile Phone 13-Jun-2017 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019",
        "Real World Applications and Services Application 1 Application 2 Application N Dynamic Updating and Management Recency Analysis and Layer 4 Mining Rule Updation Rule Discovery Contextual Rule-based Rule Layer 3 Preferences Learning Generalization Context Discretization Time Series Contextual Layer 2 Modeling Data Clustering Contextual Data Acquisition Smartphone External Layer 1 Logs Sensors Sources",
        "Published online: 31 October 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"100 30 10 90 40 20 50 60 70 80 0 13-Apr-2014 + 13-Jun-2014 13-Aug-2014 13-Oct-2014 13-Dec-2014 13-Feb-2015 13-Apr-2015 13-Jun-2015 - 13-Aug-2015 Desktop 13-Oct-2015 13-Dec-2015 1 13-Feb-2016 -Tablet 13-Apr-2016 13-Jun-2016 13-Aug-2016 - 13-Oct-2016 Laptop 13-Dec-2016 13-Feb-2017 13-Apr-2017 -Mobile Phone 13-Jun-2017 13-Aug-2017 13-Oct-2017 13-Dec-2017 13-Feb-2018 13-Apr-2018 13-Jun-2018 13-Aug-2018 13-Oct-2018 13-Dec-2018 13-Feb-2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":24},{\"x\":37,\"y\":25},{\"x\":37,\"y\":40},{\"x\":0,\"y\":39}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":10,\"y\":247},{\"x\":37,\"y\":247},{\"x\":37,\"y\":263},{\"x\":10,\"y\":263}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":11,\"y\":309},{\"x\":36,\"y\":308},{\"x\":36,\"y\":323},{\"x\":11,\"y\":324}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":10,\"y\":57},{\"x\":38,\"y\":57},{\"x\":38,\"y\":73},{\"x\":9,\"y\":72}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":11,\"y\":215},{\"x\":34,\"y\":215},{\"x\":34,\"y\":230},{\"x\":11,\"y\":230}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":12,\"y\":278},{\"x\":37,\"y\":278},{\"x\":36,\"y\":294},{\"x\":12,\"y\":293}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":12,\"y\":183},{\"x\":37,\"y\":183},{\"x\":37,\"y\":199},{\"x\":12,\"y\":199}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":12,\"y\":152},{\"x\":34,\"y\":152},{\"x\":33,\"y\":167},{\"x\":11,\"y\":167}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":13,\"y\":119},{\"x\":34,\"y\":119},{\"x\":34,\"y\":135},{\"x\":12,\"y\":135}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":13,\"y\":88},{\"x\":36,\"y\":88},{\"x\":36,\"y\":103},{\"x\":13,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":25,\"y\":340},{\"x\":36,\"y\":340},{\"x\":36,\"y\":355},{\"x\":25,\"y\":355}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":50,\"y\":466},{\"x\":50,\"y\":337},{\"x\":70,\"y\":337},{\"x\":71,\"y\":466}],\"text\":\"13-Apr-2014 +\"},{\"boundingBox\":[{\"x\":96,\"y\":464},{\"x\":96,\"y\":365},{\"x\":115,\"y\":365},{\"x\":116,\"y\":464}],\"text\":\"13-Jun-2014\"},{\"boundingBox\":[{\"x\":142,\"y\":466},{\"x\":142,\"y\":365},{\"x\":162,\"y\":365},{\"x\":161,\"y\":467}],\"text\":\"13-Aug-2014\"},{\"boundingBox\":[{\"x\":187,\"y\":467},{\"x\":187,\"y\":364},{\"x\":206,\"y\":364},{\"x\":207,\"y\":466}],\"text\":\"13-Oct-2014\"},{\"boundingBox\":[{\"x\":234,\"y\":465},{\"x\":233,\"y\":364},{\"x\":252,\"y\":364},{\"x\":253,\"y\":465}],\"text\":\"13-Dec-2014\"},{\"boundingBox\":[{\"x\":279,\"y\":464},{\"x\":278,\"y\":365},{\"x\":298,\"y\":365},{\"x\":299,\"y\":464}],\"text\":\"13-Feb-2015\"},{\"boundingBox\":[{\"x\":324,\"y\":467},{\"x\":323,\"y\":364},{\"x\":343,\"y\":364},{\"x\":344,\"y\":466}],\"text\":\"13-Apr-2015\"},{\"boundingBox\":[{\"x\":369,\"y\":464},{\"x\":368,\"y\":364},{\"x\":388,\"y\":364},{\"x\":389,\"y\":464}],\"text\":\"13-Jun-2015\"},{\"boundingBox\":[{\"x\":402,\"y\":0},{\"x\":429,\"y\":1},{\"x\":428,\"y\":17},{\"x\":401,\"y\":16}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":415,\"y\":467},{\"x\":413,\"y\":364},{\"x\":435,\"y\":363},{\"x\":436,\"y\":466}],\"text\":\"13-Aug-2015\"},{\"boundingBox\":[{\"x\":446,\"y\":1},{\"x\":528,\"y\":3},{\"x\":528,\"y\":19},{\"x\":446,\"y\":18}],\"text\":\"Desktop\"},{\"boundingBox\":[{\"x\":460,\"y\":464},{\"x\":459,\"y\":365},{\"x\":479,\"y\":364},{\"x\":480,\"y\":464}],\"text\":\"13-Oct-2015\"},{\"boundingBox\":[{\"x\":506,\"y\":464},{\"x\":505,\"y\":365},{\"x\":526,\"y\":365},{\"x\":527,\"y\":463}],\"text\":\"13-Dec-2015\"},{\"boundingBox\":[{\"x\":509,\"y\":62},{\"x\":537,\"y\":65},{\"x\":534,\"y\":100},{\"x\":506,\"y\":96}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":552,\"y\":465},{\"x\":552,\"y\":364},{\"x\":572,\"y\":364},{\"x\":572,\"y\":465}],\"text\":\"13-Feb-2016\"},{\"boundingBox\":[{\"x\":551,\"y\":1},{\"x\":667,\"y\":1},{\"x\":667,\"y\":18},{\"x\":551,\"y\":18}],\"text\":\"-Tablet\"},{\"boundingBox\":[{\"x\":596,\"y\":465},{\"x\":596,\"y\":364},{\"x\":617,\"y\":364},{\"x\":618,\"y\":465}],\"text\":\"13-Apr-2016\"},{\"boundingBox\":[{\"x\":643,\"y\":464},{\"x\":643,\"y\":364},{\"x\":663,\"y\":364},{\"x\":663,\"y\":464}],\"text\":\"13-Jun-2016\"},{\"boundingBox\":[{\"x\":689,\"y\":467},{\"x\":688,\"y\":365},{\"x\":708,\"y\":365},{\"x\":709,\"y\":466}],\"text\":\"13-Aug-2016\"},{\"boundingBox\":[{\"x\":690,\"y\":2},{\"x\":728,\"y\":2},{\"x\":727,\"y\":17},{\"x\":690,\"y\":18}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":734,\"y\":467},{\"x\":734,\"y\":364},{\"x\":753,\"y\":364},{\"x\":753,\"y\":466}],\"text\":\"13-Oct-2016\"},{\"boundingBox\":[{\"x\":742,\"y\":1},{\"x\":813,\"y\":3},{\"x\":813,\"y\":19},{\"x\":741,\"y\":17}],\"text\":\"Laptop\"},{\"boundingBox\":[{\"x\":781,\"y\":467},{\"x\":779,\"y\":364},{\"x\":798,\"y\":364},{\"x\":800,\"y\":466}],\"text\":\"13-Dec-2016\"},{\"boundingBox\":[{\"x\":826,\"y\":465},{\"x\":825,\"y\":365},{\"x\":845,\"y\":365},{\"x\":846,\"y\":465}],\"text\":\"13-Feb-2017\"},{\"boundingBox\":[{\"x\":871,\"y\":464},{\"x\":870,\"y\":365},{\"x\":890,\"y\":365},{\"x\":891,\"y\":464}],\"text\":\"13-Apr-2017\"},{\"boundingBox\":[{\"x\":879,\"y\":1},{\"x\":1023,\"y\":1},{\"x\":1023,\"y\":18},{\"x\":879,\"y\":17}],\"text\":\"-Mobile Phone\"},{\"boundingBox\":[{\"x\":916,\"y\":462},{\"x\":916,\"y\":363},{\"x\":935,\"y\":363},{\"x\":936,\"y\":462}],\"text\":\"13-Jun-2017\"},{\"boundingBox\":[{\"x\":962,\"y\":467},{\"x\":961,\"y\":365},{\"x\":982,\"y\":365},{\"x\":983,\"y\":466}],\"text\":\"13-Aug-2017\"},{\"boundingBox\":[{\"x\":1007,\"y\":462},{\"x\":1006,\"y\":365},{\"x\":1027,\"y\":365},{\"x\":1027,\"y\":462}],\"text\":\"13-Oct-2017\"},{\"boundingBox\":[{\"x\":1051,\"y\":465},{\"x\":1051,\"y\":365},{\"x\":1072,\"y\":365},{\"x\":1071,\"y\":465}],\"text\":\"13-Dec-2017\"},{\"boundingBox\":[{\"x\":1099,\"y\":465},{\"x\":1098,\"y\":364},{\"x\":1119,\"y\":364},{\"x\":1120,\"y\":465}],\"text\":\"13-Feb-2018\"},{\"boundingBox\":[{\"x\":1143,\"y\":465},{\"x\":1143,\"y\":363},{\"x\":1163,\"y\":363},{\"x\":1164,\"y\":465}],\"text\":\"13-Apr-2018\"},{\"boundingBox\":[{\"x\":1189,\"y\":462},{\"x\":1189,\"y\":363},{\"x\":1209,\"y\":363},{\"x\":1209,\"y\":462}],\"text\":\"13-Jun-2018\"},{\"boundingBox\":[{\"x\":1235,\"y\":465},{\"x\":1235,\"y\":362},{\"x\":1255,\"y\":362},{\"x\":1255,\"y\":465}],\"text\":\"13-Aug-2018\"},{\"boundingBox\":[{\"x\":1280,\"y\":467},{\"x\":1279,\"y\":363},{\"x\":1297,\"y\":363},{\"x\":1298,\"y\":466}],\"text\":\"13-Oct-2018\"},{\"boundingBox\":[{\"x\":1325,\"y\":466},{\"x\":1325,\"y\":363},{\"x\":1344,\"y\":363},{\"x\":1344,\"y\":467}],\"text\":\"13-Dec-2018\"},{\"boundingBox\":[{\"x\":1373,\"y\":464},{\"x\":1372,\"y\":366},{\"x\":1392,\"y\":366},{\"x\":1393,\"y\":464}],\"text\":\"13-Feb-2019\"}],\"words\":[{\"boundingBox\":[{\"x\":0,\"y\":24},{\"x\":31,\"y\":25},{\"x\":31,\"y\":40},{\"x\":0,\"y\":39}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":11,\"y\":247},{\"x\":32,\"y\":247},{\"x\":32,\"y\":263},{\"x\":11,\"y\":263}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":11,\"y\":309},{\"x\":31,\"y\":308},{\"x\":31,\"y\":323},{\"x\":12,\"y\":324}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":10,\"y\":57},{\"x\":32,\"y\":57},{\"x\":32,\"y\":73},{\"x\":10,\"y\":72}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":12,\"y\":215},{\"x\":32,\"y\":215},{\"x\":32,\"y\":230},{\"x\":12,\"y\":230}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":12,\"y\":278},{\"x\":32,\"y\":278},{\"x\":31,\"y\":294},{\"x\":12,\"y\":293}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":12,\"y\":183},{\"x\":32,\"y\":183},{\"x\":32,\"y\":199},{\"x\":12,\"y\":199}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":12,\"y\":152},{\"x\":32,\"y\":152},{\"x\":32,\"y\":167},{\"x\":12,\"y\":167}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":13,\"y\":119},{\"x\":32,\"y\":119},{\"x\":32,\"y\":135},{\"x\":13,\"y\":135}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":13,\"y\":88},{\"x\":32,\"y\":88},{\"x\":32,\"y\":103},{\"x\":13,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":25,\"y\":340},{\"x\":34,\"y\":340},{\"x\":34,\"y\":355},{\"x\":25,\"y\":355}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":50,\"y\":466},{\"x\":50,\"y\":365},{\"x\":70,\"y\":365},{\"x\":72,\"y\":466}],\"text\":\"13-Apr-2014\"},{\"boundingBox\":[{\"x\":50,\"y\":356},{\"x\":50,\"y\":345},{\"x\":70,\"y\":345},{\"x\":70,\"y\":356}],\"text\":\"+\"},{\"boundingBox\":[{\"x\":96,\"y\":464},{\"x\":96,\"y\":366},{\"x\":115,\"y\":366},{\"x\":116,\"y\":464}],\"text\":\"13-Jun-2014\"},{\"boundingBox\":[{\"x\":144,\"y\":467},{\"x\":142,\"y\":367},{\"x\":162,\"y\":367},{\"x\":160,\"y\":467}],\"text\":\"13-Aug-2014\"},{\"boundingBox\":[{\"x\":189,\"y\":466},{\"x\":187,\"y\":367},{\"x\":206,\"y\":367},{\"x\":206,\"y\":466}],\"text\":\"13-Oct-2014\"},{\"boundingBox\":[{\"x\":234,\"y\":465},{\"x\":233,\"y\":366},{\"x\":253,\"y\":366},{\"x\":254,\"y\":465}],\"text\":\"13-Dec-2014\"},{\"boundingBox\":[{\"x\":280,\"y\":464},{\"x\":278,\"y\":366},{\"x\":298,\"y\":366},{\"x\":300,\"y\":464}],\"text\":\"13-Feb-2015\"},{\"boundingBox\":[{\"x\":325,\"y\":467},{\"x\":323,\"y\":368},{\"x\":343,\"y\":367},{\"x\":343,\"y\":467}],\"text\":\"13-Apr-2015\"},{\"boundingBox\":[{\"x\":370,\"y\":464},{\"x\":368,\"y\":367},{\"x\":388,\"y\":366},{\"x\":389,\"y\":463}],\"text\":\"13-Jun-2015\"},{\"boundingBox\":[{\"x\":408,\"y\":0},{\"x\":418,\"y\":1},{\"x\":417,\"y\":17},{\"x\":407,\"y\":16}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":417,\"y\":467},{\"x\":414,\"y\":368},{\"x\":434,\"y\":367},{\"x\":434,\"y\":467}],\"text\":\"13-Aug-2015\"},{\"boundingBox\":[{\"x\":448,\"y\":2},{\"x\":523,\"y\":3},{\"x\":522,\"y\":19},{\"x\":447,\"y\":18}],\"text\":\"Desktop\"},{\"boundingBox\":[{\"x\":461,\"y\":465},{\"x\":460,\"y\":367},{\"x\":480,\"y\":367},{\"x\":481,\"y\":464}],\"text\":\"13-Oct-2015\"},{\"boundingBox\":[{\"x\":506,\"y\":463},{\"x\":505,\"y\":366},{\"x\":526,\"y\":367},{\"x\":528,\"y\":464}],\"text\":\"13-Dec-2015\"},{\"boundingBox\":[{\"x\":514,\"y\":62},{\"x\":537,\"y\":65},{\"x\":532,\"y\":100},{\"x\":510,\"y\":97}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":553,\"y\":466},{\"x\":552,\"y\":367},{\"x\":572,\"y\":366},{\"x\":573,\"y\":465}],\"text\":\"13-Feb-2016\"},{\"boundingBox\":[{\"x\":553,\"y\":3},{\"x\":664,\"y\":2},{\"x\":664,\"y\":19},{\"x\":553,\"y\":18}],\"text\":\"-Tablet\"},{\"boundingBox\":[{\"x\":596,\"y\":465},{\"x\":596,\"y\":366},{\"x\":616,\"y\":366},{\"x\":618,\"y\":465}],\"text\":\"13-Apr-2016\"},{\"boundingBox\":[{\"x\":643,\"y\":464},{\"x\":643,\"y\":366},{\"x\":663,\"y\":366},{\"x\":663,\"y\":464}],\"text\":\"13-Jun-2016\"},{\"boundingBox\":[{\"x\":690,\"y\":467},{\"x\":688,\"y\":367},{\"x\":708,\"y\":367},{\"x\":707,\"y\":467}],\"text\":\"13-Aug-2016\"},{\"boundingBox\":[{\"x\":691,\"y\":2},{\"x\":701,\"y\":2},{\"x\":701,\"y\":18},{\"x\":691,\"y\":18}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":735,\"y\":466},{\"x\":734,\"y\":368},{\"x\":753,\"y\":368},{\"x\":753,\"y\":466}],\"text\":\"13-Oct-2016\"},{\"boundingBox\":[{\"x\":743,\"y\":2},{\"x\":806,\"y\":4},{\"x\":806,\"y\":20},{\"x\":743,\"y\":18}],\"text\":\"Laptop\"},{\"boundingBox\":[{\"x\":782,\"y\":467},{\"x\":779,\"y\":367},{\"x\":799,\"y\":367},{\"x\":799,\"y\":467}],\"text\":\"13-Dec-2016\"},{\"boundingBox\":[{\"x\":827,\"y\":465},{\"x\":825,\"y\":367},{\"x\":846,\"y\":366},{\"x\":847,\"y\":465}],\"text\":\"13-Feb-2017\"},{\"boundingBox\":[{\"x\":871,\"y\":465},{\"x\":870,\"y\":366},{\"x\":890,\"y\":366},{\"x\":891,\"y\":464}],\"text\":\"13-Apr-2017\"},{\"boundingBox\":[{\"x\":880,\"y\":1},{\"x\":953,\"y\":2},{\"x\":953,\"y\":18},{\"x\":880,\"y\":18}],\"text\":\"-Mobile\"},{\"boundingBox\":[{\"x\":958,\"y\":2},{\"x\":1018,\"y\":2},{\"x\":1018,\"y\":18},{\"x\":959,\"y\":18}],\"text\":\"Phone\"},{\"boundingBox\":[{\"x\":916,\"y\":462},{\"x\":916,\"y\":366},{\"x\":935,\"y\":364},{\"x\":937,\"y\":462}],\"text\":\"13-Jun-2017\"},{\"boundingBox\":[{\"x\":963,\"y\":467},{\"x\":961,\"y\":366},{\"x\":982,\"y\":366},{\"x\":981,\"y\":467}],\"text\":\"13-Aug-2017\"},{\"boundingBox\":[{\"x\":1007,\"y\":463},{\"x\":1006,\"y\":367},{\"x\":1028,\"y\":367},{\"x\":1028,\"y\":462}],\"text\":\"13-Oct-2017\"},{\"boundingBox\":[{\"x\":1051,\"y\":465},{\"x\":1051,\"y\":367},{\"x\":1072,\"y\":366},{\"x\":1072,\"y\":465}],\"text\":\"13-Dec-2017\"},{\"boundingBox\":[{\"x\":1100,\"y\":465},{\"x\":1099,\"y\":366},{\"x\":1119,\"y\":366},{\"x\":1120,\"y\":464}],\"text\":\"13-Feb-2018\"},{\"boundingBox\":[{\"x\":1143,\"y\":465},{\"x\":1143,\"y\":366},{\"x\":1163,\"y\":366},{\"x\":1164,\"y\":465}],\"text\":\"13-Apr-2018\"},{\"boundingBox\":[{\"x\":1190,\"y\":462},{\"x\":1189,\"y\":367},{\"x\":1209,\"y\":367},{\"x\":1209,\"y\":461}],\"text\":\"13-Jun-2018\"},{\"boundingBox\":[{\"x\":1235,\"y\":465},{\"x\":1235,\"y\":367},{\"x\":1255,\"y\":367},{\"x\":1256,\"y\":465}],\"text\":\"13-Aug-2018\"},{\"boundingBox\":[{\"x\":1281,\"y\":466},{\"x\":1279,\"y\":367},{\"x\":1298,\"y\":367},{\"x\":1298,\"y\":466}],\"text\":\"13-Oct-2018\"},{\"boundingBox\":[{\"x\":1326,\"y\":467},{\"x\":1325,\"y\":367},{\"x\":1344,\"y\":367},{\"x\":1343,\"y\":467}],\"text\":\"13-Dec-2018\"},{\"boundingBox\":[{\"x\":1373,\"y\":465},{\"x\":1372,\"y\":368},{\"x\":1392,\"y\":368},{\"x\":1394,\"y\":463}],\"text\":\"13-Feb-2019\"}]}",
        "{\"language\":\"en\",\"text\":\"Real World Applications and Services Application 1 Application 2 Application N Dynamic Updating and Management Recency Analysis and Layer 4 Mining Rule Updation Rule Discovery Contextual Rule-based Rule Layer 3 Preferences Learning Generalization Context Discretization Time Series Contextual Layer 2 Modeling Data Clustering Contextual Data Acquisition Smartphone External Layer 1 Logs Sensors Sources\",\"lines\":[{\"boundingBox\":[{\"x\":242,\"y\":22},{\"x\":859,\"y\":22},{\"x\":859,\"y\":53},{\"x\":242,\"y\":53}],\"text\":\"Real World Applications and Services\"},{\"boundingBox\":[{\"x\":89,\"y\":101},{\"x\":301,\"y\":99},{\"x\":302,\"y\":129},{\"x\":89,\"y\":133}],\"text\":\"Application 1\"},{\"boundingBox\":[{\"x\":430,\"y\":103},{\"x\":647,\"y\":101},{\"x\":647,\"y\":130},{\"x\":430,\"y\":133}],\"text\":\"Application 2\"},{\"boundingBox\":[{\"x\":762,\"y\":103},{\"x\":983,\"y\":101},{\"x\":983,\"y\":129},{\"x\":762,\"y\":132}],\"text\":\"Application N\"},{\"boundingBox\":[{\"x\":154,\"y\":234},{\"x\":753,\"y\":234},{\"x\":753,\"y\":265},{\"x\":154,\"y\":265}],\"text\":\"Dynamic Updating and Management\"},{\"boundingBox\":[{\"x\":46,\"y\":302},{\"x\":409,\"y\":302},{\"x\":409,\"y\":330},{\"x\":46,\"y\":331}],\"text\":\"Recency Analysis and\"},{\"boundingBox\":[{\"x\":900,\"y\":284},{\"x\":1026,\"y\":283},{\"x\":1027,\"y\":313},{\"x\":900,\"y\":314}],\"text\":\"Layer 4\"},{\"boundingBox\":[{\"x\":175,\"y\":334},{\"x\":284,\"y\":336},{\"x\":283,\"y\":365},{\"x\":175,\"y\":361}],\"text\":\"Mining\"},{\"boundingBox\":[{\"x\":520,\"y\":319},{\"x\":756,\"y\":320},{\"x\":756,\"y\":348},{\"x\":520,\"y\":347}],\"text\":\"Rule Updation\"},{\"boundingBox\":[{\"x\":337,\"y\":454},{\"x\":587,\"y\":456},{\"x\":587,\"y\":483},{\"x\":337,\"y\":480}],\"text\":\"Rule Discovery\"},{\"boundingBox\":[{\"x\":63,\"y\":521},{\"x\":241,\"y\":521},{\"x\":241,\"y\":547},{\"x\":63,\"y\":547}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":319,\"y\":520},{\"x\":507,\"y\":521},{\"x\":507,\"y\":547},{\"x\":319,\"y\":546}],\"text\":\"Rule-based\"},{\"boundingBox\":[{\"x\":660,\"y\":522},{\"x\":737,\"y\":522},{\"x\":737,\"y\":546},{\"x\":659,\"y\":545}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":902,\"y\":505},{\"x\":1031,\"y\":503},{\"x\":1031,\"y\":533},{\"x\":902,\"y\":535}],\"text\":\"Layer 3\"},{\"boundingBox\":[{\"x\":53,\"y\":552},{\"x\":254,\"y\":553},{\"x\":254,\"y\":581},{\"x\":53,\"y\":580}],\"text\":\"Preferences\"},{\"boundingBox\":[{\"x\":338,\"y\":553},{\"x\":481,\"y\":554},{\"x\":481,\"y\":582},{\"x\":337,\"y\":581}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":578,\"y\":553},{\"x\":816,\"y\":553},{\"x\":816,\"y\":579},{\"x\":578,\"y\":580}],\"text\":\"Generalization\"},{\"boundingBox\":[{\"x\":248,\"y\":665},{\"x\":605,\"y\":665},{\"x\":605,\"y\":693},{\"x\":248,\"y\":693}],\"text\":\"Context Discretization\"},{\"boundingBox\":[{\"x\":129,\"y\":730},{\"x\":325,\"y\":731},{\"x\":325,\"y\":759},{\"x\":129,\"y\":759}],\"text\":\"Time Series\"},{\"boundingBox\":[{\"x\":552,\"y\":734},{\"x\":732,\"y\":733},{\"x\":732,\"y\":759},{\"x\":552,\"y\":760}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":900,\"y\":713},{\"x\":1031,\"y\":712},{\"x\":1031,\"y\":743},{\"x\":900,\"y\":743}],\"text\":\"Layer 2\"},{\"boundingBox\":[{\"x\":152,\"y\":763},{\"x\":303,\"y\":766},{\"x\":302,\"y\":795},{\"x\":152,\"y\":790}],\"text\":\"Modeling\"},{\"boundingBox\":[{\"x\":514,\"y\":765},{\"x\":769,\"y\":766},{\"x\":769,\"y\":796},{\"x\":514,\"y\":794}],\"text\":\"Data Clustering\"},{\"boundingBox\":[{\"x\":203,\"y\":885},{\"x\":655,\"y\":885},{\"x\":655,\"y\":915},{\"x\":203,\"y\":915}],\"text\":\"Contextual Data Acquisition\"},{\"boundingBox\":[{\"x\":76,\"y\":958},{\"x\":278,\"y\":960},{\"x\":278,\"y\":988},{\"x\":76,\"y\":986}],\"text\":\"Smartphone\"},{\"boundingBox\":[{\"x\":633,\"y\":955},{\"x\":768,\"y\":955},{\"x\":768,\"y\":981},{\"x\":633,\"y\":981}],\"text\":\"External\"},{\"boundingBox\":[{\"x\":899,\"y\":932},{\"x\":1029,\"y\":932},{\"x\":1029,\"y\":962},{\"x\":899,\"y\":963}],\"text\":\"Layer 1\"},{\"boundingBox\":[{\"x\":136,\"y\":992},{\"x\":216,\"y\":994},{\"x\":216,\"y\":1020},{\"x\":136,\"y\":1018}],\"text\":\"Logs\"},{\"boundingBox\":[{\"x\":379,\"y\":974},{\"x\":519,\"y\":976},{\"x\":519,\"y\":1001},{\"x\":379,\"y\":999}],\"text\":\"Sensors\"},{\"boundingBox\":[{\"x\":635,\"y\":989},{\"x\":770,\"y\":989},{\"x\":770,\"y\":1012},{\"x\":635,\"y\":1011}],\"text\":\"Sources\"}],\"words\":[{\"boundingBox\":[{\"x\":243,\"y\":23},{\"x\":321,\"y\":23},{\"x\":321,\"y\":53},{\"x\":243,\"y\":52}],\"text\":\"Real\"},{\"boundingBox\":[{\"x\":327,\"y\":23},{\"x\":420,\"y\":22},{\"x\":420,\"y\":54},{\"x\":327,\"y\":53}],\"text\":\"World\"},{\"boundingBox\":[{\"x\":435,\"y\":22},{\"x\":630,\"y\":23},{\"x\":630,\"y\":54},{\"x\":435,\"y\":54}],\"text\":\"Applications\"},{\"boundingBox\":[{\"x\":643,\"y\":23},{\"x\":701,\"y\":23},{\"x\":701,\"y\":53},{\"x\":643,\"y\":53}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":713,\"y\":23},{\"x\":854,\"y\":25},{\"x\":854,\"y\":52},{\"x\":713,\"y\":53}],\"text\":\"Services\"},{\"boundingBox\":[{\"x\":91,\"y\":102},{\"x\":266,\"y\":100},{\"x\":266,\"y\":130},{\"x\":90,\"y\":134}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":283,\"y\":100},{\"x\":301,\"y\":100},{\"x\":301,\"y\":130},{\"x\":283,\"y\":130}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":434,\"y\":104},{\"x\":607,\"y\":102},{\"x\":607,\"y\":131},{\"x\":432,\"y\":133}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":623,\"y\":101},{\"x\":640,\"y\":101},{\"x\":640,\"y\":131},{\"x\":623,\"y\":131}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":763,\"y\":104},{\"x\":939,\"y\":102},{\"x\":939,\"y\":131},{\"x\":763,\"y\":133}],\"text\":\"Application\"},{\"boundingBox\":[{\"x\":952,\"y\":102},{\"x\":969,\"y\":102},{\"x\":969,\"y\":130},{\"x\":952,\"y\":131}],\"text\":\"N\"},{\"boundingBox\":[{\"x\":156,\"y\":236},{\"x\":295,\"y\":235},{\"x\":294,\"y\":266},{\"x\":154,\"y\":265}],\"text\":\"Dynamic\"},{\"boundingBox\":[{\"x\":305,\"y\":235},{\"x\":449,\"y\":235},{\"x\":448,\"y\":266},{\"x\":304,\"y\":266}],\"text\":\"Updating\"},{\"boundingBox\":[{\"x\":463,\"y\":235},{\"x\":520,\"y\":235},{\"x\":519,\"y\":266},{\"x\":462,\"y\":266}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":534,\"y\":235},{\"x\":753,\"y\":237},{\"x\":754,\"y\":265},{\"x\":534,\"y\":266}],\"text\":\"Management\"},{\"boundingBox\":[{\"x\":47,\"y\":304},{\"x\":186,\"y\":303},{\"x\":187,\"y\":332},{\"x\":49,\"y\":331}],\"text\":\"Recency\"},{\"boundingBox\":[{\"x\":197,\"y\":303},{\"x\":333,\"y\":302},{\"x\":333,\"y\":331},{\"x\":198,\"y\":332}],\"text\":\"Analysis\"},{\"boundingBox\":[{\"x\":346,\"y\":302},{\"x\":402,\"y\":302},{\"x\":402,\"y\":331},{\"x\":347,\"y\":331}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":901,\"y\":286},{\"x\":996,\"y\":285},{\"x\":996,\"y\":315},{\"x\":901,\"y\":314}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1003,\"y\":284},{\"x\":1019,\"y\":284},{\"x\":1019,\"y\":315},{\"x\":1002,\"y\":315}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":175,\"y\":335},{\"x\":276,\"y\":336},{\"x\":276,\"y\":365},{\"x\":175,\"y\":361}],\"text\":\"Mining\"},{\"boundingBox\":[{\"x\":522,\"y\":319},{\"x\":593,\"y\":320},{\"x\":592,\"y\":348},{\"x\":521,\"y\":347}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":606,\"y\":320},{\"x\":748,\"y\":320},{\"x\":748,\"y\":348},{\"x\":605,\"y\":348}],\"text\":\"Updation\"},{\"boundingBox\":[{\"x\":337,\"y\":455},{\"x\":408,\"y\":455},{\"x\":407,\"y\":481},{\"x\":337,\"y\":480}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":422,\"y\":456},{\"x\":582,\"y\":456},{\"x\":582,\"y\":484},{\"x\":421,\"y\":482}],\"text\":\"Discovery\"},{\"boundingBox\":[{\"x\":65,\"y\":522},{\"x\":241,\"y\":522},{\"x\":241,\"y\":547},{\"x\":63,\"y\":547}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":319,\"y\":520},{\"x\":497,\"y\":522},{\"x\":497,\"y\":548},{\"x\":319,\"y\":546}],\"text\":\"Rule-based\"},{\"boundingBox\":[{\"x\":660,\"y\":522},{\"x\":727,\"y\":523},{\"x\":727,\"y\":547},{\"x\":660,\"y\":546}],\"text\":\"Rule\"},{\"boundingBox\":[{\"x\":904,\"y\":507},{\"x\":994,\"y\":506},{\"x\":995,\"y\":535},{\"x\":903,\"y\":534}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1002,\"y\":505},{\"x\":1019,\"y\":504},{\"x\":1020,\"y\":534},{\"x\":1003,\"y\":535}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":54,\"y\":552},{\"x\":245,\"y\":555},{\"x\":246,\"y\":581},{\"x\":54,\"y\":581}],\"text\":\"Preferences\"},{\"boundingBox\":[{\"x\":338,\"y\":554},{\"x\":476,\"y\":555},{\"x\":476,\"y\":583},{\"x\":338,\"y\":582}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":579,\"y\":554},{\"x\":807,\"y\":553},{\"x\":807,\"y\":580},{\"x\":579,\"y\":580}],\"text\":\"Generalization\"},{\"boundingBox\":[{\"x\":250,\"y\":666},{\"x\":377,\"y\":666},{\"x\":376,\"y\":694},{\"x\":249,\"y\":694}],\"text\":\"Context\"},{\"boundingBox\":[{\"x\":382,\"y\":666},{\"x\":599,\"y\":666},{\"x\":598,\"y\":694},{\"x\":381,\"y\":694}],\"text\":\"Discretization\"},{\"boundingBox\":[{\"x\":131,\"y\":731},{\"x\":201,\"y\":732},{\"x\":202,\"y\":759},{\"x\":131,\"y\":760}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":217,\"y\":732},{\"x\":319,\"y\":731},{\"x\":319,\"y\":760},{\"x\":217,\"y\":759}],\"text\":\"Series\"},{\"boundingBox\":[{\"x\":555,\"y\":734},{\"x\":732,\"y\":733},{\"x\":731,\"y\":760},{\"x\":553,\"y\":761}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":902,\"y\":714},{\"x\":996,\"y\":714},{\"x\":995,\"y\":744},{\"x\":901,\"y\":743}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1002,\"y\":713},{\"x\":1019,\"y\":713},{\"x\":1019,\"y\":744},{\"x\":1002,\"y\":744}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":153,\"y\":763},{\"x\":293,\"y\":767},{\"x\":293,\"y\":795},{\"x\":153,\"y\":790}],\"text\":\"Modeling\"},{\"boundingBox\":[{\"x\":515,\"y\":765},{\"x\":587,\"y\":766},{\"x\":587,\"y\":794},{\"x\":514,\"y\":794}],\"text\":\"Data\"},{\"boundingBox\":[{\"x\":601,\"y\":766},{\"x\":762,\"y\":766},{\"x\":762,\"y\":797},{\"x\":600,\"y\":794}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":204,\"y\":887},{\"x\":380,\"y\":886},{\"x\":380,\"y\":916},{\"x\":204,\"y\":915}],\"text\":\"Contextual\"},{\"boundingBox\":[{\"x\":385,\"y\":886},{\"x\":462,\"y\":886},{\"x\":462,\"y\":916},{\"x\":386,\"y\":916}],\"text\":\"Data\"},{\"boundingBox\":[{\"x\":474,\"y\":886},{\"x\":648,\"y\":887},{\"x\":648,\"y\":915},{\"x\":474,\"y\":916}],\"text\":\"Acquisition\"},{\"boundingBox\":[{\"x\":76,\"y\":959},{\"x\":270,\"y\":961},{\"x\":270,\"y\":988},{\"x\":77,\"y\":986}],\"text\":\"Smartphone\"},{\"boundingBox\":[{\"x\":634,\"y\":956},{\"x\":768,\"y\":956},{\"x\":767,\"y\":981},{\"x\":634,\"y\":981}],\"text\":\"External\"},{\"boundingBox\":[{\"x\":899,\"y\":933},{\"x\":995,\"y\":934},{\"x\":996,\"y\":963},{\"x\":900,\"y\":963}],\"text\":\"Layer\"},{\"boundingBox\":[{\"x\":1005,\"y\":933},{\"x\":1022,\"y\":933},{\"x\":1024,\"y\":962},{\"x\":1006,\"y\":963}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":137,\"y\":993},{\"x\":212,\"y\":995},{\"x\":210,\"y\":1021},{\"x\":137,\"y\":1019}],\"text\":\"Logs\"},{\"boundingBox\":[{\"x\":380,\"y\":974},{\"x\":509,\"y\":977},{\"x\":510,\"y\":1001},{\"x\":380,\"y\":1000}],\"text\":\"Sensors\"},{\"boundingBox\":[{\"x\":636,\"y\":989},{\"x\":761,\"y\":990},{\"x\":761,\"y\":1013},{\"x\":635,\"y\":1012}],\"text\":\"Sources\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 31 October 2019\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":14},{\"x\":992,\"y\":14},{\"x\":992,\"y\":70},{\"x\":4,\"y\":70}],\"text\":\"Published online: 31 October 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":16},{\"x\":272,\"y\":15},{\"x\":272,\"y\":71},{\"x\":4,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":15},{\"x\":496,\"y\":15},{\"x\":495,\"y\":71},{\"x\":291,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":507,\"y\":15},{\"x\":579,\"y\":15},{\"x\":578,\"y\":71},{\"x\":506,\"y\":71}],\"text\":\"31\"},{\"boundingBox\":[{\"x\":594,\"y\":15},{\"x\":834,\"y\":15},{\"x\":833,\"y\":71},{\"x\":593,\"y\":71}],\"text\":\"October\"},{\"boundingBox\":[{\"x\":846,\"y\":15},{\"x\":987,\"y\":16},{\"x\":986,\"y\":70},{\"x\":845,\"y\":71}],\"text\":\"2019\"}]}"
      ]
    },
    {
      "@search.score": 8.037484,
      "content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchic",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNDkzLTAxNS0wMDE1LTMucGRm0",
      "metadata_author": null,
      "metadata_title": null,
      "people": [
        "Sänger",
        "Johannes Sänger",
        "Christian Richthammer",
        "Günther Pernul",
        "johannes.saenger",
        "Hevner",
        "Gambetta",
        "Alice",
        "Bob",
        "Claire",
        "Christianson",
        "Harbison",
        "Rachel",
        "Rehak",
        "Zhang",
        "Sporas",
        "Whitby",
        "Dellarocas",
        "Epinions",
        "Sun",
        "Jøsang",
        "Dempster",
        "Shafer",
        "Yu",
        "Singh",
        "Malik"
      ],
      "keyphrases": [
        "Creative Commons Attribution License",
        "mobile ad hoc networks",
        "Günther Pernul",
        "Universitätsstraße",
        "descriptive scenario-based analysis",
        "general prob- lem",
        "one novel idea",
        "Open Access article",
        "single building blocks",
        "RESEARCH Open Access",
        "online reputation systems",
        "new reputation systems",
        "hierarchical component taxonomy",
        "most reputation systems",
        "common reputation models",
        "Johannes Sänger",
        "hierarchical taxonomy",
        "common systems",
        "peer networks",
        "social networks",
        "component repository",
        "common models",
        "Christian Richthammer",
        "various disciplines",
        "application areas",
        "promising approaches",
        "implementation level",
        "obvious utility",
        "practical point",
        "last decade",
        "wide range",
        "considerable research",
        "data accuracy",
        "several environments",
        "statistical approaches",
        "personal preferences",
        "software engineering",
        "unrestricted use",
        "effective use",
        "computation engines",
        "computation phase",
        "graph-based models",
        "natural framework",
        "design process",
        "design knowledge",
        "Design approaches",
        "computation methods",
        "Reusable components",
        "Trust Management",
        "Trust pattern",
        "reputation-based trust",
        "context information",
        "original work",
        "Journal",
        "DOI",
        "Correspondence",
        "saenger",
        "regensburg",
        "University",
        "Germany",
        "Abstract",
        "problem",
        "scratch",
        "concepts",
        "achievements",
        "others",
        "shuffle",
        "reuse",
        "respect",
        "order",
        "conceptual",
        "view",
        "properties",
        "literature",
        "aspects",
        "Keywords",
        "Reusability",
        "Introduction",
        "commerce",
        "eBay",
        "buyers",
        "relevance",
        "quality",
        "arithmetic",
        "multiple",
        "factors",
        "propagation",
        "solutions",
        "authors",
        "proposals",
        "development",
        "attention",
        "licensee",
        "Springer",
        "terms",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "mailto",
        "wiwi",
        "Page",
        "community",
        "specialists",
        "reliability",
        "goal",
        "classification",
        "functions",
        "many applied computation techniques",
        "design science research paradigm",
        "design pattern-like solution",
        "rational choice mechanism",
        "general problem context",
        "multifaceted terms trust",
        "computational trust models",
        "current models",
        "reputation models",
        "building block",
        "web services",
        "research gap",
        "objec- tives",
        "same time",
        "increasing distribution",
        "decision making",
        "network environments",
        "common understanding",
        "various fields",
        "multidimension- ality",
        "abstract concept",
        "behavioral dimension",
        "interpersonal phenomenon",
        "particular level",
        "subjective probability",
        "Sänger",
        "Multiple authors",
        "reputation systems",
        "conceptual level",
        "following section",
        "important artifacts",
        "particular action",
        "future work",
        "several properties",
        "means",
        "rest",
        "paper",
        "guidelines",
        "Hevner",
        "overview",
        "motivation",
        "approach",
        "contribution",
        "name",
        "plans",
        "success",
        "Internet",
        "connectivity",
        "nition",
        "notion",
        "regard",
        "objectives",
        "topic",
        "decades",
        "uniform",
        "Reasons",
        "circumstance",
        "credibility",
        "confidence",
        "emotional",
        "nature",
        "sociologists",
        "psychologists",
        "Economists",
        "online",
        "Gambetta",
        "distrust",
        "agent",
        "group",
        "capacity",
        "security",
        "risk",
        "variety",
        "Table",
        "basis",
        "two common ways",
        "multiple discrete levels",
        "hard security mechanism",
        "one aggregated picture",
        "One negative experience",
        "Propagative One property",
        "many trust models",
        "multiple trust chains",
        "several trust models",
        "several models",
        "propagative nature",
        "different chains",
        "recent years",
        "new experiences",
        "time-based aging",
        "greater importance",
        "old experiences",
        "delicious meal",
        "rela- tionships",
        "final decision",
        "greater influence",
        "distance-based aging",
        "subjective nature",
        "high level",
        "binary manner",
        "human nature",
        "hard evidence",
        "one agent",
        "other agents",
        "untrustworthy agents",
        "maximum value",
        "Dynamic Trust",
        "overall trust",
        "trust information",
        "trust statements",
        "lower trust",
        "Reflexive Trust",
        "specific context",
        "particular restaurant",
        "specific aspects",
        "long time",
        "particular agent",
        "distant nodes",
        "reviewer Rachel",
        "book review",
        "trust value",
        "trust transitivity",
        "Policy-based trust",
        "same context",
        "values",
        "Table 1",
        "Overview",
        "acteristics",
        "example",
        "Alice",
        "Bob",
        "doctor",
        "cook",
        "customer",
        "food",
        "service",
        "combination",
        "amount",
        "use",
        "propagativity",
        "turn",
        "Claire",
        "Christianson",
        "Harbison",
        "transitive",
        "reverse",
        "case",
        "Composable",
        "Composition",
        "zon",
        "opinion",
        "numbers",
        "continuous",
        "variable",
        "interval",
        "reinforcing",
        "trustworthiness",
        "consideration",
        "establishment",
        "exchange",
        "credentials",
        "contrast",
        "history",
        "interactions",
        "estimation",
        "most reputation-based trust models",
        "repu- tation system",
        "three fundamental phases",
        "function-based component classification",
        "current reputation models",
        "three process steps",
        "existing trust models",
        "existing models",
        "three steps",
        "component classes",
        "generic process",
        "existing approaches",
        "existing systems",
        "soft security",
        "collective measure",
        "global value",
        "personal experience",
        "many approvedmodels",
        "promising thoughts",
        "generic mechanism",
        "compo- nent",
        "accepted principles",
        "one hand",
        "other hand",
        "two steps",
        "critical question",
        "feedback generation/collection",
        "feedback distribution",
        "central part",
        "common trust",
        "feedback aggregation",
        "existing concepts",
        "academic community",
        "context setting",
        "single components",
        "reusable components",
        "trust properties",
        "Research gap",
        "worthiness",
        "work",
        "thing",
        "character",
        "referrals",
        "ratings",
        "reviews",
        "members",
        "someone",
        "ideas",
        "benefits",
        "years",
        "Rehak",
        "instance",
        "capabilities",
        "sound",
        "new",
        "repository",
        "artifacts",
        "developers",
        "researchers",
        "novel",
        "way",
        "implemented",
        "communication",
        "ability",
        "table",
        "computationmethods",
        "analysis",
        "identification",
        "logical",
        "Figure",
        "preparation",
        "storage",
        "weighting",
        "trustor",
        "novel hierarchical component taxonomy",
        "common rep- utation systems",
        "three generic process steps",
        "two broad categories",
        "reputa- tion systems",
        "subsequent aggre- gation",
        "current trust models",
        "second process step",
        "single process steps",
        "Common reputation systems",
        "first two steps",
        "several reputation scores",
        "common examples",
        "subsequent computing",
        "utation information",
        "second case",
        "computation process",
        "trust relation",
        "first phase",
        "reputation value",
        "personalization parameters",
        "past behavior",
        "sonal experience",
        "other sources",
        "personal collections",
        "ferent peers",
        "distributed network",
        "different sources",
        "uniform format",
        "specific situation",
        "weight factors",
        "decentralized environments",
        "functional blocks",
        "different surveys",
        "computation engine",
        "last phase",
        "first question",
        "filtering process",
        "hard selection",
        "soft selection",
        "secondary classes",
        "reputation data",
        "information amount",
        "extra information",
        "available data",
        "Preparation techniques",
        "preparation phase",
        "Figure 2 Classes",
        "public storage",
        "input data",
        "trustee",
        "context",
        "normalization",
        "output",
        "aggregation",
        "need",
        "processing",
        "line",
        "Zhang",
        "difference",
        "algo",
        "rithm",
        "structure",
        "decentralized/hybrid",
        "meaning",
        "transparency",
        "design",
        "section",
        "detail",
        "primary",
        "three broad classes attribute-based, statistic- based",
        "simple ballot stuffing attacks",
        "payment Sänger",
        "statistical filter technique",
        "one application area",
        "value imbalance problem",
        "Bayesian reputation systems",
        "cluster analysis approaches",
        "following classes",
        "Attribute-based filtering",
        "Attribute-based filters",
        "simple logic",
        "statistical patterns",
        "Clustering-based filter",
        "filtering step",
        "Other models",
        "filtering phase",
        "single attributes",
        "reference value",
        "huge amounts",
        "initial filtering",
        "weighting phase",
        "last 12 months",
        "positive, neutral",
        "Statistic-based filtering",
        "false rumors",
        "exemplary procedure",
        "cluster filtering",
        "available information",
        "current situation",
        "various data",
        "different factors",
        "different prices",
        "product types",
        "malicious seller",
        "cheap products",
        "many current",
        "reputation calculation",
        "Reputation data",
        "high reputation",
        "negative ratings",
        "positive ratings",
        "unfair ratings",
        "false ratings",
        "Further techniques",
        "majority rule",
        "filtering techniques",
        "weighting techniques",
        "false feedback",
        "other parties",
        "common experience",
        "fake transactions",
        "obvious way",
        "crucial attribute",
        "Context comparability",
        "constraint-factor",
        "lightweight",
        "Time",
        "Sporas",
        "party",
        "robustness",
        "spread",
        "Whitby",
        "dishonest",
        "advisor",
        "rater",
        "iCLUB",
        "clusters",
        "evaluations",
        "bootstrapping",
        "Dellarocas",
        "characteristics",
        "reason",
        "discounting",
        "methods",
        "non",
        "expensive",
        "situations",
        "1.",
        "several trust/reputa- tion values",
        "biometric identity trust model",
        "human decision makers",
        "adaptive forgetting scheme",
        "distinct information sources",
        "common aggregation techniques",
        "one important factor",
        "different application areas",
        "simple aggregation techniques",
        "low feedback reputation",
        "reputation values",
        "forgetting factor",
        "low impact",
        "Simple arithmetic",
        "critical factor",
        "first-hand information",
        "Different actors",
        "different perceptions",
        "aggregation process",
        "aggregated reputation",
        "good reputation",
        "reputation scoring",
        "Reputation systems",
        "file-sharing networks",
        "successful transaction",
        "customer satisfaction",
        "network structures",
        "transitivity) rate",
        "honest feedback",
        "recursive algorithm",
        "stronger punishment",
        "bad behavior",
        "good behavior",
        "dynamic nature",
        "Old feedback",
        "Jøsang",
        "Personal preferences",
        "various end-users",
        "direct experience",
        "weight-factor(s",
        "third step",
        "iterative manner",
        "single steps",
        "first class",
        "Criteria comparability",
        "different communities",
        "Rating value",
        "trust chain",
        "Other distinctions",
        "propagation degree",
        "web graph",
        "example measure",
        "time-based weighting",
        "new referrals",
        "agents’ reliability",
        "approaches",
        "evaluation",
        "costs",
        "level",
        "anonymity",
        "number",
        "peers",
        "differences",
        "recommendation",
        "second",
        "distance",
        "reputation-factor",
        "nodes",
        "honesty",
        "concept",
        "consequence",
        "bases",
        "calculation",
        "Google",
        "PageRank",
        "position",
        "website",
        "Epinions",
        "reviewers",
        "effects",
        "Sun",
        "adaptation",
        "importance",
        "significance",
        "newcomers",
        "tuple",
        "input",
        "phase",
        "cases",
        "list",
        "algorithms",
        "fuzzy",
        "2.",
        "4.",
        "5.",
        "beta PDF function parameter tuple",
        "human decision making process",
        "beta probability density function",
        "solid mathematical basis",
        "More complex solutions",
        "The beta PDF",
        "Beta Reputation system",
        "exact reputation score",
        "priori reputation score",
        "online social networks",
        "Other aggregation techniques",
        "Bayesian probabilistic models",
        "Hidden Markov Model",
        "graph-based flow model",
        "prominent trust models",
        "beta model",
        "Bayesian probability",
        "probability distributions",
        "probabilistic approach",
        "Markov Models",
        "graph-based approach",
        "belief model",
        "model truth",
        "Reputation values",
        "overall reputation",
        "basic way",
        "descending order",
        "message boards",
        "citation counts",
        "academic literature",
        "understandable algorithm",
        "last years",
        "statistical approach",
        "Applied techniques",
        "binary events",
        "Dempster-Shafer theory",
        "subjective logic",
        "machine learning",
        "dynamic behavior",
        "classical logic",
        "linguistic approach",
        "prominent example",
        "graph theory",
        "incoming edges",
        "outgoing edges",
        "several factors",
        "Popular algorithms",
        "Eigentrust Algorithm",
        "many models",
        "fuzzy logic",
        "trust management",
        "transitive trust",
        "trust chains",
        "fuzzy models",
        "new ratings",
        "Other examples",
        "ranking algorithms",
        "impact factor",
        "different measures",
        "one edge",
        "two nodes",
        "summation",
        "average",
        "proxy",
        "systems",
        "Slashdot",
        "implementation",
        "Amazon",
        "positive",
        "composition",
        "result",
        "weakness",
        "uncertainty",
        "DST",
        "Yu",
        "Singh",
        "generalization",
        "Malik",
        "falsity",
        "degree",
        "agent/resource",
        "REGRET",
        "information",
        "people",
        "field",
        "centrality",
        "Eigenvector",
        "α",
        "β",
        "reputa- tion system",
        "single logical components",
        "aggregation phases",
        "incremental nature",
        "one component",
        "primary class",
        "aggregation techniques",
        "hybrid model",
        "new classes",
        "novel models",
        "taxonomy",
        "heuristic",
        "knowledge",
        "extension",
        "hierarchic"
      ],
      "merged_content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n reputation system computation collection & storage & preparation filtering weighting aggregation communication Bo input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchic",
      "text": [
        "reputation system computation collection & storage & preparation filtering weighting aggregation communication Bo input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation",
        "filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences",
        "context comparability attribute-based simple arithmetic criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level) graph-based personal preferences knowledge repository (conceptual level). filtering weighting aggregation",
        "Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract ... CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute .. TimeDiscounting Relative",
        "Filter: Weighting: referral reduced context similarity weighted Aggregation: set age-based reputation filter (absolut) referral (absolute referral set set average value congruence)",
        "Published online: 13 May 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"reputation system computation collection & storage & preparation filtering weighting aggregation communication Bo input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation\",\"lines\":[{\"boundingBox\":[{\"x\":1077,\"y\":9},{\"x\":1241,\"y\":9},{\"x\":1241,\"y\":32},{\"x\":1077,\"y\":32}],\"text\":\"reputation system\"},{\"boundingBox\":[{\"x\":612,\"y\":94},{\"x\":745,\"y\":94},{\"x\":745,\"y\":124},{\"x\":612,\"y\":124}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":181,\"y\":148},{\"x\":306,\"y\":147},{\"x\":306,\"y\":168},{\"x\":181,\"y\":170}],\"text\":\"collection &\"},{\"boundingBox\":[{\"x\":1067,\"y\":147},{\"x\":1172,\"y\":147},{\"x\":1172,\"y\":171},{\"x\":1067,\"y\":171}],\"text\":\"storage &\"},{\"boundingBox\":[{\"x\":183,\"y\":178},{\"x\":304,\"y\":177},{\"x\":305,\"y\":201},{\"x\":183,\"y\":203}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":443,\"y\":170},{\"x\":523,\"y\":173},{\"x\":522,\"y\":198},{\"x\":443,\"y\":194}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":634,\"y\":170},{\"x\":742,\"y\":170},{\"x\":742,\"y\":199},{\"x\":634,\"y\":200}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":816,\"y\":173},{\"x\":945,\"y\":172},{\"x\":946,\"y\":195},{\"x\":816,\"y\":197}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1038,\"y\":177},{\"x\":1200,\"y\":177},{\"x\":1200,\"y\":199},{\"x\":1038,\"y\":199}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":22,\"y\":212},{\"x\":23,\"y\":134},{\"x\":72,\"y\":133},{\"x\":72,\"y\":210}],\"text\":\"Bo\"},{\"boundingBox\":[{\"x\":8,\"y\":425},{\"x\":56,\"y\":425},{\"x\":55,\"y\":446},{\"x\":8,\"y\":445}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":259,\"y\":415},{\"x\":479,\"y\":414},{\"x\":479,\"y\":438},{\"x\":259,\"y\":439}],\"text\":\"context, personalisation\"},{\"boundingBox\":[{\"x\":617,\"y\":417},{\"x\":734,\"y\":417},{\"x\":734,\"y\":435},{\"x\":617,\"y\":435}],\"text\":\"trust relation\"},{\"boundingBox\":[{\"x\":918,\"y\":416},{\"x\":1096,\"y\":415},{\"x\":1096,\"y\":438},{\"x\":918,\"y\":440}],\"text\":\"reputation value(s)\"},{\"boundingBox\":[{\"x\":1219,\"y\":424},{\"x\":1280,\"y\":422},{\"x\":1280,\"y\":442},{\"x\":1219,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":490,\"y\":473},{\"x\":552,\"y\":473},{\"x\":552,\"y\":491},{\"x\":490,\"y\":491}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":802,\"y\":476},{\"x\":868,\"y\":477},{\"x\":868,\"y\":494},{\"x\":802,\"y\":493}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1059,\"y\":467},{\"x\":1244,\"y\":467},{\"x\":1244,\"y\":488},{\"x\":1059,\"y\":489}],\"text\":\"transaction/situation\"}],\"words\":[{\"boundingBox\":[{\"x\":1078,\"y\":10},{\"x\":1167,\"y\":10},{\"x\":1167,\"y\":32},{\"x\":1077,\"y\":33}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1172,\"y\":10},{\"x\":1233,\"y\":10},{\"x\":1233,\"y\":33},{\"x\":1172,\"y\":32}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":613,\"y\":95},{\"x\":744,\"y\":95},{\"x\":746,\"y\":124},{\"x\":612,\"y\":125}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":181,\"y\":149},{\"x\":279,\"y\":148},{\"x\":280,\"y\":169},{\"x\":181,\"y\":171}],\"text\":\"collection\"},{\"boundingBox\":[{\"x\":288,\"y\":148},{\"x\":302,\"y\":148},{\"x\":303,\"y\":169},{\"x\":289,\"y\":169}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":1068,\"y\":148},{\"x\":1147,\"y\":148},{\"x\":1148,\"y\":172},{\"x\":1067,\"y\":172}],\"text\":\"storage\"},{\"boundingBox\":[{\"x\":1154,\"y\":148},{\"x\":1169,\"y\":147},{\"x\":1170,\"y\":172},{\"x\":1154,\"y\":172}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":184,\"y\":179},{\"x\":303,\"y\":178},{\"x\":303,\"y\":201},{\"x\":184,\"y\":204}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":444,\"y\":171},{\"x\":520,\"y\":173},{\"x\":521,\"y\":198},{\"x\":443,\"y\":195}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":635,\"y\":171},{\"x\":740,\"y\":170},{\"x\":740,\"y\":199},{\"x\":634,\"y\":200}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":817,\"y\":174},{\"x\":942,\"y\":173},{\"x\":943,\"y\":196},{\"x\":816,\"y\":198}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1039,\"y\":179},{\"x\":1197,\"y\":177},{\"x\":1197,\"y\":199},{\"x\":1038,\"y\":200}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":22,\"y\":189},{\"x\":22,\"y\":138},{\"x\":72,\"y\":138},{\"x\":72,\"y\":190}],\"text\":\"Bo\"},{\"boundingBox\":[{\"x\":8,\"y\":425},{\"x\":56,\"y\":425},{\"x\":55,\"y\":446},{\"x\":8,\"y\":445}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":261,\"y\":417},{\"x\":332,\"y\":417},{\"x\":333,\"y\":439},{\"x\":261,\"y\":440}],\"text\":\"context,\"},{\"boundingBox\":[{\"x\":337,\"y\":417},{\"x\":477,\"y\":415},{\"x\":478,\"y\":439},{\"x\":337,\"y\":439}],\"text\":\"personalisation\"},{\"boundingBox\":[{\"x\":620,\"y\":417},{\"x\":661,\"y\":417},{\"x\":660,\"y\":436},{\"x\":619,\"y\":435}],\"text\":\"trust\"},{\"boundingBox\":[{\"x\":664,\"y\":417},{\"x\":731,\"y\":417},{\"x\":731,\"y\":436},{\"x\":664,\"y\":436}],\"text\":\"relation\"},{\"boundingBox\":[{\"x\":920,\"y\":418},{\"x\":1012,\"y\":416},{\"x\":1011,\"y\":440},{\"x\":918,\"y\":439}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1016,\"y\":416},{\"x\":1095,\"y\":416},{\"x\":1095,\"y\":439},{\"x\":1016,\"y\":440}],\"text\":\"value(s)\"},{\"boundingBox\":[{\"x\":1220,\"y\":424},{\"x\":1281,\"y\":423},{\"x\":1280,\"y\":443},{\"x\":1220,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":492,\"y\":474},{\"x\":552,\"y\":474},{\"x\":551,\"y\":492},{\"x\":491,\"y\":491}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":802,\"y\":477},{\"x\":863,\"y\":477},{\"x\":863,\"y\":495},{\"x\":802,\"y\":493}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1059,\"y\":469},{\"x\":1241,\"y\":468},{\"x\":1241,\"y\":489},{\"x\":1059,\"y\":489}],\"text\":\"transaction/situation\"}]}",
        "{\"language\":\"en\",\"text\":\"filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences\",\"lines\":[{\"boundingBox\":[{\"x\":177,\"y\":14},{\"x\":314,\"y\":19},{\"x\":313,\"y\":61},{\"x\":176,\"y\":56}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":639,\"y\":16},{\"x\":820,\"y\":18},{\"x\":820,\"y\":62},{\"x\":639,\"y\":60}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1070,\"y\":21},{\"x\":1292,\"y\":17},{\"x\":1292,\"y\":58},{\"x\":1070,\"y\":63}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":557,\"y\":130},{\"x\":868,\"y\":129},{\"x\":868,\"y\":166},{\"x\":557,\"y\":166}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":1054,\"y\":179},{\"x\":1307,\"y\":178},{\"x\":1307,\"y\":212},{\"x\":1054,\"y\":213}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":136,\"y\":209},{\"x\":358,\"y\":210},{\"x\":358,\"y\":238},{\"x\":136,\"y\":238}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":566,\"y\":250},{\"x\":858,\"y\":251},{\"x\":858,\"y\":286},{\"x\":566,\"y\":285}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":552,\"y\":371},{\"x\":877,\"y\":371},{\"x\":877,\"y\":406},{\"x\":552,\"y\":406}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1125,\"y\":396},{\"x\":1238,\"y\":395},{\"x\":1238,\"y\":426},{\"x\":1125,\"y\":428}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":138,\"y\":506},{\"x\":354,\"y\":506},{\"x\":354,\"y\":536},{\"x\":138,\"y\":536}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":645,\"y\":494},{\"x\":778,\"y\":495},{\"x\":778,\"y\":530},{\"x\":645,\"y\":529}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":626,\"y\":624},{\"x\":801,\"y\":624},{\"x\":801,\"y\":654},{\"x\":626,\"y\":655}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1142,\"y\":598},{\"x\":1220,\"y\":604},{\"x\":1216,\"y\":646},{\"x\":1137,\"y\":642}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":680,\"y\":749},{\"x\":746,\"y\":750},{\"x\":746,\"y\":779},{\"x\":679,\"y\":778}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":125,\"y\":799},{\"x\":368,\"y\":798},{\"x\":368,\"y\":831},{\"x\":125,\"y\":832}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1084,\"y\":824},{\"x\":1273,\"y\":821},{\"x\":1274,\"y\":854},{\"x\":1084,\"y\":859}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":556,\"y\":873},{\"x\":870,\"y\":870},{\"x\":870,\"y\":903},{\"x\":556,\"y\":906}],\"text\":\"personal preferences\"}],\"words\":[{\"boundingBox\":[{\"x\":179,\"y\":15},{\"x\":314,\"y\":19},{\"x\":313,\"y\":62},{\"x\":176,\"y\":57}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":641,\"y\":17},{\"x\":818,\"y\":20},{\"x\":817,\"y\":63},{\"x\":639,\"y\":61}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1074,\"y\":22},{\"x\":1287,\"y\":18},{\"x\":1289,\"y\":59},{\"x\":1073,\"y\":63}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":557,\"y\":131},{\"x\":663,\"y\":131},{\"x\":663,\"y\":165},{\"x\":557,\"y\":166}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":669,\"y\":131},{\"x\":868,\"y\":130},{\"x\":869,\"y\":167},{\"x\":670,\"y\":165}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":1054,\"y\":180},{\"x\":1150,\"y\":180},{\"x\":1150,\"y\":213},{\"x\":1055,\"y\":214}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1157,\"y\":180},{\"x\":1306,\"y\":178},{\"x\":1306,\"y\":213},{\"x\":1158,\"y\":213}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":137,\"y\":210},{\"x\":353,\"y\":210},{\"x\":353,\"y\":239},{\"x\":136,\"y\":239}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":566,\"y\":251},{\"x\":667,\"y\":251},{\"x\":667,\"y\":285},{\"x\":566,\"y\":285}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":674,\"y\":251},{\"x\":859,\"y\":251},{\"x\":859,\"y\":287},{\"x\":674,\"y\":285}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":553,\"y\":371},{\"x\":873,\"y\":371},{\"x\":874,\"y\":407},{\"x\":553,\"y\":407}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1126,\"y\":397},{\"x\":1236,\"y\":395},{\"x\":1236,\"y\":427},{\"x\":1126,\"y\":428}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":140,\"y\":507},{\"x\":349,\"y\":507},{\"x\":350,\"y\":537},{\"x\":139,\"y\":537}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":647,\"y\":495},{\"x\":777,\"y\":495},{\"x\":776,\"y\":531},{\"x\":646,\"y\":528}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":627,\"y\":624},{\"x\":706,\"y\":625},{\"x\":706,\"y\":656},{\"x\":626,\"y\":656}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":717,\"y\":625},{\"x\":795,\"y\":625},{\"x\":795,\"y\":654},{\"x\":717,\"y\":656}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1139,\"y\":598},{\"x\":1218,\"y\":603},{\"x\":1215,\"y\":647},{\"x\":1137,\"y\":642}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":680,\"y\":749},{\"x\":742,\"y\":750},{\"x\":741,\"y\":779},{\"x\":680,\"y\":778}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":125,\"y\":800},{\"x\":366,\"y\":799},{\"x\":366,\"y\":831},{\"x\":126,\"y\":833}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1087,\"y\":825},{\"x\":1271,\"y\":822},{\"x\":1271,\"y\":855},{\"x\":1086,\"y\":860}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":557,\"y\":874},{\"x\":684,\"y\":872},{\"x\":685,\"y\":904},{\"x\":557,\"y\":906}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":691,\"y\":872},{\"x\":868,\"y\":871},{\"x\":868,\"y\":904},{\"x\":691,\"y\":904}],\"text\":\"preferences\"}]}",
        "{\"language\":\"en\",\"text\":\"context comparability attribute-based simple arithmetic criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level) graph-based personal preferences knowledge repository (conceptual level). filtering weighting aggregation\",\"lines\":[{\"boundingBox\":[{\"x\":806,\"y\":101},{\"x\":1020,\"y\":101},{\"x\":1020,\"y\":125},{\"x\":806,\"y\":125}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":565,\"y\":160},{\"x\":721,\"y\":160},{\"x\":721,\"y\":182},{\"x\":565,\"y\":183}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":1097,\"y\":140},{\"x\":1269,\"y\":139},{\"x\":1269,\"y\":163},{\"x\":1097,\"y\":164}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":813,\"y\":193},{\"x\":1013,\"y\":193},{\"x\":1013,\"y\":218},{\"x\":813,\"y\":218}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":801,\"y\":286},{\"x\":1025,\"y\":287},{\"x\":1025,\"y\":312},{\"x\":801,\"y\":311}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1144,\"y\":308},{\"x\":1222,\"y\":307},{\"x\":1222,\"y\":329},{\"x\":1144,\"y\":331}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":567,\"y\":379},{\"x\":716,\"y\":379},{\"x\":716,\"y\":400},{\"x\":567,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":866,\"y\":377},{\"x\":958,\"y\":377},{\"x\":958,\"y\":404},{\"x\":866,\"y\":404}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":853,\"y\":474},{\"x\":970,\"y\":473},{\"x\":970,\"y\":496},{\"x\":853,\"y\":496}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1157,\"y\":471},{\"x\":1212,\"y\":474},{\"x\":1211,\"y\":495},{\"x\":1155,\"y\":491}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":890,\"y\":567},{\"x\":935,\"y\":567},{\"x\":935,\"y\":588},{\"x\":891,\"y\":585}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":559,\"y\":602},{\"x\":724,\"y\":602},{\"x\":724,\"y\":625},{\"x\":559,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":646},{\"x\":401,\"y\":645},{\"x\":401,\"y\":670},{\"x\":0,\"y\":671}],\"text\":\"service repository (implementation level)\"},{\"boundingBox\":[{\"x\":1120,\"y\":631},{\"x\":1247,\"y\":626},{\"x\":1248,\"y\":650},{\"x\":1120,\"y\":655}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":662},{\"x\":1020,\"y\":661},{\"x\":1020,\"y\":685},{\"x\":806,\"y\":686}],\"text\":\"personal preferences\"},{\"boundingBox\":[{\"x\":41,\"y\":688},{\"x\":445,\"y\":688},{\"x\":445,\"y\":715},{\"x\":41,\"y\":714}],\"text\":\"knowledge repository (conceptual level).\"},{\"boundingBox\":[{\"x\":615,\"y\":765},{\"x\":687,\"y\":767},{\"x\":686,\"y\":786},{\"x\":614,\"y\":783}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":862,\"y\":765},{\"x\":961,\"y\":766},{\"x\":961,\"y\":786},{\"x\":862,\"y\":786}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1115,\"y\":767},{\"x\":1236,\"y\":767},{\"x\":1236,\"y\":786},{\"x\":1115,\"y\":786}],\"text\":\"aggregation\"}],\"words\":[{\"boundingBox\":[{\"x\":807,\"y\":101},{\"x\":878,\"y\":101},{\"x\":878,\"y\":126},{\"x\":807,\"y\":125}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":883,\"y\":101},{\"x\":1020,\"y\":101},{\"x\":1020,\"y\":126},{\"x\":883,\"y\":126}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":566,\"y\":162},{\"x\":714,\"y\":160},{\"x\":714,\"y\":182},{\"x\":566,\"y\":183}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":1098,\"y\":141},{\"x\":1163,\"y\":141},{\"x\":1163,\"y\":165},{\"x\":1098,\"y\":165}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1168,\"y\":141},{\"x\":1269,\"y\":140},{\"x\":1269,\"y\":164},{\"x\":1168,\"y\":165}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":814,\"y\":194},{\"x\":880,\"y\":194},{\"x\":880,\"y\":218},{\"x\":813,\"y\":218}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":884,\"y\":194},{\"x\":1013,\"y\":193},{\"x\":1013,\"y\":219},{\"x\":884,\"y\":218}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":802,\"y\":288},{\"x\":1021,\"y\":287},{\"x\":1022,\"y\":313},{\"x\":803,\"y\":311}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1145,\"y\":309},{\"x\":1221,\"y\":308},{\"x\":1221,\"y\":330},{\"x\":1145,\"y\":331}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":569,\"y\":380},{\"x\":712,\"y\":379},{\"x\":712,\"y\":401},{\"x\":569,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":867,\"y\":377},{\"x\":958,\"y\":379},{\"x\":956,\"y\":403},{\"x\":867,\"y\":404}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":854,\"y\":475},{\"x\":910,\"y\":474},{\"x\":910,\"y\":497},{\"x\":854,\"y\":496}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":916,\"y\":474},{\"x\":970,\"y\":474},{\"x\":970,\"y\":496},{\"x\":916,\"y\":497}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1157,\"y\":471},{\"x\":1211,\"y\":474},{\"x\":1210,\"y\":495},{\"x\":1156,\"y\":491}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":891,\"y\":567},{\"x\":933,\"y\":567},{\"x\":932,\"y\":588},{\"x\":890,\"y\":586}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":559,\"y\":603},{\"x\":724,\"y\":602},{\"x\":724,\"y\":626},{\"x\":559,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":649},{\"x\":69,\"y\":648},{\"x\":69,\"y\":670},{\"x\":0,\"y\":669}],\"text\":\"service\"},{\"boundingBox\":[{\"x\":73,\"y\":648},{\"x\":175,\"y\":646},{\"x\":175,\"y\":671},{\"x\":73,\"y\":670}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":180,\"y\":646},{\"x\":339,\"y\":646},{\"x\":340,\"y\":671},{\"x\":180,\"y\":671}],\"text\":\"(implementation\"},{\"boundingBox\":[{\"x\":344,\"y\":646},{\"x\":400,\"y\":646},{\"x\":400,\"y\":669},{\"x\":345,\"y\":670}],\"text\":\"level)\"},{\"boundingBox\":[{\"x\":1121,\"y\":633},{\"x\":1245,\"y\":626},{\"x\":1246,\"y\":651},{\"x\":1121,\"y\":655}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":663},{\"x\":892,\"y\":662},{\"x\":892,\"y\":687},{\"x\":806,\"y\":687}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":897,\"y\":662},{\"x\":1019,\"y\":662},{\"x\":1019,\"y\":686},{\"x\":897,\"y\":687}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":43,\"y\":689},{\"x\":149,\"y\":689},{\"x\":148,\"y\":714},{\"x\":41,\"y\":712}],\"text\":\"knowledge\"},{\"boundingBox\":[{\"x\":154,\"y\":689},{\"x\":255,\"y\":689},{\"x\":255,\"y\":715},{\"x\":153,\"y\":715}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":260,\"y\":689},{\"x\":377,\"y\":689},{\"x\":377,\"y\":715},{\"x\":260,\"y\":715}],\"text\":\"(conceptual\"},{\"boundingBox\":[{\"x\":382,\"y\":689},{\"x\":444,\"y\":688},{\"x\":445,\"y\":714},{\"x\":382,\"y\":715}],\"text\":\"level).\"},{\"boundingBox\":[{\"x\":615,\"y\":766},{\"x\":685,\"y\":768},{\"x\":684,\"y\":787},{\"x\":615,\"y\":784}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":863,\"y\":766},{\"x\":958,\"y\":766},{\"x\":958,\"y\":787},{\"x\":863,\"y\":786}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1116,\"y\":769},{\"x\":1232,\"y\":767},{\"x\":1232,\"y\":787},{\"x\":1117,\"y\":786}],\"text\":\"aggregation\"}]}",
        "{\"language\":\"en\",\"text\":\"Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract ... CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute .. TimeDiscounting Relative\",\"lines\":[{\"boundingBox\":[{\"x\":203,\"y\":11},{\"x\":274,\"y\":11},{\"x\":274,\"y\":35},{\"x\":203,\"y\":36}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":290,\"y\":12},{\"x\":368,\"y\":13},{\"x\":368,\"y\":35},{\"x\":289,\"y\":34}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":694,\"y\":79},{\"x\":761,\"y\":81},{\"x\":760,\"y\":102},{\"x\":693,\"y\":98}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":822,\"y\":115},{\"x\":1000,\"y\":114},{\"x\":1000,\"y\":135},{\"x\":822,\"y\":137}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":864,\"y\":216},{\"x\":959,\"y\":218},{\"x\":959,\"y\":239},{\"x\":864,\"y\":237}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1116,\"y\":220},{\"x\":1217,\"y\":218},{\"x\":1218,\"y\":239},{\"x\":1116,\"y\":241}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1139,\"y\":244},{\"x\":1197,\"y\":243},{\"x\":1197,\"y\":264},{\"x\":1139,\"y\":264}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":693,\"y\":315},{\"x\":782,\"y\":316},{\"x\":782,\"y\":339},{\"x\":692,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":814,\"y\":352},{\"x\":1009,\"y\":352},{\"x\":1009,\"y\":371},{\"x\":814,\"y\":372}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":41,\"y\":380},{\"x\":179,\"y\":381},{\"x\":179,\"y\":405},{\"x\":41,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":388,\"y\":380},{\"x\":525,\"y\":381},{\"x\":525,\"y\":405},{\"x\":388,\"y\":403}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1250,\"y\":386},{\"x\":1346,\"y\":385},{\"x\":1346,\"y\":408},{\"x\":1250,\"y\":408}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":914,\"y\":402},{\"x\":915,\"y\":425},{\"x\":909,\"y\":425},{\"x\":909,\"y\":403}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":50,\"y\":411},{\"x\":172,\"y\":412},{\"x\":172,\"y\":439},{\"x\":50,\"y\":438}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":389,\"y\":411},{\"x\":525,\"y\":412},{\"x\":525,\"y\":436},{\"x\":389,\"y\":435}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1231,\"y\":415},{\"x\":1366,\"y\":415},{\"x\":1367,\"y\":441},{\"x\":1231,\"y\":442}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":834,\"y\":441},{\"x\":989,\"y\":442},{\"x\":988,\"y\":463},{\"x\":834,\"y\":461}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":872,\"y\":466},{\"x\":949,\"y\":466},{\"x\":949,\"y\":485},{\"x\":872,\"y\":484}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":688,\"y\":554},{\"x\":799,\"y\":552},{\"x\":800,\"y\":573},{\"x\":688,\"y\":575}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":814,\"y\":587},{\"x\":1009,\"y\":587},{\"x\":1009,\"y\":610},{\"x\":814,\"y\":610}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":914,\"y\":646},{\"x\":914,\"y\":662},{\"x\":909,\"y\":663},{\"x\":909,\"y\":646}],\"text\":\"..\"},{\"boundingBox\":[{\"x\":833,\"y\":677},{\"x\":990,\"y\":678},{\"x\":990,\"y\":700},{\"x\":833,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":873,\"y\":701},{\"x\":951,\"y\":702},{\"x\":950,\"y\":722},{\"x\":873,\"y\":721}],\"text\":\"Relative\"}],\"words\":[{\"boundingBox\":[{\"x\":205,\"y\":12},{\"x\":274,\"y\":11},{\"x\":273,\"y\":36},{\"x\":204,\"y\":35}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":290,\"y\":12},{\"x\":368,\"y\":14},{\"x\":368,\"y\":36},{\"x\":290,\"y\":35}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":694,\"y\":79},{\"x\":758,\"y\":82},{\"x\":756,\"y\":102},{\"x\":693,\"y\":98}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":823,\"y\":116},{\"x\":999,\"y\":115},{\"x\":999,\"y\":136},{\"x\":822,\"y\":138}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":865,\"y\":216},{\"x\":956,\"y\":219},{\"x\":956,\"y\":240},{\"x\":864,\"y\":238}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1117,\"y\":221},{\"x\":1217,\"y\":218},{\"x\":1217,\"y\":240},{\"x\":1116,\"y\":242}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1139,\"y\":245},{\"x\":1197,\"y\":244},{\"x\":1197,\"y\":265},{\"x\":1140,\"y\":264}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":693,\"y\":317},{\"x\":780,\"y\":318},{\"x\":780,\"y\":339},{\"x\":693,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":815,\"y\":353},{\"x\":1006,\"y\":353},{\"x\":1006,\"y\":371},{\"x\":815,\"y\":372}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":42,\"y\":381},{\"x\":175,\"y\":383},{\"x\":175,\"y\":405},{\"x\":41,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":389,\"y\":381},{\"x\":521,\"y\":382},{\"x\":521,\"y\":406},{\"x\":388,\"y\":403}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1253,\"y\":386},{\"x\":1346,\"y\":386},{\"x\":1345,\"y\":409},{\"x\":1252,\"y\":409}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":914,\"y\":403},{\"x\":915,\"y\":422},{\"x\":910,\"y\":423},{\"x\":909,\"y\":404}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":51,\"y\":411},{\"x\":171,\"y\":413},{\"x\":170,\"y\":439},{\"x\":50,\"y\":439}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":390,\"y\":412},{\"x\":525,\"y\":412},{\"x\":526,\"y\":437},{\"x\":390,\"y\":435}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1233,\"y\":416},{\"x\":1366,\"y\":416},{\"x\":1366,\"y\":442},{\"x\":1231,\"y\":443}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":835,\"y\":441},{\"x\":987,\"y\":442},{\"x\":986,\"y\":464},{\"x\":835,\"y\":461}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":872,\"y\":467},{\"x\":948,\"y\":467},{\"x\":948,\"y\":485},{\"x\":873,\"y\":485}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":690,\"y\":556},{\"x\":797,\"y\":553},{\"x\":798,\"y\":574},{\"x\":690,\"y\":574}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":814,\"y\":589},{\"x\":1008,\"y\":587},{\"x\":1008,\"y\":611},{\"x\":814,\"y\":610}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":914,\"y\":648},{\"x\":914,\"y\":659},{\"x\":909,\"y\":659},{\"x\":909,\"y\":648}],\"text\":\"..\"},{\"boundingBox\":[{\"x\":835,\"y\":677},{\"x\":987,\"y\":678},{\"x\":987,\"y\":701},{\"x\":835,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":702},{\"x\":948,\"y\":702},{\"x\":949,\"y\":723},{\"x\":874,\"y\":722}],\"text\":\"Relative\"}]}",
        "{\"language\":\"en\",\"text\":\"Filter: Weighting: referral reduced context similarity weighted Aggregation: set age-based reputation filter (absolut) referral (absolute referral set set average value congruence)\",\"lines\":[{\"boundingBox\":[{\"x\":244,\"y\":20},{\"x\":302,\"y\":20},{\"x\":302,\"y\":39},{\"x\":244,\"y\":39}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":5},{\"x\":782,\"y\":7},{\"x\":781,\"y\":31},{\"x\":668,\"y\":28}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":26,\"y\":34},{\"x\":102,\"y\":34},{\"x\":102,\"y\":54},{\"x\":26,\"y\":55}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":451,\"y\":26},{\"x\":539,\"y\":24},{\"x\":540,\"y\":44},{\"x\":451,\"y\":46}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":638,\"y\":33},{\"x\":814,\"y\":33},{\"x\":814,\"y\":57},{\"x\":638,\"y\":57}],\"text\":\"context similarity\"},{\"boundingBox\":[{\"x\":916,\"y\":24},{\"x\":1012,\"y\":23},{\"x\":1012,\"y\":48},{\"x\":916,\"y\":49}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":1110,\"y\":34},{\"x\":1248,\"y\":33},{\"x\":1248,\"y\":57},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":80,\"y\":64},{\"x\":80,\"y\":82},{\"x\":47,\"y\":83}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":217,\"y\":50},{\"x\":331,\"y\":48},{\"x\":332,\"y\":71},{\"x\":217,\"y\":73}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":1330,\"y\":41},{\"x\":1436,\"y\":39},{\"x\":1437,\"y\":60},{\"x\":1330,\"y\":62}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":200,\"y\":74},{\"x\":346,\"y\":76},{\"x\":346,\"y\":101},{\"x\":199,\"y\":100}],\"text\":\"filter (absolut)\"},{\"boundingBox\":[{\"x\":456,\"y\":54},{\"x\":535,\"y\":52},{\"x\":536,\"y\":72},{\"x\":456,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":673,\"y\":63},{\"x\":776,\"y\":63},{\"x\":776,\"y\":84},{\"x\":673,\"y\":85}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":925,\"y\":53},{\"x\":1003,\"y\":52},{\"x\":1004,\"y\":74},{\"x\":925,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":478,\"y\":85},{\"x\":512,\"y\":84},{\"x\":512,\"y\":103},{\"x\":478,\"y\":103}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":982,\"y\":84},{\"x\":982,\"y\":102},{\"x\":947,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1135,\"y\":65},{\"x\":1224,\"y\":65},{\"x\":1223,\"y\":86},{\"x\":1135,\"y\":85}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1355,\"y\":70},{\"x\":1412,\"y\":69},{\"x\":1412,\"y\":87},{\"x\":1355,\"y\":87}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":658,\"y\":93},{\"x\":793,\"y\":92},{\"x\":793,\"y\":113},{\"x\":658,\"y\":114}],\"text\":\"congruence)\"}],\"words\":[{\"boundingBox\":[{\"x\":245,\"y\":21},{\"x\":302,\"y\":20},{\"x\":302,\"y\":40},{\"x\":245,\"y\":39}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":5},{\"x\":782,\"y\":7},{\"x\":782,\"y\":32},{\"x\":669,\"y\":29}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":27,\"y\":35},{\"x\":102,\"y\":34},{\"x\":102,\"y\":55},{\"x\":27,\"y\":56}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":451,\"y\":27},{\"x\":534,\"y\":25},{\"x\":534,\"y\":45},{\"x\":451,\"y\":46}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":638,\"y\":36},{\"x\":714,\"y\":34},{\"x\":713,\"y\":57},{\"x\":638,\"y\":57}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":718,\"y\":34},{\"x\":814,\"y\":35},{\"x\":814,\"y\":58},{\"x\":718,\"y\":57}],\"text\":\"similarity\"},{\"boundingBox\":[{\"x\":917,\"y\":26},{\"x\":1009,\"y\":24},{\"x\":1009,\"y\":49},{\"x\":917,\"y\":49}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":1111,\"y\":35},{\"x\":1248,\"y\":34},{\"x\":1248,\"y\":58},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":79,\"y\":64},{\"x\":79,\"y\":82},{\"x\":47,\"y\":83}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":218,\"y\":51},{\"x\":326,\"y\":49},{\"x\":326,\"y\":71},{\"x\":217,\"y\":73}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":1331,\"y\":41},{\"x\":1433,\"y\":40},{\"x\":1432,\"y\":61},{\"x\":1331,\"y\":62}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":200,\"y\":75},{\"x\":246,\"y\":76},{\"x\":245,\"y\":101},{\"x\":200,\"y\":101}],\"text\":\"filter\"},{\"boundingBox\":[{\"x\":250,\"y\":76},{\"x\":345,\"y\":77},{\"x\":345,\"y\":102},{\"x\":250,\"y\":101}],\"text\":\"(absolut)\"},{\"boundingBox\":[{\"x\":456,\"y\":55},{\"x\":535,\"y\":52},{\"x\":536,\"y\":73},{\"x\":456,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":676,\"y\":64},{\"x\":773,\"y\":64},{\"x\":773,\"y\":85},{\"x\":675,\"y\":85}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":925,\"y\":54},{\"x\":1003,\"y\":52},{\"x\":1003,\"y\":75},{\"x\":925,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":478,\"y\":84},{\"x\":511,\"y\":84},{\"x\":511,\"y\":103},{\"x\":478,\"y\":103}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":982,\"y\":84},{\"x\":982,\"y\":102},{\"x\":947,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1136,\"y\":66},{\"x\":1219,\"y\":66},{\"x\":1219,\"y\":87},{\"x\":1136,\"y\":85}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1356,\"y\":70},{\"x\":1407,\"y\":70},{\"x\":1408,\"y\":87},{\"x\":1355,\"y\":88}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":660,\"y\":94},{\"x\":794,\"y\":93},{\"x\":794,\"y\":114},{\"x\":659,\"y\":114}],\"text\":\"congruence)\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 13 May 2015\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":14},{\"x\":890,\"y\":16},{\"x\":890,\"y\":74},{\"x\":4,\"y\":71}],\"text\":\"Published online: 13 May 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":17},{\"x\":271,\"y\":15},{\"x\":272,\"y\":72},{\"x\":6,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":15},{\"x\":498,\"y\":15},{\"x\":498,\"y\":73},{\"x\":291,\"y\":72}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":511,\"y\":15},{\"x\":573,\"y\":15},{\"x\":573,\"y\":73},{\"x\":510,\"y\":73}],\"text\":\"13\"},{\"boundingBox\":[{\"x\":593,\"y\":15},{\"x\":726,\"y\":16},{\"x\":725,\"y\":74},{\"x\":592,\"y\":74}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":745,\"y\":16},{\"x\":886,\"y\":17},{\"x\":885,\"y\":75},{\"x\":744,\"y\":74}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 3.2302282,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczEzNjQwLTAyMC0wMDU0NS16LnBkZg2",
      "metadata_author": "Haoliang Cui",
      "metadata_title": "A classification method for social information of sellers on social network",
      "people": [
        "Haoliang Cui1,",
        "Shuai Shao2",
        "Shaozhang Niu1,",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Bengio",
        "Kim",
        "Liu",
        "Peters",
        "Devlin",
        "Wu",
        "Bayes",
        "Malli",
        "Chen",
        "Ning",
        "Yin",
        "Binder",
        "Jieba",
        "thon"
      ],
      "keyphrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "commerce platforms",
        "orcid.org",
        "Correspondence",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "licenses",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "Related work",
        "present stage",
        "good results",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "capacity",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "background processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "subsequent processing",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "characteristic dimension",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "tf matrix",
        "Data label",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dj",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "hidden",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "official recommended values",
        "feature matching scheme",
        "Text classification fine-tuning",
        "full connection layer",
        "natural language processing",
        "machine learning scheme",
        "text information token",
        "additional 9500 text data",
        "word segmentation process",
        "social information data",
        "first token",
        "5.2 Classification scheme",
        "TF-IDF model",
        "text length",
        "text description",
        "deep learning",
        "learning rate",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "test set",
        "commodity terms",
        "experimental results",
        "TF-IDF-based model",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "classification problem",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "input",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "commodities",
        "social e-commerce market",
        "standard description text",
        "social e-commerce environment",
        "knowledge distillation technology",
        "standard product names",
        "large-scale data marking",
        "social e-commerce classification",
        "model recognition rate",
        "product information",
        "test data",
        "text-based classification",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "high correlation",
        "work",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "semi"
      ],
      "merged_content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10 \n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi",
      "text": [
        "CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10",
        "Published online: 14 January 2021"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"CLSIRATDET (E) ECHOIKON Tokcls Tok1 . Tok10\",\"lines\":[{\"boundingBox\":[{\"x\":99,\"y\":15},{\"x\":893,\"y\":14},{\"x\":893,\"y\":54},{\"x\":99,\"y\":55}],\"text\":\"CLSIRATDET (E) ECHOIKON\"},{\"boundingBox\":[{\"x\":49,\"y\":112},{\"x\":171,\"y\":112},{\"x\":172,\"y\":141},{\"x\":49,\"y\":142}],\"text\":\"Tokcls\"},{\"boundingBox\":[{\"x\":282,\"y\":113},{\"x\":372,\"y\":113},{\"x\":373,\"y\":142},{\"x\":282,\"y\":143}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":473,\"y\":110},{\"x\":490,\"y\":109},{\"x\":492,\"y\":129},{\"x\":474,\"y\":131}],\"text\":\".\"},{\"boundingBox\":[{\"x\":615,\"y\":112},{\"x\":730,\"y\":112},{\"x\":730,\"y\":142},{\"x\":615,\"y\":141}],\"text\":\"Tok10\"}],\"words\":[{\"boundingBox\":[{\"x\":99,\"y\":16},{\"x\":435,\"y\":15},{\"x\":434,\"y\":56},{\"x\":99,\"y\":55}],\"text\":\"CLSIRATDET\"},{\"boundingBox\":[{\"x\":478,\"y\":15},{\"x\":573,\"y\":15},{\"x\":572,\"y\":56},{\"x\":478,\"y\":56}],\"text\":\"(E)\"},{\"boundingBox\":[{\"x\":597,\"y\":15},{\"x\":892,\"y\":15},{\"x\":892,\"y\":55},{\"x\":597,\"y\":56}],\"text\":\"ECHOIKON\"},{\"boundingBox\":[{\"x\":51,\"y\":113},{\"x\":168,\"y\":113},{\"x\":167,\"y\":142},{\"x\":51,\"y\":143}],\"text\":\"Tokcls\"},{\"boundingBox\":[{\"x\":284,\"y\":115},{\"x\":372,\"y\":114},{\"x\":372,\"y\":142},{\"x\":284,\"y\":144}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":473,\"y\":110},{\"x\":485,\"y\":109},{\"x\":487,\"y\":130},{\"x\":475,\"y\":131}],\"text\":\".\"},{\"boundingBox\":[{\"x\":617,\"y\":113},{\"x\":724,\"y\":113},{\"x\":724,\"y\":143},{\"x\":617,\"y\":142}],\"text\":\"Tok10\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 14 January 2021\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":978,\"y\":18},{\"x\":978,\"y\":72},{\"x\":5,\"y\":69}],\"text\":\"Published online: 14 January 2021\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":269,\"y\":17},{\"x\":269,\"y\":70},{\"x\":5,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":498,\"y\":17},{\"x\":498,\"y\":71},{\"x\":291,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":510,\"y\":17},{\"x\":572,\"y\":18},{\"x\":573,\"y\":72},{\"x\":510,\"y\":71}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":591,\"y\":18},{\"x\":816,\"y\":18},{\"x\":817,\"y\":73},{\"x\":591,\"y\":72}],\"text\":\"January\"},{\"boundingBox\":[{\"x\":835,\"y\":18},{\"x\":977,\"y\":19},{\"x\":977,\"y\":73},{\"x\":835,\"y\":73}],\"text\":\"2021\"}]}"
      ]
    },
    {
      "@search.score": 2.4956415,
      "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicat",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNTM3LTAxOS0wMjEwLTcucGRm0",
      "metadata_author": "Taiwo Kolajo ",
      "metadata_title": "Big data stream analysis: a systematic literature review",
      "people": [
        "Taiwo Kolajo",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "Kolajo",
        "Kafka",
        "Spark",
        "Moore",
        "Redis",
        "SAMOA",
        "ETALIS",
        "Anodot",
        "Cloudet",
        "Artemis",
        "Kyvos",
        "AtScale"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "effective resource allocation strategy",
        "existing data mining tools",
        "big data stream tools",
        "data stream computing mode",
        "big data streaming tools",
        "Big data stream analysis",
        "big data stream computing",
        "big data streams analysis",
        "Creative Commons license",
        "big data batch computing",
        "big data computing",
        "big data tool",
        "J Big Data",
        "several computational challenges",
        "process- ing requirements",
        "inherent dynamic characteristics",
        "Three major databases",
        "first search string",
        "standard benchmark dataset",
        "load balancing issues",
        "nificant research efforts",
        "technologies Open Access",
        "SURVEY PAPER Kolajo",
        "empirical analysis",
        "real-time analysis",
        "future computing",
        "data sources",
        "parallelization issues",
        "research questions",
        "literature review",
        "Olawande Daramola3",
        "Ayodele Adebiyi",
        "two types",
        "huge amount",
        "methodical approach",
        "global view",
        "exclusion criteria",
        "preprocessing stage",
        "iterative jobs",
        "scalable frameworks",
        "growing size",
        "iveco mmons",
        "unrestricted use",
        "appropriate credit",
        "original author",
        "Information Sciences",
        "Covenant University",
        "Full list",
        "author information",
        "Taiwo Kolajo",
        "orcid.org",
        "information technology",
        "large volume",
        "great velocity",
        "systematic review",
        "initial 2295 papers",
        "47 papers",
        "Introduction",
        "Advances",
        "high-velocity",
        "ability",
        "nature",
        "terms",
        "variety",
        "veracity",
        "volatility",
        "value",
        "new",
        "trend",
        "Abstract",
        "fact",
        "number",
        "applications",
        "methods",
        "techniques",
        "rigorous",
        "comparisons",
        "Scopus",
        "ScienceDirect",
        "EBSCO",
        "journals",
        "conferences",
        "entities",
        "IEEE",
        "ACM",
        "SpringerLink",
        "Elsevier",
        "inclusion",
        "study",
        "privacy",
        "attention",
        "key",
        "features",
        "lytics",
        "conclusion",
        "algorithms",
        "complexity",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "changes",
        "Correspondence",
        "fulokoja",
        "1 Department",
        "Computer",
        "Ota",
        "Nigeria",
        "Page",
        "30Kolajo",
        "big data streaming analytics",
        "big data stream analysis",
        "Big data batch processing",
        "Streaming processing frameworks",
        "stream processing paradigms",
        "Stream processing solutions",
        "data flow graph",
        "real-time application scenarios",
        "real-time, high volume",
        "time data analysis",
        "real-time data stream",
        "batch computing",
        "stream computing",
        "stream processor",
        "high-velocity flow",
        "incoming data",
        "data generating",
        "market data",
        "real-time sources",
        "changing conditions",
        "common understanding",
        "scientific community",
        "key issues",
        "detailed evaluation",
        "massive amount",
        "high- velocity",
        "multiple sources",
        "low latency",
        "new sources",
        "location services",
        "mobile devices",
        "sensor pervasiveness",
        "fundamental assumption",
        "potential value",
        "parallel architectures",
        "decision-making process",
        "static questions",
        "continuous queries",
        "diverse sources",
        "sideration availability",
        "fault tolerance",
        "infinite tuple",
        "actionable results",
        "interconnected streams",
        "related work",
        "research works",
        "Research method",
        "crucial need",
        "real contrasts",
        "ming paradigm",
        "Discussion” section",
        "Result” section",
        "addition",
        "output",
        "low-latency",
        "seconds",
        "demand",
        "reason",
        "huge",
        "organisations",
        "businesses",
        "paper",
        "purpose",
        "overview",
        "findings",
        "implications",
        "practice",
        "update",
        "state",
        "areas",
        "challenges",
        "rest",
        "Background",
        "information",
        "Limitation",
        "Conclusion",
        "Internet",
        "Things",
        "Sensors",
        "freshness",
        "platforms",
        "Storm",
        "Kafka",
        "Spark",
        "Table",
        "motion",
        "essence",
        "fly",
        "scalability",
        "assimilation",
        "production",
        "operations",
        "Fig.",
        "Dimension Batch processing Streaming processing Input Data chunks",
        "big data stream computing environments",
        "big data streaming analysis",
        "domain Web mining",
        "streaming analytics system",
        "big data streams",
        "Hardware Multiple CPUs",
        "Data flow graph",
        "high data rates",
        "data processing node",
        "single limited amount",
        "memory Storage Store",
        "new incoming data",
        "Such processing",
        "new data",
        "missing data",
        "data normalization",
        "multiple rounds",
        "distributed system",
        "High fault-tolerance",
        "life-critical systems",
        "data tuples",
        "Data size",
        "new information",
        "new tuple",
        "new trends",
        "model predictions",
        "feature extraction",
        "single pass",
        "external feeds",
        "typical example",
        "non-trivial portion",
        "traffic monitoring",
        "sensor networks",
        "Key issues",
        "useful knowledge",
        "current happenings",
        "speedy manner",
        "organisa- tions",
        "load balancing",
        "privacy issues",
        "exponential growth",
        "computer resources",
        "research efforts",
        "effective resource",
        "cation strategy",
        "com- plexity",
        "small number",
        "different datasets",
        "component failure",
        "time-sensitive processes",
        "security threats",
        "data Time",
        "latest tuples",
        "analytic applications",
        "sliding window",
        "milliseconds Applications",
        "logical container",
        "main challenges",
        "Fault‑tolerance",
        "integration technique",
        "efficient operations",
        "results",
        "operators",
        "models",
        "access",
        "idea",
        "duplicates",
        "parsing",
        "windows",
        "Table 1",
        "Comparison",
        "updates",
        "advance",
        "passes",
        "figure",
        "need",
        "order",
        "problems",
        "performance",
        "timeliness",
        "consistency",
        "heterogeneity",
        "incompleteness",
        "accuracy",
        "The",
        "way",
        "processors",
        "Moore",
        "law",
        "fore",
        "views",
        "interruption",
        "big data stream computing system",
        "organisational resource management specifi",
        "big data stream dataset",
        "Big data stream analytics",
        "good multiple instances replication",
        "big data analytics literature",
        "big data analytics methods",
        "good system structure",
        "distributing envi- ronment",
        "effective tech- niques",
        "Big data streams",
        "competent data presentation",
        "national Data Cooperation",
        "big data analysis",
        "data streams changes",
        "partial data streams",
        "previous research efforts",
        "big data streaming",
        "big data challenges",
        "systematic literature review",
        "High throughput Decision",
        "big threat",
        "streaming analytics",
        "streaming” analytics",
        "streaming data",
        "natural disaster",
        "continuous processing",
        "local views",
        "meaningful content",
        "Load balancing",
        "load shedding",
        "peak loads",
        "average load",
        "global centre",
        "entire information",
        "needs protection",
        "main objectives",
        "future observations",
        "processing algorithms",
        "stream-specific requirements",
        "various tools",
        "research focus",
        "business values",
        "high consistency",
        "high accuracy",
        "main challenge",
        "communicating nodes",
        "individual privacy",
        "ent characteristics",
        "The study",
        "reviews",
        "fraud",
        "tures",
        "architecture",
        "minimal",
        "latency",
        "stability",
        "Heterogeneity",
        "semantics",
        "granularity",
        "correlate",
        "real-time",
        "diversity",
        "hierarchy",
        "resources",
        "variance",
        "result",
        "respect",
        "sub-graph",
        "replicas",
        "portion",
        "issue",
        "opportunities",
        "IDC",
        "half",
        "volume",
        "velocity",
        "variability",
        "consideration",
        "work",
        "section",
        "technologies",
        "Authors",
        "definitions",
        "types",
        "technology",
        "four big data streaming tools",
        "big data stream analysis tools",
        "big data stream literature review",
        "authoritative, full-text scientific, technical",
        "data stream analysis reviews",
        "big data stream framework",
        "big data stream processing",
        "big data stream algorithms",
        "big data stream analytics",
        "three standard database indexes",
        "big data stream technologies",
        "big data analytics",
        "machine learning algorithms",
        "full-text data- bases",
        "large data- base",
        "smart intuitive functionality",
        "electronic journal service",
        "big data technologies",
        "academic journal articles",
        "leading information solution",
        "systematic mapping method",
        "following research questions",
        "good search string",
        "Data sources",
        "four categories",
        "peer-reviewed literature",
        "citation database",
        "standard databases",
        "information professionals",
        "bibliographic database",
        "empirical research",
        "same vein",
        "particular focus",
        "anomaly detection",
        "clear explanation",
        "Relevant publications",
        "Science Direct",
        "hensive overview",
        "research output",
        "social sciences",
        "largest abstract",
        "open access",
        "health publications",
        "14 million publications",
        "Life Sciences",
        "Physical Sciences",
        "Health Sciences",
        "wide range",
        "academic researchers",
        "application areas",
        "healthcare profes",
        "evaluation techniques",
        "peer-reviewed journals",
        "3800 journals",
        "status",
        "survey",
        "scope",
        "comprehensive",
        "differences",
        "concept",
        "capabilities",
        "limitations",
        "strengths",
        "benchmarks",
        "population",
        "son",
        "intervention",
        "outcome",
        "keywords",
        "searches",
        "EBSCOhost",
        "rich",
        "abstracts",
        "citations",
        "36,377 titles",
        "11,678 publishers",
        "world",
        "arts",
        "humanities",
        "students",
        "teachers",
        "35,000 books",
        "Engineering",
        "porate",
        "Third Search string refinement result",
        "free online professional network",
        "First search string result",
        "Second search string result",
        "Scopus ScienceDirect EBSCOhost Total",
        "major academic publishers",
        "nine (9) search strings",
        "high impact journals",
        "Inclusion criteria Papers",
        "Table 5 Final Selection",
        "Further refinement",
        "Data retrieval",
        "easy analysis",
        "high-quality e-books",
        "100 million publications",
        "secondary source",
        "rich databases",
        "Boolean ‘OR",
        "2295 arti- cles",
        "three databases",
        "computer science",
        "subject domain",
        "quick overview",
        "Microsoft Excel",
        "three categories",
        "black colour",
        "similar investigations",
        "following categories",
        "primary study",
        "source language",
        "11 million researchers",
        "year range",
        "total number",
        "peer-reviewed conferences",
        "relevant papers",
        "relevant” papers",
        "recent papers",
        "16,711 journals",
        "Table 2",
        "Table 3",
        "Table 4",
        "1989 papers",
        "315 papers",
        "111 papers",
        "45 papers",
        "18 papers",
        "magazine",
        "titles",
        "60,000 audiobooks",
        "iv.",
        "ResearchGate4",
        "scientists",
        "collaborators",
        "authors",
        "subscription",
        "set",
        "sources",
        "interest",
        "stage",
        "PDF",
        "introduction",
        "green",
        "red",
        "colours",
        "end",
        "workshops",
        "technical",
        "symposium",
        "case",
        "part",
        "English",
        "contributions",
        "900,000",
        "1500",
        "alternative big data streaming solutions",
        "effective data management decisions",
        "spike load profile platform",
        "Big data stream platforms",
        "Streaming data sources",
        "many NoSQL databases",
        "specific application interfaces",
        "data loading procedure",
        "enterprise technology vendors",
        "open source community",
        "open source solutions",
        "Workload profile",
        "proprietary solutions",
        "stream applications",
        "high-velocity data",
        "data stores",
        "recent technology",
        "Such platforms",
        "platform distribution",
        "Data access",
        "Several tools",
        "sary tools",
        "single flow",
        "growing demand",
        "pro- jection",
        "different structures",
        "different ways",
        "CAP theorem",
        "consistent loads",
        "consistent flows",
        "web-based services",
        "soft- ware",
        "critical functions",
        "Latency requirement",
        "minimal delay",
        "key-value stores",
        "memory solution",
        "licensing issues",
        "limited maturity",
        "developer communities",
        "modification challenges",
        "large datasets",
        "large scale",
        "network partition",
        "service deployment",
        "service cloud",
        "premise approach",
        "easy integration",
        "consistency requirement",
        "careful selection",
        "serialization technologies",
        "execution",
        "functionalities",
        "response",
        "factors",
        "Shape",
        "capturing",
        "storing",
        "rep",
        "instance",
        "room",
        "flexibility",
        "storage",
        "users",
        "Availability",
        "presence",
        "break",
        "scenario",
        "requests",
        "Infrastructure",
        "option",
        "processing",
        "predictable",
        "workloads",
        "spikes",
        "combination",
        "go",
        "Tables",
        "pricing",
        "innovation",
        "development",
        "lack",
        "support",
        "outdating",
        "problem",
        "big data stream analysis Tools",
        "Multinomial latent dirichlet allocation",
        "Elastic streaming processing engine",
        "real-time social media data",
        "Microsoft azure stream analytics",
        "old-based stream clustering approaches",
        "Table 6 Open source tools",
        "social media analysis",
        "social media streams",
        "high dimensional data",
        "Markov Random Field",
        "Sentiment brand monitoring",
        "IBM InfoSphere streams",
        "Density-based clustering algorithm",
        "Voltage clustering algorithm",
        "little research efforts",
        "maximum similarity threshold",
        "algorithm threshold setting",
        "incremental clustering approaches",
        "Online Spherical K-means",
        "Splunk stream",
        "incoming stream",
        "data grouping",
        "Proprietary tools",
        "WSO2 analytics",
        "Microsoft StreamInsight",
        "clustering algorithms",
        "Spark streaming",
        "Complete Clustering",
        "hierarchical clustering",
        "online clustering",
        "Research Question",
        "true costs",
        "Apache storm",
        "Apache Samza",
        "Apache Aurora",
        "Apache Kylin",
        "dynamic nature",
        "desirable number",
        "prior knowledge",
        "scalable graph",
        "limited space",
        "apriori number",
        "Google MillWheel",
        "TIBCO StreamBase",
        "Kyvos insights",
        "Lambda architecture",
        "Much work",
        "fragmentation issues",
        "tive approach",
        "static values",
        "partitioning algorithms",
        "technology Article",
        "balanced partitioning",
        "time window",
        "Condensed Clusters",
        "existing clusters",
        "Threshold-based techniques",
        "Table 8 Methods",
        "Table 7",
        "understanding",
        "benefits",
        "BlockMon",
        "NoSQL",
        "Photon",
        "MavEStream",
        "EsperTech",
        "Redis",
        "C-SPARQL",
        "SAMOA",
        "CQELS",
        "ETALIS",
        "XSEQ",
        "k-median",
        "k-medoid",
        "expectation-maximization",
        "tendency",
        "decisions",
        "DenStream",
        "OpticStream",
        "Exclusive",
        "outliers",
        "HDDStream",
        "PreDeCon-Stream",
        "PKS-Stream",
        "memory",
        "face",
        "CodeBlue",
        "Anodot",
        "Cloudet",
        "Numenta",
        "Artemis",
        "Striim",
        "AtScale",
        "efficiency",
        "SPADE",
        "learning",
        "LSML",
        "KTS",
        "Dynamic prime-number based security verification",
        "User profile vector update algorithm",
        "Singular spectrum matrix completion",
        "Temporal fuzzy concept analysis",
        "MI outlier detection algorithm",
        "Tag assignment stream clustering",
        "QRS detection algorithm",
        "cloud computing algorithm",
        "Parallel K-means clustering",
        "Locality sensitive hashing",
        "Forward chaining rule",
        "Continuous query processing",
        "Multi-query optimization strategy",
        "Outlier method",
        "Density cognition",
        "Inc I-MLOF",
        "Adaptive windowing",
        "online ensemble",
        "Nearest neighbour",
        "Markov chains",
        "LSH",
        "TASC",
        "StreamMap",
        "CluStream",
        "HPClustering",
        "D-Stream",
        "DCStream",
        "P-Stream",
        "ADStream",
        "CQR",
        "FPSPAN-growth",
        "OMCA",
        "MQOS",
        "automata",
        "VPA",
        "AWOE",
        "K-anonymity",
        "closeness",
        "ECM-sketch",
        "Block-QuickSort-AdjacentJobMatch",
        "Block-QuickSort-OverlapReplicat"
      ],
      "merged_content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3  and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data            (2019) 6:47  \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence:   \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n operator stream NYMEX \n\n\n\nPage 4 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data            (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicat",
      "text": [
        "operator stream NYMEX",
        "Magnitude of change in paper distribution over the studied years 180 160 140 120 100 80 156 60 40 98 No of Papers 20 22 28 38 02 -1 2 3 5 2 5 4 5 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years",
        "Percentage of Publication Type 34,9% Journal Conferences 155, 41% 192, 50% . Workshop/Technical/Sym posium",
        "90 80 70 60 50 40 w 20 0 No of Researchers Frequency of Researchers 10 0 Italy Canada China India USA Germany France Japan Turkey Republic of Korea Countries Ireland Spain Poland Los Angeles Switzerland Iran Greece Norway",
        "Published online: 06 June 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"operator stream NYMEX\",\"lines\":[{\"boundingBox\":[{\"x\":340,\"y\":4},{\"x\":518,\"y\":2},{\"x\":519,\"y\":43},{\"x\":340,\"y\":45}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":779,\"y\":107},{\"x\":919,\"y\":107},{\"x\":918,\"y\":143},{\"x\":779,\"y\":143}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":21,\"y\":615},{\"x\":131,\"y\":612},{\"x\":132,\"y\":634},{\"x\":21,\"y\":639}],\"text\":\"NYMEX\"}],\"words\":[{\"boundingBox\":[{\"x\":342,\"y\":4},{\"x\":518,\"y\":2},{\"x\":516,\"y\":45},{\"x\":340,\"y\":45}],\"text\":\"operator\"},{\"boundingBox\":[{\"x\":781,\"y\":107},{\"x\":899,\"y\":109},{\"x\":899,\"y\":144},{\"x\":779,\"y\":144}],\"text\":\"stream\"},{\"boundingBox\":[{\"x\":47,\"y\":616},{\"x\":124,\"y\":613},{\"x\":124,\"y\":634},{\"x\":46,\"y\":638}],\"text\":\"NYMEX\"}]}",
        "{\"language\":\"en\",\"text\":\"Magnitude of change in paper distribution over the studied years 180 160 140 120 100 80 156 60 40 98 No of Papers 20 22 28 38 02 -1 2 3 5 2 5 4 5 10 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Years\",\"lines\":[{\"boundingBox\":[{\"x\":203,\"y\":0},{\"x\":1033,\"y\":0},{\"x\":1033,\"y\":34},{\"x\":203,\"y\":38}],\"text\":\"Magnitude of change in paper distribution over the\"},{\"boundingBox\":[{\"x\":507,\"y\":40},{\"x\":725,\"y\":42},{\"x\":725,\"y\":79},{\"x\":506,\"y\":76}],\"text\":\"studied years\"},{\"boundingBox\":[{\"x\":51,\"y\":77},{\"x\":94,\"y\":77},{\"x\":95,\"y\":102},{\"x\":50,\"y\":102}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":52,\"y\":122},{\"x\":96,\"y\":122},{\"x\":96,\"y\":143},{\"x\":52,\"y\":143}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":50,\"y\":165},{\"x\":96,\"y\":165},{\"x\":96,\"y\":187},{\"x\":50,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":53,\"y\":212},{\"x\":95,\"y\":212},{\"x\":95,\"y\":232},{\"x\":52,\"y\":231}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":96,\"y\":255},{\"x\":96,\"y\":275},{\"x\":51,\"y\":276}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":65,\"y\":299},{\"x\":94,\"y\":299},{\"x\":94,\"y\":319},{\"x\":65,\"y\":319}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1129,\"y\":304},{\"x\":1173,\"y\":303},{\"x\":1175,\"y\":326},{\"x\":1129,\"y\":325}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":66,\"y\":342},{\"x\":94,\"y\":342},{\"x\":94,\"y\":364},{\"x\":66,\"y\":363}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":65,\"y\":384},{\"x\":95,\"y\":384},{\"x\":95,\"y\":408},{\"x\":65,\"y\":408}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1060,\"y\":367},{\"x\":1093,\"y\":366},{\"x\":1094,\"y\":390},{\"x\":1059,\"y\":391}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":0,\"y\":387},{\"x\":3,\"y\":187},{\"x\":34,\"y\":188},{\"x\":29,\"y\":388}],\"text\":\"No of Papers\"},{\"boundingBox\":[{\"x\":65,\"y\":430},{\"x\":93,\"y\":431},{\"x\":94,\"y\":453},{\"x\":65,\"y\":453}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":838,\"y\":451},{\"x\":870,\"y\":451},{\"x\":871,\"y\":473},{\"x\":839,\"y\":473}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":910,\"y\":443},{\"x\":945,\"y\":442},{\"x\":946,\"y\":467},{\"x\":910,\"y\":469}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":985,\"y\":432},{\"x\":1018,\"y\":431},{\"x\":1019,\"y\":456},{\"x\":986,\"y\":457}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":78,\"y\":474},{\"x\":124,\"y\":473},{\"x\":124,\"y\":495},{\"x\":78,\"y\":496}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":167,\"y\":472},{\"x\":201,\"y\":471},{\"x\":201,\"y\":499},{\"x\":166,\"y\":500}],\"text\":\"-1\"},{\"boundingBox\":[{\"x\":245,\"y\":468},{\"x\":499,\"y\":467},{\"x\":499,\"y\":496},{\"x\":246,\"y\":497}],\"text\":\"2 3 5 2\"},{\"boundingBox\":[{\"x\":537,\"y\":467},{\"x\":571,\"y\":467},{\"x\":571,\"y\":498},{\"x\":536,\"y\":498}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":613,\"y\":463},{\"x\":654,\"y\":464},{\"x\":653,\"y\":497},{\"x\":612,\"y\":496}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":684,\"y\":467},{\"x\":796,\"y\":461},{\"x\":798,\"y\":489},{\"x\":684,\"y\":496}],\"text\":\"5 10\"},{\"boundingBox\":[{\"x\":82,\"y\":504},{\"x\":1178,\"y\":504},{\"x\":1178,\"y\":531},{\"x\":82,\"y\":531}],\"text\":\"2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\"},{\"boundingBox\":[{\"x\":591,\"y\":554},{\"x\":678,\"y\":556},{\"x\":677,\"y\":580},{\"x\":591,\"y\":578}],\"text\":\"Years\"}],\"words\":[{\"boundingBox\":[{\"x\":204,\"y\":1},{\"x\":381,\"y\":2},{\"x\":380,\"y\":38},{\"x\":203,\"y\":36}],\"text\":\"Magnitude\"},{\"boundingBox\":[{\"x\":387,\"y\":2},{\"x\":424,\"y\":2},{\"x\":423,\"y\":38},{\"x\":386,\"y\":38}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":431,\"y\":2},{\"x\":545,\"y\":2},{\"x\":544,\"y\":37},{\"x\":430,\"y\":38}],\"text\":\"change\"},{\"boundingBox\":[{\"x\":552,\"y\":2},{\"x\":583,\"y\":2},{\"x\":583,\"y\":37},{\"x\":551,\"y\":37}],\"text\":\"in\"},{\"boundingBox\":[{\"x\":593,\"y\":2},{\"x\":691,\"y\":2},{\"x\":691,\"y\":36},{\"x\":592,\"y\":37}],\"text\":\"paper\"},{\"boundingBox\":[{\"x\":698,\"y\":2},{\"x\":886,\"y\":2},{\"x\":885,\"y\":32},{\"x\":698,\"y\":36}],\"text\":\"distribution\"},{\"boundingBox\":[{\"x\":899,\"y\":2},{\"x\":972,\"y\":1},{\"x\":972,\"y\":29},{\"x\":899,\"y\":31}],\"text\":\"over\"},{\"boundingBox\":[{\"x\":979,\"y\":1},{\"x\":1033,\"y\":1},{\"x\":1032,\"y\":27},{\"x\":979,\"y\":29}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":508,\"y\":40},{\"x\":628,\"y\":42},{\"x\":626,\"y\":79},{\"x\":507,\"y\":78}],\"text\":\"studied\"},{\"boundingBox\":[{\"x\":636,\"y\":42},{\"x\":726,\"y\":44},{\"x\":724,\"y\":79},{\"x\":634,\"y\":79}],\"text\":\"years\"},{\"boundingBox\":[{\"x\":51,\"y\":77},{\"x\":92,\"y\":77},{\"x\":92,\"y\":102},{\"x\":51,\"y\":102}],\"text\":\"180\"},{\"boundingBox\":[{\"x\":52,\"y\":122},{\"x\":90,\"y\":122},{\"x\":90,\"y\":143},{\"x\":52,\"y\":143}],\"text\":\"160\"},{\"boundingBox\":[{\"x\":51,\"y\":165},{\"x\":92,\"y\":165},{\"x\":92,\"y\":187},{\"x\":51,\"y\":187}],\"text\":\"140\"},{\"boundingBox\":[{\"x\":52,\"y\":212},{\"x\":90,\"y\":212},{\"x\":90,\"y\":232},{\"x\":52,\"y\":231}],\"text\":\"120\"},{\"boundingBox\":[{\"x\":52,\"y\":256},{\"x\":90,\"y\":255},{\"x\":90,\"y\":275},{\"x\":52,\"y\":276}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":65,\"y\":299},{\"x\":90,\"y\":299},{\"x\":90,\"y\":319},{\"x\":65,\"y\":319}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":1129,\"y\":303},{\"x\":1171,\"y\":303},{\"x\":1171,\"y\":326},{\"x\":1129,\"y\":326}],\"text\":\"156\"},{\"boundingBox\":[{\"x\":66,\"y\":342},{\"x\":92,\"y\":342},{\"x\":91,\"y\":364},{\"x\":66,\"y\":363}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":65,\"y\":384},{\"x\":91,\"y\":384},{\"x\":91,\"y\":408},{\"x\":65,\"y\":408}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1059,\"y\":367},{\"x\":1088,\"y\":366},{\"x\":1089,\"y\":390},{\"x\":1060,\"y\":391}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":0,\"y\":387},{\"x\":0,\"y\":347},{\"x\":28,\"y\":347},{\"x\":27,\"y\":387}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":0,\"y\":338},{\"x\":1,\"y\":304},{\"x\":29,\"y\":305},{\"x\":28,\"y\":339}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":298},{\"x\":3,\"y\":188},{\"x\":35,\"y\":189},{\"x\":30,\"y\":299}],\"text\":\"Papers\"},{\"boundingBox\":[{\"x\":66,\"y\":430},{\"x\":90,\"y\":430},{\"x\":90,\"y\":453},{\"x\":66,\"y\":452}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":839,\"y\":451},{\"x\":867,\"y\":451},{\"x\":867,\"y\":473},{\"x\":839,\"y\":473}],\"text\":\"22\"},{\"boundingBox\":[{\"x\":912,\"y\":443},{\"x\":940,\"y\":442},{\"x\":941,\"y\":467},{\"x\":913,\"y\":468}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":985,\"y\":432},{\"x\":1015,\"y\":431},{\"x\":1016,\"y\":456},{\"x\":986,\"y\":457}],\"text\":\"38\"},{\"boundingBox\":[{\"x\":78,\"y\":474},{\"x\":116,\"y\":473},{\"x\":117,\"y\":495},{\"x\":78,\"y\":496}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":166,\"y\":472},{\"x\":192,\"y\":471},{\"x\":193,\"y\":499},{\"x\":166,\"y\":500}],\"text\":\"-1\"},{\"boundingBox\":[{\"x\":254,\"y\":470},{\"x\":272,\"y\":469},{\"x\":274,\"y\":497},{\"x\":255,\"y\":497}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":326,\"y\":468},{\"x\":344,\"y\":468},{\"x\":345,\"y\":497},{\"x\":327,\"y\":497}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":401,\"y\":467},{\"x\":419,\"y\":467},{\"x\":420,\"y\":497},{\"x\":402,\"y\":497}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":476,\"y\":468},{\"x\":495,\"y\":468},{\"x\":495,\"y\":496},{\"x\":477,\"y\":497}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":545,\"y\":467},{\"x\":565,\"y\":467},{\"x\":565,\"y\":498},{\"x\":545,\"y\":498}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":622,\"y\":463},{\"x\":642,\"y\":464},{\"x\":641,\"y\":497},{\"x\":622,\"y\":496}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":696,\"y\":467},{\"x\":712,\"y\":467},{\"x\":712,\"y\":495},{\"x\":695,\"y\":496}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":764,\"y\":464},{\"x\":793,\"y\":462},{\"x\":794,\"y\":490},{\"x\":765,\"y\":491}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":84,\"y\":505},{\"x\":141,\"y\":505},{\"x\":140,\"y\":532},{\"x\":83,\"y\":532}],\"text\":\"2004\"},{\"boundingBox\":[{\"x\":156,\"y\":505},{\"x\":213,\"y\":505},{\"x\":212,\"y\":532},{\"x\":155,\"y\":532}],\"text\":\"2005\"},{\"boundingBox\":[{\"x\":230,\"y\":505},{\"x\":289,\"y\":505},{\"x\":288,\"y\":532},{\"x\":228,\"y\":532}],\"text\":\"2006\"},{\"boundingBox\":[{\"x\":303,\"y\":505},{\"x\":363,\"y\":505},{\"x\":362,\"y\":532},{\"x\":302,\"y\":532}],\"text\":\"2007\"},{\"boundingBox\":[{\"x\":377,\"y\":505},{\"x\":436,\"y\":505},{\"x\":435,\"y\":531},{\"x\":376,\"y\":532}],\"text\":\"2008\"},{\"boundingBox\":[{\"x\":453,\"y\":505},{\"x\":512,\"y\":505},{\"x\":511,\"y\":531},{\"x\":452,\"y\":531}],\"text\":\"2009\"},{\"boundingBox\":[{\"x\":527,\"y\":505},{\"x\":586,\"y\":505},{\"x\":585,\"y\":531},{\"x\":526,\"y\":531}],\"text\":\"2010\"},{\"boundingBox\":[{\"x\":600,\"y\":505},{\"x\":660,\"y\":505},{\"x\":659,\"y\":531},{\"x\":599,\"y\":531}],\"text\":\"2011\"},{\"boundingBox\":[{\"x\":676,\"y\":505},{\"x\":735,\"y\":505},{\"x\":734,\"y\":531},{\"x\":675,\"y\":531}],\"text\":\"2012\"},{\"boundingBox\":[{\"x\":750,\"y\":505},{\"x\":809,\"y\":505},{\"x\":808,\"y\":531},{\"x\":749,\"y\":531}],\"text\":\"2013\"},{\"boundingBox\":[{\"x\":824,\"y\":505},{\"x\":883,\"y\":505},{\"x\":882,\"y\":531},{\"x\":823,\"y\":531}],\"text\":\"2014\"},{\"boundingBox\":[{\"x\":899,\"y\":505},{\"x\":957,\"y\":505},{\"x\":956,\"y\":531},{\"x\":898,\"y\":531}],\"text\":\"2015\"},{\"boundingBox\":[{\"x\":973,\"y\":505},{\"x\":1032,\"y\":505},{\"x\":1032,\"y\":531},{\"x\":972,\"y\":531}],\"text\":\"2016\"},{\"boundingBox\":[{\"x\":1047,\"y\":505},{\"x\":1108,\"y\":505},{\"x\":1107,\"y\":531},{\"x\":1046,\"y\":531}],\"text\":\"2017\"},{\"boundingBox\":[{\"x\":1122,\"y\":505},{\"x\":1179,\"y\":505},{\"x\":1178,\"y\":531},{\"x\":1122,\"y\":531}],\"text\":\"2018\"},{\"boundingBox\":[{\"x\":593,\"y\":554},{\"x\":673,\"y\":558},{\"x\":673,\"y\":580},{\"x\":592,\"y\":579}],\"text\":\"Years\"}]}",
        "{\"language\":\"en\",\"text\":\"Percentage of Publication Type 34,9% Journal Conferences 155, 41% 192, 50% . Workshop/Technical/Sym posium\",\"lines\":[{\"boundingBox\":[{\"x\":153,\"y\":2},{\"x\":611,\"y\":3},{\"x\":610,\"y\":31},{\"x\":153,\"y\":29}],\"text\":\"Percentage of Publication Type\"},{\"boundingBox\":[{\"x\":122,\"y\":79},{\"x\":210,\"y\":79},{\"x\":210,\"y\":105},{\"x\":122,\"y\":106}],\"text\":\"34,9%\"},{\"boundingBox\":[{\"x\":503,\"y\":123},{\"x\":610,\"y\":122},{\"x\":611,\"y\":145},{\"x\":503,\"y\":146}],\"text\":\"Journal\"},{\"boundingBox\":[{\"x\":502,\"y\":210},{\"x\":672,\"y\":210},{\"x\":672,\"y\":237},{\"x\":502,\"y\":236}],\"text\":\"Conferences\"},{\"boundingBox\":[{\"x\":20,\"y\":280},{\"x\":143,\"y\":279},{\"x\":143,\"y\":307},{\"x\":20,\"y\":307}],\"text\":\"155, 41%\"},{\"boundingBox\":[{\"x\":281,\"y\":259},{\"x\":403,\"y\":260},{\"x\":403,\"y\":286},{\"x\":281,\"y\":285}],\"text\":\"192, 50%\"},{\"boundingBox\":[{\"x\":502,\"y\":304},{\"x\":520,\"y\":305},{\"x\":519,\"y\":326},{\"x\":501,\"y\":326}],\"text\":\".\"},{\"boundingBox\":[{\"x\":521,\"y\":302},{\"x\":824,\"y\":301},{\"x\":824,\"y\":330},{\"x\":521,\"y\":331}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":522,\"y\":340},{\"x\":612,\"y\":338},{\"x\":612,\"y\":363},{\"x\":523,\"y\":365}],\"text\":\"posium\"}],\"words\":[{\"boundingBox\":[{\"x\":154,\"y\":4},{\"x\":313,\"y\":3},{\"x\":313,\"y\":30},{\"x\":154,\"y\":29}],\"text\":\"Percentage\"},{\"boundingBox\":[{\"x\":322,\"y\":3},{\"x\":356,\"y\":3},{\"x\":356,\"y\":30},{\"x\":322,\"y\":30}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":361,\"y\":3},{\"x\":525,\"y\":4},{\"x\":524,\"y\":31},{\"x\":361,\"y\":31}],\"text\":\"Publication\"},{\"boundingBox\":[{\"x\":539,\"y\":4},{\"x\":605,\"y\":5},{\"x\":604,\"y\":32},{\"x\":539,\"y\":31}],\"text\":\"Type\"},{\"boundingBox\":[{\"x\":122,\"y\":80},{\"x\":207,\"y\":79},{\"x\":207,\"y\":106},{\"x\":122,\"y\":107}],\"text\":\"34,9%\"},{\"boundingBox\":[{\"x\":520,\"y\":124},{\"x\":610,\"y\":123},{\"x\":611,\"y\":145},{\"x\":520,\"y\":147}],\"text\":\"Journal\"},{\"boundingBox\":[{\"x\":520,\"y\":211},{\"x\":670,\"y\":212},{\"x\":671,\"y\":237},{\"x\":521,\"y\":237}],\"text\":\"Conferences\"},{\"boundingBox\":[{\"x\":20,\"y\":280},{\"x\":76,\"y\":280},{\"x\":76,\"y\":308},{\"x\":20,\"y\":308}],\"text\":\"155,\"},{\"boundingBox\":[{\"x\":82,\"y\":280},{\"x\":137,\"y\":280},{\"x\":137,\"y\":308},{\"x\":81,\"y\":308}],\"text\":\"41%\"},{\"boundingBox\":[{\"x\":282,\"y\":260},{\"x\":337,\"y\":260},{\"x\":337,\"y\":286},{\"x\":281,\"y\":286}],\"text\":\"192,\"},{\"boundingBox\":[{\"x\":342,\"y\":260},{\"x\":397,\"y\":260},{\"x\":397,\"y\":287},{\"x\":342,\"y\":286}],\"text\":\"50%\"},{\"boundingBox\":[{\"x\":501,\"y\":304},{\"x\":513,\"y\":304},{\"x\":513,\"y\":326},{\"x\":501,\"y\":325}],\"text\":\".\"},{\"boundingBox\":[{\"x\":522,\"y\":303},{\"x\":819,\"y\":302},{\"x\":819,\"y\":331},{\"x\":521,\"y\":331}],\"text\":\"Workshop/Technical/Sym\"},{\"boundingBox\":[{\"x\":523,\"y\":340},{\"x\":600,\"y\":338},{\"x\":601,\"y\":364},{\"x\":523,\"y\":366}],\"text\":\"posium\"}]}",
        "{\"language\":\"en\",\"text\":\"90 80 70 60 50 40 w 20 0 No of Researchers Frequency of Researchers 10 0 Italy Canada China India USA Germany France Japan Turkey Republic of Korea Countries Ireland Spain Poland Los Angeles Switzerland Iran Greece Norway\",\"lines\":[{\"boundingBox\":[{\"x\":52,\"y\":40},{\"x\":87,\"y\":38},{\"x\":88,\"y\":69},{\"x\":53,\"y\":71}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":54,\"y\":78},{\"x\":86,\"y\":78},{\"x\":86,\"y\":102},{\"x\":54,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":54,\"y\":114},{\"x\":86,\"y\":113},{\"x\":85,\"y\":137},{\"x\":53,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":55,\"y\":149},{\"x\":85,\"y\":148},{\"x\":84,\"y\":173},{\"x\":54,\"y\":173}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":55,\"y\":187},{\"x\":86,\"y\":186},{\"x\":85,\"y\":211},{\"x\":53,\"y\":211}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":55,\"y\":222},{\"x\":85,\"y\":222},{\"x\":84,\"y\":246},{\"x\":55,\"y\":246}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":56,\"y\":281},{\"x\":55,\"y\":257},{\"x\":72,\"y\":257},{\"x\":72,\"y\":282}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":53,\"y\":295},{\"x\":85,\"y\":294},{\"x\":85,\"y\":318},{\"x\":53,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":65,\"y\":285},{\"x\":66,\"y\":258},{\"x\":83,\"y\":258},{\"x\":82,\"y\":285}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1,\"y\":357},{\"x\":2,\"y\":58},{\"x\":30,\"y\":58},{\"x\":29,\"y\":357}],\"text\":\"No of Researchers\"},{\"boundingBox\":[{\"x\":393,\"y\":2},{\"x\":833,\"y\":1},{\"x\":833,\"y\":35},{\"x\":393,\"y\":39}],\"text\":\"Frequency of Researchers\"},{\"boundingBox\":[{\"x\":52,\"y\":334},{\"x\":86,\"y\":333},{\"x\":85,\"y\":356},{\"x\":53,\"y\":357}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":69,\"y\":394},{\"x\":67,\"y\":366},{\"x\":85,\"y\":366},{\"x\":85,\"y\":394}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":92,\"y\":432},{\"x\":134,\"y\":397},{\"x\":151,\"y\":416},{\"x\":108,\"y\":453}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":124,\"y\":465},{\"x\":192,\"y\":395},{\"x\":209,\"y\":410},{\"x\":139,\"y\":482}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":200,\"y\":446},{\"x\":250,\"y\":395},{\"x\":266,\"y\":412},{\"x\":215,\"y\":463}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":263,\"y\":438},{\"x\":309,\"y\":395},{\"x\":325,\"y\":412},{\"x\":279,\"y\":457}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":331,\"y\":432},{\"x\":369,\"y\":393},{\"x\":387,\"y\":413},{\"x\":351,\"y\":449}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":345,\"y\":478},{\"x\":431,\"y\":399},{\"x\":446,\"y\":417},{\"x\":360,\"y\":495}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":427,\"y\":458},{\"x\":489,\"y\":396},{\"x\":505,\"y\":411},{\"x\":443,\"y\":474}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":494,\"y\":449},{\"x\":548,\"y\":396},{\"x\":566,\"y\":414},{\"x\":511,\"y\":467}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":544,\"y\":456},{\"x\":607,\"y\":398},{\"x\":624,\"y\":417},{\"x\":561,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":505,\"y\":556},{\"x\":664,\"y\":395},{\"x\":685,\"y\":414},{\"x\":524,\"y\":577}],\"text\":\"Republic of Korea\"},{\"boundingBox\":[{\"x\":507,\"y\":597},{\"x\":664,\"y\":597},{\"x\":664,\"y\":624},{\"x\":507,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":657,\"y\":459},{\"x\":723,\"y\":392},{\"x\":744,\"y\":411},{\"x\":677,\"y\":480}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":734,\"y\":445},{\"x\":783,\"y\":396},{\"x\":801,\"y\":415},{\"x\":751,\"y\":465}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":781,\"y\":457},{\"x\":844,\"y\":393},{\"x\":861,\"y\":410},{\"x\":798,\"y\":474}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":796,\"y\":500},{\"x\":904,\"y\":393},{\"x\":924,\"y\":413},{\"x\":816,\"y\":520}],\"text\":\"Los Angeles\"},{\"boundingBox\":[{\"x\":853,\"y\":502},{\"x\":963,\"y\":392},{\"x\":983,\"y\":412},{\"x\":872,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":987,\"y\":431},{\"x\":1026,\"y\":396},{\"x\":1039,\"y\":411},{\"x\":1001,\"y\":448}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1018,\"y\":462},{\"x\":1083,\"y\":397},{\"x\":1098,\"y\":412},{\"x\":1033,\"y\":477}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":463},{\"x\":1145,\"y\":396},{\"x\":1164,\"y\":416},{\"x\":1088,\"y\":484}],\"text\":\"Norway\"}],\"words\":[{\"boundingBox\":[{\"x\":52,\"y\":40},{\"x\":83,\"y\":38},{\"x\":85,\"y\":69},{\"x\":53,\"y\":71}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":54,\"y\":78},{\"x\":81,\"y\":78},{\"x\":81,\"y\":103},{\"x\":54,\"y\":103}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":54,\"y\":113},{\"x\":81,\"y\":113},{\"x\":81,\"y\":137},{\"x\":54,\"y\":137}],\"text\":\"70\"},{\"boundingBox\":[{\"x\":54,\"y\":148},{\"x\":81,\"y\":148},{\"x\":81,\"y\":173},{\"x\":54,\"y\":173}],\"text\":\"60\"},{\"boundingBox\":[{\"x\":53,\"y\":186},{\"x\":80,\"y\":186},{\"x\":80,\"y\":211},{\"x\":53,\"y\":211}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":55,\"y\":222},{\"x\":81,\"y\":222},{\"x\":81,\"y\":246},{\"x\":55,\"y\":246}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":56,\"y\":281},{\"x\":55,\"y\":271},{\"x\":72,\"y\":270},{\"x\":73,\"y\":281}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":53,\"y\":295},{\"x\":80,\"y\":294},{\"x\":81,\"y\":318},{\"x\":54,\"y\":319}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":66,\"y\":270},{\"x\":66,\"y\":259},{\"x\":83,\"y\":260},{\"x\":83,\"y\":271}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1,\"y\":357},{\"x\":1,\"y\":314},{\"x\":28,\"y\":314},{\"x\":28,\"y\":357}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1,\"y\":302},{\"x\":1,\"y\":268},{\"x\":29,\"y\":269},{\"x\":29,\"y\":303}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1,\"y\":262},{\"x\":2,\"y\":60},{\"x\":29,\"y\":62},{\"x\":29,\"y\":262}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":395,\"y\":2},{\"x\":572,\"y\":3},{\"x\":572,\"y\":39},{\"x\":393,\"y\":39}],\"text\":\"Frequency\"},{\"boundingBox\":[{\"x\":582,\"y\":3},{\"x\":618,\"y\":3},{\"x\":617,\"y\":38},{\"x\":581,\"y\":38}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":624,\"y\":3},{\"x\":833,\"y\":2},{\"x\":833,\"y\":30},{\"x\":624,\"y\":38}],\"text\":\"Researchers\"},{\"boundingBox\":[{\"x\":52,\"y\":334},{\"x\":80,\"y\":333},{\"x\":80,\"y\":356},{\"x\":53,\"y\":357}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":68,\"y\":394},{\"x\":68,\"y\":384},{\"x\":86,\"y\":383},{\"x\":86,\"y\":393}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":93,\"y\":432},{\"x\":133,\"y\":399},{\"x\":149,\"y\":418},{\"x\":109,\"y\":452}],\"text\":\"Italy\"},{\"boundingBox\":[{\"x\":124,\"y\":464},{\"x\":190,\"y\":398},{\"x\":206,\"y\":415},{\"x\":140,\"y\":482}],\"text\":\"Canada\"},{\"boundingBox\":[{\"x\":200,\"y\":445},{\"x\":248,\"y\":398},{\"x\":264,\"y\":415},{\"x\":215,\"y\":462}],\"text\":\"China\"},{\"boundingBox\":[{\"x\":263,\"y\":439},{\"x\":307,\"y\":396},{\"x\":324,\"y\":414},{\"x\":280,\"y\":457}],\"text\":\"India\"},{\"boundingBox\":[{\"x\":332,\"y\":432},{\"x\":364,\"y\":400},{\"x\":383,\"y\":419},{\"x\":351,\"y\":450}],\"text\":\"USA\"},{\"boundingBox\":[{\"x\":346,\"y\":480},{\"x\":430,\"y\":400},{\"x\":447,\"y\":418},{\"x\":362,\"y\":495}],\"text\":\"Germany\"},{\"boundingBox\":[{\"x\":428,\"y\":458},{\"x\":486,\"y\":401},{\"x\":500,\"y\":418},{\"x\":444,\"y\":474}],\"text\":\"France\"},{\"boundingBox\":[{\"x\":494,\"y\":448},{\"x\":545,\"y\":399},{\"x\":563,\"y\":417},{\"x\":511,\"y\":467}],\"text\":\"Japan\"},{\"boundingBox\":[{\"x\":544,\"y\":457},{\"x\":606,\"y\":400},{\"x\":623,\"y\":418},{\"x\":563,\"y\":475}],\"text\":\"Turkey\"},{\"boundingBox\":[{\"x\":505,\"y\":555},{\"x\":582,\"y\":478},{\"x\":601,\"y\":498},{\"x\":525,\"y\":577}],\"text\":\"Republic\"},{\"boundingBox\":[{\"x\":587,\"y\":473},{\"x\":606,\"y\":454},{\"x\":625,\"y\":474},{\"x\":606,\"y\":494}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":610,\"y\":450},{\"x\":664,\"y\":396},{\"x\":683,\"y\":415},{\"x\":629,\"y\":470}],\"text\":\"Korea\"},{\"boundingBox\":[{\"x\":507,\"y\":600},{\"x\":661,\"y\":598},{\"x\":660,\"y\":625},{\"x\":508,\"y\":625}],\"text\":\"Countries\"},{\"boundingBox\":[{\"x\":658,\"y\":459},{\"x\":722,\"y\":395},{\"x\":741,\"y\":415},{\"x\":678,\"y\":480}],\"text\":\"Ireland\"},{\"boundingBox\":[{\"x\":734,\"y\":446},{\"x\":782,\"y\":398},{\"x\":801,\"y\":417},{\"x\":752,\"y\":465}],\"text\":\"Spain\"},{\"boundingBox\":[{\"x\":782,\"y\":457},{\"x\":841,\"y\":398},{\"x\":858,\"y\":415},{\"x\":798,\"y\":474}],\"text\":\"Poland\"},{\"boundingBox\":[{\"x\":797,\"y\":501},{\"x\":824,\"y\":475},{\"x\":844,\"y\":493},{\"x\":817,\"y\":520}],\"text\":\"Los\"},{\"boundingBox\":[{\"x\":828,\"y\":471},{\"x\":904,\"y\":394},{\"x\":925,\"y\":415},{\"x\":848,\"y\":489}],\"text\":\"Angeles\"},{\"boundingBox\":[{\"x\":853,\"y\":503},{\"x\":960,\"y\":396},{\"x\":979,\"y\":416},{\"x\":873,\"y\":521}],\"text\":\"Switzerland\"},{\"boundingBox\":[{\"x\":987,\"y\":432},{\"x\":1019,\"y\":402},{\"x\":1034,\"y\":418},{\"x\":1002,\"y\":448}],\"text\":\"Iran\"},{\"boundingBox\":[{\"x\":1019,\"y\":462},{\"x\":1080,\"y\":401},{\"x\":1094,\"y\":417},{\"x\":1034,\"y\":476}],\"text\":\"Greece\"},{\"boundingBox\":[{\"x\":1070,\"y\":464},{\"x\":1143,\"y\":398},{\"x\":1161,\"y\":420},{\"x\":1089,\"y\":484}],\"text\":\"Norway\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 06 June 2019\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":14},{\"x\":889,\"y\":14},{\"x\":889,\"y\":70},{\"x\":4,\"y\":70}],\"text\":\"Published online: 06 June 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":270,\"y\":15},{\"x\":270,\"y\":71},{\"x\":4,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":289,\"y\":15},{\"x\":495,\"y\":15},{\"x\":495,\"y\":71},{\"x\":290,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":506,\"y\":15},{\"x\":574,\"y\":15},{\"x\":575,\"y\":71},{\"x\":506,\"y\":71}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":594,\"y\":15},{\"x\":725,\"y\":15},{\"x\":726,\"y\":71},{\"x\":594,\"y\":71}],\"text\":\"June\"},{\"boundingBox\":[{\"x\":744,\"y\":15},{\"x\":886,\"y\":15},{\"x\":887,\"y\":70},{\"x\":745,\"y\":71}],\"text\":\"2019\"}]}"
      ]
    },
    {
      "@search.score": 2.3053336,
      "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WS",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQxMTA5LTAyMC0wMDMzMC14LnBkZg2",
      "metadata_author": " Shun Kodate ",
      "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network",
      "people": [
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "Anderson",
        "Kodate",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "Akoglu",
        "Pu",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "Savage",
        "Šubelj",
        "Dreżewski",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "Van Vlasselaer",
        "Bhat",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Van Vlas",
        "selaer",
        "Li",
        "Monamo",
        "Ferrara",
        "Hu",
        "Yan",
        "ini",
        "kouti",
        "vi",
        "vj",
        "sini",
        "souti",
        "Palla",
        "Milo",
        "Breiman",
        "Hastie",
        "Pedregosa",
        "Strobl",
        "Altmann",
        "Ci"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Computational social science Open Access",
        "other third party material",
        "eight local network indices",
        "various online transaction platforms",
        "Applied Network Science",
        "Creative Commons licence",
        "random forest classifiers",
        "Appl Netw Sci",
        "eight network indices",
        "present descriptive analysis",
        "credit card fraud",
        "Network analysis",
        "present study",
        "problematic transaction",
        "appropriate credit",
        "credit line",
        "online marketplaces",
        "Shun Kodate",
        "Ryusuke Chiba",
        "Shunya Kimura3",
        "Naoki Masuda",
        "rapid growth",
        "electronic transactions",
        "communi- cations",
        "dramatic speed",
        "daily lives",
        "UK Parliament",
        "recent era",
        "money laundering",
        "computer intrusion",
        "typical approach",
        "individual transactions",
        "traditional approach",
        "corresponding buyer",
        "several hundreds",
        "similar number",
        "focal node",
        "twelve features",
        "four types",
        "classification performance",
        "Machine learning",
        "author(s",
        "statutory regulation",
        "copyright holder",
        "RESEARCH Kodate",
        "Full list",
        "malicious users",
        "fraudulent users",
        "normal users",
        "online consumer",
        "intended use",
        "permitted use",
        "Fraud detection",
        "illegal items",
        "consumer marketplace",
        "egocentric networks",
        "author information",
        "orcid.org",
        "user profiles",
        "fraud activity",
        "Introduction",
        "tandem",
        "cybercrimes",
        "billions",
        "dollars",
        "year",
        "security",
        "society",
        "McAfee",
        "system",
        "Anderson",
        "dimension",
        "ranges",
        "plagiarism",
        "Abstract",
        "Providers",
        "behavior",
        "texts",
        "background",
        "frauds",
        "seller",
        "edge",
        "up",
        "connectivity",
        "neighbors",
        "aim",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "original",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "buffalo",
        "4 Department",
        "Mathematics",
        "University",
        "USA",
        "dialog",
        "Page",
        "18Kodate",
        "13 million monthly active users",
        "credit card systems",
        "social security system",
        "various other fields",
        "online review forums",
        "large online consumer",
        "machine learning algorithms",
        "online social networks",
        "mobile phone network",
        "credit card transactions",
        "suspicious connectivity patterns",
        "online auction system",
        "fraud detec- tion",
        "fraud detection techniques",
        "online C2C marketplaces",
        "online auctions",
        "supervised learning",
        "telecommunica- tion",
        "transaction networks",
        "C2C) marketplace",
        "Standard practice",
        "transaction amount",
        "item category",
        "call duration",
        "call type",
        "geographical region",
        "transaction history",
        "advanced fraudsters",
        "Google LLC",
        "particular words",
        "anomalous behavior",
        "one jargon",
        "alternative way",
        "graph-theoretic quantities",
        "Dreżewski",
        "car- booking",
        "Van Vlasselaer",
        "cryptocurrency transaction",
        "accom- plices",
        "bipartite cores",
        "belief propagation",
        "133 billion yen",
        "1.2 billion USD",
        "quarter year",
        "previous studies",
        "anomalous users",
        "fraudulent samples",
        "health-care data",
        "empirical data",
        "data set",
        "statistical methods",
        "transaction frauds",
        "reputation frauds",
        "statistical classifier",
        "Exemplar features",
        "Computational",
        "decades",
        "Bolton",
        "Hand",
        "Phua",
        "Abdallah",
        "West",
        "Bhattacharya",
        "case",
        "day",
        "week",
        "address",
        "number",
        "calls",
        "age",
        "gender",
        "Akoglu",
        "eyes",
        "administrators",
        "authorities",
        "signature",
        "Webb",
        "Hayes",
        "Bhowmick",
        "Hazarika",
        "example",
        "authority",
        "drug",
        "idea",
        "nodes",
        "goods",
        "scores",
        "expectation",
        "insurance",
        "Šubelj",
        "Colladon",
        "Remondi",
        "Liu",
        "Shchur",
        "advertising",
        "Ferrara",
        "Abulaish",
        "Jiang",
        "Hooi",
        "Rasheed",
        "Wang",
        "Chau",
        "Pandit",
        "Chiu",
        "Bangcharoensap",
        "Yanchun",
        "Monamo",
        "reputations",
        "authors",
        "eBay",
        "Mercari",
        "Japan",
        "Many prior network-based fraud detection algorithms",
        "online C2C marketplace service",
        "one problematic sell",
        "local clustering coefficient",
        "local infor- mation",
        "two fraudulent users",
        "two normal users",
        "two indices",
        "sell probability",
        "local indices",
        "two users",
        "local features",
        "connected components",
        "Yan- chun",
        "commercial implementations",
        "various items",
        "United States",
        "Japanese market",
        "following types",
        "non-existing items",
        "medicinal supplies",
        "weighted network",
        "egocentric network",
        "eight indices",
        "trans- actions",
        "separate index",
        "three indices",
        "users’ network",
        "node vi",
        "node strength",
        "methods Data",
        "normal transactions",
        "lematic transactions",
        "Fictive transactions",
        "mean number",
        "focal user",
        "directed edge",
        "networks",
        "global",
        "communities",
        "k-cores",
        "Bhat",
        "Savage",
        "Others",
        "degree",
        "triangles",
        "Materials",
        "July",
        "January",
        "addition",
        "underwear",
        "medicine",
        "weapon",
        "perspective",
        "morality",
        "law",
        "crime",
        "Table",
        "buyer",
        "Figure",
        "Fig.",
        "edges",
        "pairs",
        "information",
        "Barrat",
        "direction",
        "SPi",
        "local clustering coefficient Ci",
        "Random forest classifier",
        "unweighted trian- gles",
        "other three indices",
        "one feedforward triangle",
        "one cyclic triangle",
        "local network",
        "triangle congregation",
        "other four",
        "one item",
        "k ini",
        "weighted version",
        "overlapping set",
        "three-node subnetworks",
        "work motifs",
        "natural interpretation",
        "three nodes",
        "unweighted triangles",
        "two triangles",
        "feedforward triangles",
        "cyclic triangles",
        "many users",
        "different directed",
        "particular neighbors",
        "Such neighbors",
        "bidirectional edges",
        "ki(ki",
        "degree zero",
        "proportion",
        "kouti",
        "Examples",
        "vj",
        "contribution",
        "sini",
        "buys",
        "souti",
        "sells",
        "abundance",
        "undirected",
        "Newman",
        "increase",
        "community",
        "Radicchi",
        "Palla",
        "hypothesis",
        "extent",
        "concept",
        "mi",
        "Ciki",
        "Note",
        "Frequencies",
        "marketplace",
        "contrast",
        "index",
        "cycle",
        "CYPi",
        "CYi",
        "definition",
        "calculation",
        "weights",
        "random forest clas- sifier",
        "precision–recall (PR) curve",
        "random forest classifier",
        "ensemble learning method",
        "classifica- tion performance",
        "receiver operating characteristic",
        "two performance measures",
        "true positive rate",
        "false positive rate",
        "piecewise linear manner",
        "same test sample",
        "tive training samples",
        "ROC) curve",
        "ROC curve",
        "two sets",
        "same order",
        "single-tree positive",
        "tive samples",
        "test data",
        "positive probability",
        "multiple classifiers",
        "decision-tree classifiers",
        "best split",
        "triangle patterns",
        "Cyclic triangle",
        "three-node patterns",
        "directed triangles",
        "reciprocal edges",
        "fraudulent user",
        "fraudulent types",
        "precision range",
        "descending order",
        "horizontal axis",
        "vertical axis",
        "test samples",
        "training data",
        "fraudulent) samples",
        "terminal node",
        "negative probability",
        "candidate features",
        "decision trees",
        "Feedforward triangle",
        "300 trees",
        "ties",
        "Breiman",
        "Hastie",
        "scikit-learn",
        "Pedregosa",
        "overfitting",
        "basis",
        "replace",
        "ment",
        "fraction",
        "average",
        "count",
        "Five",
        "numbers",
        "figure",
        "The",
        "2",
        "100 random forest classifiers",
        "accurate binary classifier",
        "random number generator",
        "ten different permutations",
        "ten permutations",
        "different types",
        "PR curves",
        "grid search",
        "10-fold cross-validation",
        "maximum depth",
        "max_depth parameter",
        "other hyperparameters",
        "default values",
        "parameter optimization",
        "sampling bias",
        "sampling scheme",
        "Descriptive statistics",
        "survival probability",
        "frac- tion",
        "one transaction",
        "clear distinction",
        "large value",
        "The AUC",
        "lent users",
        "mal users",
        "minimum number",
        "seed number",
        "single set",
        "degree distribution",
        "unweighted degree",
        "AUC value",
        "permutation importance",
        "nal nodes",
        "scikit-learn version",
        "useful information",
        "user type",
        "training set",
        "performance measure",
        "degree ki",
        "normal",
        "area",
        "good",
        "Strobl",
        "Altmann",
        "method",
        "decrease",
        "tree",
        "integers",
        "split",
        "max_features",
        "n_estimators",
        "Results",
        "specified",
        "classification",
        "difference",
        "many",
        "fact",
        "≥",
        "Normal Fictive Underwear Medicine Weapon",
        "unweighted sell probability",
        "different user types",
        "less frequent transactions",
        "one sell transaction",
        "Seed user type",
        "large fraction",
        "smaller value",
        "characteristic behavior",
        "first column",
        "exclusive buyer",
        "fraudulent type",
        "seed users",
        "sell transactions",
        "c Strength",
        "specific neighbor",
        "average number",
        "Total number",
        "Table 1 Properties",
        "conditional mean",
        "Degree",
        "ki",
        "majority",
        "result",
        "ence",
        "median",
        "FFi",
        "samples",
        "peak",
        "smallest possible weighted sell probability",
        "smallest possible out-degree",
        "corresponding frac- tions",
        "tinct frequency distributions",
        "Survival probability",
        "cycle probability",
        "dashed lines",
        "two aspects",
        "larger value",
        "clear difference",
        "descriptive statistics",
        "ous section",
        "horizontal line",
        "Figure  5b",
        "Figure 5b",
        "second feature",
        "relationships",
        "buyers",
        "one",
        "transactions",
        "neighbor",
        "results",
        "analysis",
        "tendency",
        "Classification",
        "12 features",
        "Fig. 8 Cycle probability",
        "Fig. 7 Triangle congregation",
        "b Survival probability",
        "third feature",
        "real number",
        "fourth feature",
        "fifth feature",
        "sixth feature",
        "seventh feature",
        "eighth feature",
        "ini",
        "WS"
      ],
      "merged_content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci            (2020) 5:90  \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence:   \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n a b O- 0 0404 O C 0 O 0 20 04 20070 \n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nwhere k ini  is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti  is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti  is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini  is node vi ’s weighted in-degree (i.e., the number of buys) and souti  is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci            (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci            (2020) 5:90  \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WS",
      "text": [
        "a b O- 0 0404 O C 0 O 0 20 04 20070",
        "Published online: 16 November 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"a b O- 0 0404 O C 0 O 0 20 04 20070\",\"lines\":[{\"boundingBox\":[{\"x\":40,\"y\":30},{\"x\":67,\"y\":28},{\"x\":67,\"y\":57},{\"x\":40,\"y\":59}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":750,\"y\":23},{\"x\":778,\"y\":24},{\"x\":777,\"y\":58},{\"x\":750,\"y\":57}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1219,\"y\":129},{\"x\":1211,\"y\":157},{\"x\":1186,\"y\":148},{\"x\":1195,\"y\":120}],\"text\":\"O-\"},{\"boundingBox\":[{\"x\":1063,\"y\":151},{\"x\":1075,\"y\":190},{\"x\":1058,\"y\":197},{\"x\":1045,\"y\":158}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1249,\"y\":176},{\"x\":1182,\"y\":262},{\"x\":1163,\"y\":246},{\"x\":1226,\"y\":160}],\"text\":\"0404\"},{\"boundingBox\":[{\"x\":772,\"y\":925},{\"x\":772,\"y\":962},{\"x\":754,\"y\":965},{\"x\":752,\"y\":928}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":37,\"y\":933},{\"x\":69,\"y\":933},{\"x\":71,\"y\":962},{\"x\":40,\"y\":962}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1080,\"y\":1074},{\"x\":1083,\"y\":1123},{\"x\":1053,\"y\":1124},{\"x\":1050,\"y\":1074}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1116,\"y\":1186},{\"x\":1142,\"y\":1172},{\"x\":1150,\"y\":1190},{\"x\":1123,\"y\":1203}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1261,\"y\":1349},{\"x\":1346,\"y\":1351},{\"x\":1346,\"y\":1372},{\"x\":1261,\"y\":1370}],\"text\":\"0 20\"},{\"boundingBox\":[{\"x\":887,\"y\":1375},{\"x\":939,\"y\":1369},{\"x\":942,\"y\":1393},{\"x\":888,\"y\":1398}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1005,\"y\":1454},{\"x\":928,\"y\":1595},{\"x\":903,\"y\":1581},{\"x\":978,\"y\":1439}],\"text\":\"20070\"}],\"words\":[{\"boundingBox\":[{\"x\":40,\"y\":30},{\"x\":57,\"y\":29},{\"x\":59,\"y\":57},{\"x\":41,\"y\":58}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":750,\"y\":23},{\"x\":769,\"y\":24},{\"x\":768,\"y\":58},{\"x\":750,\"y\":57}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1219,\"y\":131},{\"x\":1212,\"y\":154},{\"x\":1187,\"y\":146},{\"x\":1194,\"y\":123}],\"text\":\"O-\"},{\"boundingBox\":[{\"x\":1071,\"y\":175},{\"x\":1075,\"y\":186},{\"x\":1057,\"y\":192},{\"x\":1053,\"y\":180}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1238,\"y\":190},{\"x\":1183,\"y\":259},{\"x\":1165,\"y\":247},{\"x\":1216,\"y\":174}],\"text\":\"0404\"},{\"boundingBox\":[{\"x\":772,\"y\":931},{\"x\":772,\"y\":942},{\"x\":752,\"y\":943},{\"x\":752,\"y\":931}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":38,\"y\":933},{\"x\":55,\"y\":933},{\"x\":55,\"y\":962},{\"x\":38,\"y\":962}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1083,\"y\":1105},{\"x\":1084,\"y\":1120},{\"x\":1054,\"y\":1122},{\"x\":1053,\"y\":1106}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1128,\"y\":1178},{\"x\":1139,\"y\":1172},{\"x\":1148,\"y\":1190},{\"x\":1137,\"y\":1196}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1278,\"y\":1351},{\"x\":1291,\"y\":1351},{\"x\":1288,\"y\":1371},{\"x\":1275,\"y\":1370}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1310,\"y\":1352},{\"x\":1346,\"y\":1352},{\"x\":1344,\"y\":1373},{\"x\":1307,\"y\":1372}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":902,\"y\":1372},{\"x\":933,\"y\":1369},{\"x\":936,\"y\":1393},{\"x\":904,\"y\":1396}],\"text\":\"04\"},{\"boundingBox\":[{\"x\":1003,\"y\":1458},{\"x\":929,\"y\":1593},{\"x\":904,\"y\":1579},{\"x\":976,\"y\":1443}],\"text\":\"20070\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 16 November 2020\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":1059,\"y\":16},{\"x\":1059,\"y\":70},{\"x\":5,\"y\":69}],\"text\":\"Published online: 16 November 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":6,\"y\":17},{\"x\":268,\"y\":17},{\"x\":269,\"y\":70},{\"x\":6,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":290,\"y\":17},{\"x\":501,\"y\":17},{\"x\":501,\"y\":70},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":512,\"y\":17},{\"x\":575,\"y\":17},{\"x\":575,\"y\":70},{\"x\":513,\"y\":70}],\"text\":\"16\"},{\"boundingBox\":[{\"x\":593,\"y\":17},{\"x\":899,\"y\":17},{\"x\":899,\"y\":71},{\"x\":594,\"y\":71}],\"text\":\"November\"},{\"boundingBox\":[{\"x\":911,\"y\":17},{\"x\":1051,\"y\":17},{\"x\":1051,\"y\":71},{\"x\":911,\"y\":71}],\"text\":\"2020\"}]}"
      ]
    },
    {
      "@search.score": 1.9367393,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. ",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczEzNjczLTAxOS0wMTc3LTYucGRm0",
      "metadata_author": "Chunyong Yin ",
      "metadata_title": "Mobile marketing recommendation method based on user location feedback",
      "people": [
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "Zhu",
        "Yin",
        "Lian",
        "Lee",
        "Gemmis",
        "Semeraro",
        "lem",
        "Markov",
        "Relu",
        "Zeiler",
        "bk",
        "nario"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "current mobile marketing recommendation system",
        "Sequential behavior Open Access",
        "Mobile marketing recommendation method",
        "users’ timing preference characteristics",
        "Location-based mobile marketing recommendation",
        "potential mobile user preferences",
        "Creative Commons license",
        "convolutional neural network model",
        "users’ location feedback behavior",
        "user location feedback data",
        "mobile recommendation system",
        "traditional recommendation models",
        "user preference information",
        "users’ location-based behaviors",
        "mobile information overload",
        "user-product binary matrix",
        "original author(s",
        "different geographical locations",
        "shopping location information",
        "famous e-commerce platforms",
        "physical store products",
        "different time windows",
        "users’ location information",
        "Hum. Cent. Comput",
        "mobile network",
        "information recommendation",
        "different preferences",
        "mobile users",
        "convolutional model",
        "author information",
        "irrelevant information",
        "binary relationships",
        "different dimensions",
        "behavioral data",
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "recent years",
        "e-commerce industry",
        "one hand",
        "various products",
        "other hand",
        "large amount",
        "next consumer",
        "recommended methods",
        "third factor",
        "limited check",
        "timeli- ness",
        "hot spots",
        "accuracy rate",
        "recall rate",
        "iveco mmons",
        "unrestricted use",
        "appropriate credit",
        "RESEARCH Yin",
        "Inf. Sci.",
        "Communication Engineering",
        "Full list",
        "individual needs",
        "experimental results",
        "Changsha University",
        "orcid.org",
        "Introduction",
        "popularization",
        "Internet",
        "Alibaba",
        "zon",
        "home",
        "sellers",
        "goods",
        "costs",
        "wealth",
        "consumption",
        "case",
        "advantage",
        "terms",
        "Abstract",
        "attribute",
        "role",
        "paper",
        "LBCNN",
        "extractor",
        "classifier",
        "Keywords",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "jinwang",
        "2 School",
        "Computer",
        "Science",
        "Technology",
        "China",
        "licenses",
        "dialog",
        "Page",
        "17Yin",
        "means",
        "problem",
        "implied feature-based cognitive feature collaborative filter- ing",
        "location-aware based generation probability model",
        "collaborative filtering methods",
        "biological neural networks",
        "Convolutional neural networks",
        "advanced abstract features",
        "loca- tion information",
        "accurate personalized recommendations",
        "small-scale training data",
        "detailed comparative analysis",
        "user informa- tion",
        "mobile marketing recommendation",
        "original input features",
        "business location information",
        "user preference model",
        "mobile recommendation research",
        "feature size",
        "deep learning algorithms",
        "mobile users’ preferences",
        "ing models",
        "deep model",
        "DL) model",
        "shallow model",
        "multimedia recommendations",
        "mobile environments",
        "accurate prediction",
        "local features",
        "overall features",
        "two-dimensional features",
        "user preferences",
        "location recommendation",
        "convolutional layer",
        "spatial information",
        "product information",
        "user information",
        "hot topic",
        "service resources",
        "different assumptions",
        "spatial activities",
        "location movement",
        "negative samples",
        "semantic content",
        "data size",
        "mation analysis",
        "long-term interest",
        "time stamps",
        "great achievements",
        "speech tasks",
        "novel field",
        "interventional optimization",
        "artificial intelligence",
        "great breakthroughs",
        "similar effects",
        "Weight sharing",
        "great advantages",
        "user needs",
        "positional relevance",
        "location relevance",
        "first layer",
        "previous layer",
        "convolution kernel",
        "pooling layer",
        "context information",
        "ventional methods",
        "user models",
        "many researches",
        "two layers",
        "network structure",
        "shortcomings",
        "Zhu",
        "retrograde",
        "approach",
        "Yin",
        "LA-LDA",
        "scoring",
        "items",
        "studies",
        "distance",
        "merchant",
        "area",
        "proximity",
        "importance",
        "gap",
        "core",
        "Lian",
        "impact",
        "author",
        "project",
        "Lee",
        "visual",
        "study",
        "aspects",
        "machine",
        "Experiments",
        "characteristics",
        "CNN",
        "difficulty",
        "number",
        "weights",
        "neuron",
        "Hum",
        "Cent",
        "Comput",
        "images",
        "location-based",
        "Location-based mobile marketing recommendation model",
        "Content‑based recommendation method",
        "convolutional neural network models",
        "Representative content-based recommendation systems",
        "general products recommendation system",
        "Traditional recommendation method",
        "traditional recommendation algorithm",
        "content-based recommendation algorithm",
        "similar Top-N products",
        "timing preference characteristics",
        "interest feature vector",
        "Content-based information filtering",
        "Experimental analysis” section",
        "item feature vector",
        "project feature vector",
        "other existing methods",
        "cosine similarity algorithm",
        "user feature vector",
        "Related work” section",
        "users’ timing preferences",
        "User description file",
        "feature models",
        "Content-based methods",
        "content filtering",
        "content characteristics",
        "other methods",
        "user-product information",
        "collaborative filtering",
        "feedback information",
        "hybrid methods",
        "historical content",
        "training sample",
        "next moment",
        "time window",
        "overall preferences",
        "Top-K sample",
        "four sections",
        "Necessary definitions",
        "specific implementation",
        "future progress",
        "current chapter",
        "three parts",
        "time series",
        "text documents",
        "accurate comparisons",
        "different texts",
        "lowing aspects",
        "Hum. Cent",
        "project profile",
        "user u",
        "similarity calculation",
        "sample features",
        "cast results",
        "effective application",
        "calculation formula",
        "two-class problem",
        "purchase behavior",
        "category",
        "order",
        "merchandise",
        "length",
        "test",
        "Remain",
        "sion",
        "strengths",
        "weaknesses",
        "plans",
        "establishment",
        "differences",
        "research",
        "threshold",
        "a.",
        "topics",
        "projects",
        "purchasing",
        "operation",
        "addition",
        "Lops",
        "Gemmis",
        "Semeraro",
        "content filtering based recommendation method",
        "m * n matrix R",
        "traditional data mining method",
        "user historical behavior data",
        "traditional collaborative filtering method",
        "likely N objects",
        "user behavior data",
        "various draw- backs",
        "nearest neigh- bor",
        "conven- tional method",
        "association rule algorithm",
        "higher evaluation algorithms",
        "hybrid recommendation algorithm",
        "similar user sets",
        "mutual user relationships",
        "content-based recommendation method",
        "Hybrid recommendation method",
        "other similar users",
        "different recommendation algorithms",
        "The matrix  Umn",
        "content-based filtering",
        "other method",
        "hybrid method",
        "matrix decomposition",
        "different relationships",
        "One method",
        "new content",
        "evaluation standard",
        "association rules",
        "recommendation problem",
        "standard recommendation",
        "two algorithms",
        "different ways",
        "target user",
        "user neighbor",
        "cold-start issues",
        "same type",
        "same partition",
        "expert research",
        "three steps",
        "recommendation results",
        "ommended items",
        "normal methods",
        "supervised learning",
        "online stores",
        "informa- tion",
        "new users",
        "two types",
        "two specific",
        "compensa- tion",
        "different values",
        "common features",
        "rating sparsity",
        "erence information",
        "new projects",
        "mendations",
        "availability",
        "context",
        "party",
        "requirements",
        "merits",
        "quality",
        "memory",
        "following",
        "collection",
        "feedback",
        "range",
        "U1n",
        "U2n",
        "classification",
        "survey",
        "Amazon",
        "personalized",
        "customer",
        "ratings",
        "techniques",
        "performance",
        "ally",
        "degree",
        "improvement",
        "complexity",
        "different timing behavior characteristics",
        "good recom- mendations",
        "relevant professional research",
        "T-time consumer behavior",
        "behavioral counting feature",
        "duplication counting feature",
        "feature statistics window",
        "historical behavior data",
        "timing behavior data",
        "user position feedback",
        "similar behavioral items",
        "collaborative filtering algorithm",
        "next purchase behavior",
        "feature statistics method",
        "user behavior record",
        "time series behavior",
        "timing recommendation model",
        "user interface layer",
        "U, V data",
        "high quality rules",
        "last behavior",
        "behavior count",
        "Different actions",
        "different times",
        "U data",
        "next section",
        "current window",
        "feature group",
        "product purchase",
        "recommendation process",
        "recommendation systems",
        "ready-made data",
        "data sets",
        "many years",
        "core idea",
        "rule algorithms",
        "total project",
        "disjoint sets",
        "association rule",
        "two criteria",
        "transac- tions",
        "key- word",
        "description layer",
        "main function",
        "strong dependence",
        "most systems",
        "time information",
        "time label",
        "Markov chain",
        "full use",
        "prediction problem",
        "time T",
        "three groups",
        "processing manner",
        "cumulative measure",
        "keyword layer",
        "user layer",
        "unknown items",
        "traditional algorithm",
        "preference features",
        "corresponding features",
        "location visit",
        "total number",
        "∩V",
        "V.",
        "∪ V",
        "business",
        "effect",
        "quantity",
        "focus",
        "transactions",
        "statement",
        "strength",
        "support",
        "confidence",
        "ratio",
        "Formula",
        "interest",
        "other",
        "TOP-N",
        "presentation",
        "attributes",
        "dependencies",
        "keywords",
        "resource",
        "users",
        "dataset",
        "explanation",
        "Construction",
        "Fig.",
        "example",
        "user1",
        "Table",
        "False",
        "behaviors",
        "Location‑based mobile marketing recommendation model",
        "Counting feature Mean feature Ratio feature",
        "location-based mobile marketing recom",
        "Table 1 Characteristic system diagram",
        "collaborative filtering calculation process",
        "False True User feature",
        "K user preference feature",
        "evaluation crite- rion",
        "ior preference feature",
        "duplication count feature",
        "repetitive behavioral data",
        "different feature maps",
        "Specific implementa- tion",
        "pool- ing layer",
        "multi-window convolutional layer",
        "real-world work applications",
        "location visit behavior",
        "N time windows",
        "multi-window convolution layer",
        "user characteristics group",
        "products’ total visit",
        "user position features",
        "counting features",
        "Feature group",
        "True False",
        "product location",
        "behavioral count",
        "feature expression",
        "feature vector",
        "network structures",
        "input feature",
        "Model framework",
        "different lengths",
        "Product feature",
        "mean-type features",
        "output layer",
        "time axis",
        "input layer",
        "total behavior",
        "current research",
        "low difficulty",
        "mendation quality",
        "following sections",
        "important aspects",
        "next step",
        "evant definitions",
        "timing sensitivity",
        "timing behav",
        "four layers",
        "two-dimensional plane",
        "ture plane",
        "The model",
        "Specific information",
        "average number",
        "user-product behavior",
        "product preferences",
        "relevant definition",
        "K.",
        "Definition 1",
        "Definition 2",
        "visits",
        "activity",
        "popularity",
        "method",
        "usable",
        "achievement",
        "speed",
        "avgui",
        "Ij",
        "True/False",
        "sure",
        "correctness",
        "basis",
        "above",
        "analysis",
        "eigenvector",
        "commodity",
        "matrix",
        "new location-based marketing resources recommendation",
        "location-based mobile marketing resources",
        "M convolution kernels",
        "convolu- tion kernel",
        "gradient descent method",
        "convolution kernel w",
        "C × M matrix",
        "nonlinear activation function",
        "kth implicit class",
        "implicit semantic model",
        "maximum pooling operation",
        "latent factor vector",
        "training data set",
        "latent factor model",
        "traditional CNN parameters",
        "likelihood probability value",
        "i-th feature fi",
        "kth product",
        "convolutional kernel",
        "training method",
        "Dropout method",
        "CNN inputs",
        "i-th category",
        "loss function",
        "i-th sample",
        "Feature map",
        "pooled feature",
        "training results",
        "window length",
        "offset term",
        "maximum features",
        "C categories",
        "M-dimensional vector",
        "k-th offset",
        "real category",
        "two items",
        "same time",
        "same class",
        "hidden classes",
        "Specific implementation",
        "two processes",
        "two parts",
        "top module",
        "historical data",
        "other module",
        "second pro",
        "traditional LFM",
        "L2-norm regularization",
        "smoothing problems",
        "training process",
        "dation process",
        "model parameters",
        "Probability distribution",
        "real number",
        "weight parameter",
        "many users",
        "user interest",
        "∑C",
        "j.",
        "framework",
        "ReLu",
        "Tanh",
        "Definition",
        "network",
        "fk",
        "bk",
        "teristic",
        "Zeiler",
        "fitting",
        "neurons",
        "behavior",
        "high",
        "Pool_feature",
        "log",
        "rui",
        "pTu",
        "relationship",
        "outputs",
        "excessive",
        "σ",
        "θ",
        "existing authoritative standard training set",
        "Gradient Boosting Regression Tree Model",
        "mining users’ temporal behavior characteristics",
        "Linear Logistic Regression Classification Model",
        "offline training model phase",
        "location-based marketing resources",
        "convolutional neural network",
        "hyper parameter settings",
        "real-time recommendation stage",
        "LBCNN model structure",
        "Random Forest Model",
        "past behavior data",
        "matrix decomposition method",
        "network model parameters",
        "LFM training data",
        "user bias item",
        "item offset item",
        "regression analysis",
        "linear model",
        "classification models",
        "model training",
        "training section",
        "training features",
        "behavior information",
        "previous data",
        "language model",
        "same model",
        "real-time performance",
        "LFM results",
        "sion coefficient",
        "specification results",
        "next question",
        "two parameters",
        "intrinsic property",
        "positive samples",
        "negative sample",
        "first thing",
        "historical score",
        "implied tag",
        "Experimental analysis",
        "timing preferences",
        "same conditions",
        "Vector Machine",
        "Experimental tool",
        "sklearn kit",
        "L2 regular",
        "regularization term",
        "regularization coefficient",
        "user tag",
        "rating score",
        "User information",
        "kind",
        "thinking",
        "sparseness",
        "analyses",
        "solution",
        "premise",
        "mula",
        "calculation",
        "puk",
        "average",
        "qik",
        "public",
        "experience",
        "latter",
        "overfitting",
        "Description",
        "input",
        "problems",
        "output",
        "L1-norm",
        "part",
        "update",
        "background",
        "people",
        "products",
        "advance",
        "period",
        "advantages",
        "several",
        "LR",
        "Support",
        "SVM",
        "GBDT",
        "8 h",
        "λ",
        "∑",
        "lion users’ various behaviors",
        "mobile recommendation algorithm contest",
        "radial basis kernel function",
        "personalized recommendation model",
        "feature segmentation standard",
        "random feature ratio",
        "real business sce",
        "product category information",
        "data set",
        "behavior record",
        "behavior time",
        "c. RF",
        "learning rate",
        "maximum depth",
        "Alibaba Group",
        "offline type",
        "RBF",
        "gamma",
        "trees",
        "entropy",
        "experiment",
        "1 month",
        "2,876,947 items",
        "types",
        "clicks",
        "shopping",
        "carts",
        "purchases",
        "hour",
        "online",
        "subset"
      ],
      "merged_content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n Time series T-3 T-2 T-1 T User 1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? User3 Visit location: a, b Forecast Shopping: b, c Result ? \n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n - - Time series- - - - - - Multi-window Max-pooling Output layer Input feature convolutional layer layer \n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n The existing location- Users' Latent historical factor based location data model marketing resources Time series Features of User feature resources preferences model Training process Input Output CNN A new location-based Features of marketing resources new resources Language model CNN Results process User preferences Recommending \n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. ",
      "text": [
        "Time series T-3 T-2 T-1 T User 1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? User3 Visit location: a, b Forecast Shopping: b, c Result ?",
        "- - Time series- - - - - - Multi-window Max-pooling Output layer Input feature convolutional layer layer",
        "The existing location- Users' Latent historical factor based location data model marketing resources Time series Features of User feature resources preferences model Training process Input Output CNN A new location-based Features of marketing resources new resources Language model CNN Results process User preferences Recommending",
        "User behavior data Pageviews> 30,000 No- Retain data Yes Purchase ≥3 Yes Retain data 0 No Eliminate data",
        "Validation accuracy A-Testing accuary Training accuracy 98 96 94 92 90 88 86 84 Accuracy rate/% 82 1 2 3 4 5 6 7 Number of iterations",
        "Published online: 01 May 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Time series T-3 T-2 T-1 T User 1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? User3 Visit location: a, b Forecast Shopping: b, c Result ?\",\"lines\":[{\"boundingBox\":[{\"x\":896,\"y\":2},{\"x\":1150,\"y\":2},{\"x\":1150,\"y\":48},{\"x\":896,\"y\":47}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":476,\"y\":101},{\"x\":553,\"y\":102},{\"x\":551,\"y\":148},{\"x\":474,\"y\":145}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":902,\"y\":99},{\"x\":973,\"y\":102},{\"x\":973,\"y\":146},{\"x\":899,\"y\":145}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1324,\"y\":101},{\"x\":1401,\"y\":102},{\"x\":1400,\"y\":146},{\"x\":1323,\"y\":144}],\"text\":\"T-1\"},{\"boundingBox\":[{\"x\":1816,\"y\":101},{\"x\":1847,\"y\":104},{\"x\":1845,\"y\":148},{\"x\":1813,\"y\":145}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":40,\"y\":316},{\"x\":172,\"y\":316},{\"x\":172,\"y\":359},{\"x\":40,\"y\":358}],\"text\":\"User 1\"},{\"boundingBox\":[{\"x\":313,\"y\":281},{\"x\":714,\"y\":281},{\"x\":714,\"y\":332},{\"x\":313,\"y\":330}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1162,\"y\":280},{\"x\":1565,\"y\":280},{\"x\":1564,\"y\":333},{\"x\":1162,\"y\":331}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1729,\"y\":284},{\"x\":1924,\"y\":284},{\"x\":1924,\"y\":329},{\"x\":1729,\"y\":328}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":310,\"y\":345},{\"x\":630,\"y\":347},{\"x\":629,\"y\":403},{\"x\":310,\"y\":401}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1160,\"y\":344},{\"x\":1479,\"y\":347},{\"x\":1478,\"y\":403},{\"x\":1159,\"y\":400}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1739,\"y\":346},{\"x\":1917,\"y\":346},{\"x\":1917,\"y\":392},{\"x\":1739,\"y\":393}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":38,\"y\":517},{\"x\":172,\"y\":518},{\"x\":172,\"y\":562},{\"x\":38,\"y\":561}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":734,\"y\":484},{\"x\":1137,\"y\":484},{\"x\":1137,\"y\":534},{\"x\":734,\"y\":532}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1730,\"y\":487},{\"x\":1920,\"y\":487},{\"x\":1920,\"y\":530},{\"x\":1730,\"y\":530}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":738,\"y\":549},{\"x\":1054,\"y\":547},{\"x\":1055,\"y\":604},{\"x\":738,\"y\":606}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1734,\"y\":550},{\"x\":1916,\"y\":549},{\"x\":1916,\"y\":595},{\"x\":1735,\"y\":596}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":42,\"y\":721},{\"x\":172,\"y\":721},{\"x\":172,\"y\":764},{\"x\":42,\"y\":763}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1161,\"y\":685},{\"x\":1562,\"y\":685},{\"x\":1562,\"y\":738},{\"x\":1161,\"y\":737}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1730,\"y\":690},{\"x\":1922,\"y\":690},{\"x\":1922,\"y\":733},{\"x\":1730,\"y\":732}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":1160,\"y\":752},{\"x\":1481,\"y\":751},{\"x\":1482,\"y\":808},{\"x\":1160,\"y\":810}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1733,\"y\":751},{\"x\":1915,\"y\":751},{\"x\":1915,\"y\":798},{\"x\":1733,\"y\":798}],\"text\":\"Result ?\"}],\"words\":[{\"boundingBox\":[{\"x\":898,\"y\":2},{\"x\":1007,\"y\":4},{\"x\":1007,\"y\":48},{\"x\":898,\"y\":48}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1017,\"y\":4},{\"x\":1149,\"y\":3},{\"x\":1149,\"y\":49},{\"x\":1017,\"y\":48}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":477,\"y\":101},{\"x\":547,\"y\":102},{\"x\":546,\"y\":148},{\"x\":476,\"y\":146}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":902,\"y\":99},{\"x\":972,\"y\":101},{\"x\":971,\"y\":147},{\"x\":901,\"y\":145}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1325,\"y\":101},{\"x\":1398,\"y\":102},{\"x\":1397,\"y\":146},{\"x\":1324,\"y\":144}],\"text\":\"T-1\"},{\"boundingBox\":[{\"x\":1816,\"y\":101},{\"x\":1844,\"y\":104},{\"x\":1840,\"y\":147},{\"x\":1813,\"y\":145}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":41,\"y\":317},{\"x\":140,\"y\":317},{\"x\":141,\"y\":359},{\"x\":40,\"y\":359}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":148,\"y\":317},{\"x\":171,\"y\":317},{\"x\":173,\"y\":360},{\"x\":149,\"y\":360}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":315,\"y\":281},{\"x\":417,\"y\":283},{\"x\":416,\"y\":331},{\"x\":313,\"y\":328}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":427,\"y\":283},{\"x\":622,\"y\":283},{\"x\":623,\"y\":333},{\"x\":426,\"y\":331}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":632,\"y\":283},{\"x\":674,\"y\":282},{\"x\":675,\"y\":333},{\"x\":633,\"y\":333}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":683,\"y\":282},{\"x\":711,\"y\":281},{\"x\":713,\"y\":333},{\"x\":685,\"y\":333}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1162,\"y\":282},{\"x\":1265,\"y\":283},{\"x\":1265,\"y\":331},{\"x\":1162,\"y\":329}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1275,\"y\":283},{\"x\":1469,\"y\":283},{\"x\":1468,\"y\":333},{\"x\":1274,\"y\":331}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1479,\"y\":282},{\"x\":1521,\"y\":282},{\"x\":1520,\"y\":334},{\"x\":1478,\"y\":334}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1530,\"y\":282},{\"x\":1559,\"y\":281},{\"x\":1557,\"y\":334},{\"x\":1529,\"y\":334}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":284},{\"x\":1921,\"y\":284},{\"x\":1923,\"y\":329},{\"x\":1729,\"y\":329}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":314,\"y\":345},{\"x\":533,\"y\":348},{\"x\":531,\"y\":403},{\"x\":311,\"y\":399}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":544,\"y\":348},{\"x\":587,\"y\":348},{\"x\":585,\"y\":403},{\"x\":541,\"y\":403}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":598,\"y\":348},{\"x\":627,\"y\":348},{\"x\":626,\"y\":402},{\"x\":596,\"y\":402}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1162,\"y\":344},{\"x\":1382,\"y\":347},{\"x\":1382,\"y\":403},{\"x\":1160,\"y\":400}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1393,\"y\":347},{\"x\":1437,\"y\":347},{\"x\":1437,\"y\":403},{\"x\":1393,\"y\":403}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1448,\"y\":347},{\"x\":1479,\"y\":347},{\"x\":1479,\"y\":403},{\"x\":1449,\"y\":403}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1740,\"y\":347},{\"x\":1877,\"y\":346},{\"x\":1878,\"y\":393},{\"x\":1739,\"y\":394}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1886,\"y\":346},{\"x\":1913,\"y\":346},{\"x\":1914,\"y\":393},{\"x\":1887,\"y\":393}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":41,\"y\":518},{\"x\":167,\"y\":519},{\"x\":166,\"y\":563},{\"x\":41,\"y\":561}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":737,\"y\":485},{\"x\":839,\"y\":485},{\"x\":840,\"y\":533},{\"x\":738,\"y\":531}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":849,\"y\":485},{\"x\":1048,\"y\":484},{\"x\":1047,\"y\":535},{\"x\":849,\"y\":533}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1057,\"y\":484},{\"x\":1099,\"y\":484},{\"x\":1098,\"y\":535},{\"x\":1057,\"y\":535}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1108,\"y\":484},{\"x\":1136,\"y\":484},{\"x\":1136,\"y\":535},{\"x\":1108,\"y\":535}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1732,\"y\":488},{\"x\":1920,\"y\":487},{\"x\":1919,\"y\":531},{\"x\":1731,\"y\":529}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":740,\"y\":550},{\"x\":954,\"y\":548},{\"x\":953,\"y\":606},{\"x\":738,\"y\":606}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":966,\"y\":548},{\"x\":1011,\"y\":548},{\"x\":1010,\"y\":606},{\"x\":965,\"y\":606}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1022,\"y\":547},{\"x\":1054,\"y\":547},{\"x\":1053,\"y\":605},{\"x\":1022,\"y\":606}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1735,\"y\":550},{\"x\":1878,\"y\":549},{\"x\":1880,\"y\":596},{\"x\":1736,\"y\":597}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":549},{\"x\":1914,\"y\":549},{\"x\":1916,\"y\":596},{\"x\":1889,\"y\":596}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":42,\"y\":722},{\"x\":165,\"y\":722},{\"x\":167,\"y\":765},{\"x\":42,\"y\":764}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1161,\"y\":685},{\"x\":1263,\"y\":686},{\"x\":1263,\"y\":737},{\"x\":1161,\"y\":737}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1274,\"y\":686},{\"x\":1469,\"y\":686},{\"x\":1468,\"y\":739},{\"x\":1273,\"y\":737}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1480,\"y\":686},{\"x\":1521,\"y\":685},{\"x\":1520,\"y\":739},{\"x\":1479,\"y\":739}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1531,\"y\":685},{\"x\":1561,\"y\":685},{\"x\":1560,\"y\":739},{\"x\":1530,\"y\":739}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1732,\"y\":691},{\"x\":1922,\"y\":690},{\"x\":1923,\"y\":734},{\"x\":1731,\"y\":731}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":1161,\"y\":754},{\"x\":1380,\"y\":752},{\"x\":1379,\"y\":809},{\"x\":1161,\"y\":810}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1391,\"y\":752},{\"x\":1440,\"y\":752},{\"x\":1438,\"y\":809},{\"x\":1390,\"y\":809}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1451,\"y\":752},{\"x\":1482,\"y\":752},{\"x\":1480,\"y\":809},{\"x\":1449,\"y\":809}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1736,\"y\":752},{\"x\":1881,\"y\":752},{\"x\":1881,\"y\":799},{\"x\":1736,\"y\":799}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1890,\"y\":752},{\"x\":1915,\"y\":752},{\"x\":1915,\"y\":798},{\"x\":1890,\"y\":799}],\"text\":\"?\"}]}",
        "{\"language\":\"en\",\"text\":\"- - Time series- - - - - - Multi-window Max-pooling Output layer Input feature convolutional layer layer\",\"lines\":[{\"boundingBox\":[{\"x\":1164,\"y\":75},{\"x\":1184,\"y\":87},{\"x\":1179,\"y\":94},{\"x\":1159,\"y\":82}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":894,\"y\":163},{\"x\":905,\"y\":159},{\"x\":906,\"y\":162},{\"x\":895,\"y\":167}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":47,\"y\":260},{\"x\":50,\"y\":529},{\"x\":3,\"y\":529},{\"x\":2,\"y\":260}],\"text\":\"Time series-\"},{\"boundingBox\":[{\"x\":991,\"y\":275},{\"x\":1007,\"y\":274},{\"x\":1007,\"y\":280},{\"x\":992,\"y\":281}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1424,\"y\":277},{\"x\":1441,\"y\":287},{\"x\":1437,\"y\":293},{\"x\":1420,\"y\":283}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":924,\"y\":286},{\"x\":937,\"y\":283},{\"x\":939,\"y\":289},{\"x\":925,\"y\":293}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1452,\"y\":300},{\"x\":1463,\"y\":307},{\"x\":1461,\"y\":311},{\"x\":1450,\"y\":305}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1442,\"y\":408},{\"x\":1453,\"y\":401},{\"x\":1455,\"y\":404},{\"x\":1444,\"y\":411}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":997,\"y\":731},{\"x\":1999,\"y\":750},{\"x\":1998,\"y\":814},{\"x\":997,\"y\":784}],\"text\":\"Multi-window Max-pooling Output layer\"},{\"boundingBox\":[{\"x\":196,\"y\":778},{\"x\":501,\"y\":775},{\"x\":501,\"y\":832},{\"x\":196,\"y\":834}],\"text\":\"Input feature\"},{\"boundingBox\":[{\"x\":1006,\"y\":803},{\"x\":1324,\"y\":800},{\"x\":1325,\"y\":851},{\"x\":1006,\"y\":855}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1447,\"y\":803},{\"x\":1573,\"y\":804},{\"x\":1572,\"y\":858},{\"x\":1443,\"y\":856}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":1105,\"y\":872},{\"x\":1228,\"y\":873},{\"x\":1227,\"y\":921},{\"x\":1105,\"y\":919}],\"text\":\"layer\"}],\"words\":[{\"boundingBox\":[{\"x\":1167,\"y\":75},{\"x\":1171,\"y\":78},{\"x\":1167,\"y\":86},{\"x\":1162,\"y\":83}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":895,\"y\":164},{\"x\":897,\"y\":163},{\"x\":898,\"y\":166},{\"x\":896,\"y\":167}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":48,\"y\":264},{\"x\":49,\"y\":381},{\"x\":2,\"y\":380},{\"x\":2,\"y\":261}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":49,\"y\":391},{\"x\":50,\"y\":529},{\"x\":3,\"y\":529},{\"x\":2,\"y\":390}],\"text\":\"series-\"},{\"boundingBox\":[{\"x\":993,\"y\":275},{\"x\":997,\"y\":275},{\"x\":997,\"y\":281},{\"x\":994,\"y\":281}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1427,\"y\":279},{\"x\":1430,\"y\":281},{\"x\":1427,\"y\":286},{\"x\":1423,\"y\":284}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":924,\"y\":286},{\"x\":928,\"y\":285},{\"x\":929,\"y\":292},{\"x\":925,\"y\":293}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1455,\"y\":302},{\"x\":1459,\"y\":304},{\"x\":1456,\"y\":308},{\"x\":1453,\"y\":306}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1443,\"y\":408},{\"x\":1445,\"y\":407},{\"x\":1446,\"y\":410},{\"x\":1444,\"y\":411}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":997,\"y\":732},{\"x\":1314,\"y\":732},{\"x\":1315,\"y\":788},{\"x\":999,\"y\":780}],\"text\":\"Multi-window\"},{\"boundingBox\":[{\"x\":1349,\"y\":733},{\"x\":1655,\"y\":740},{\"x\":1656,\"y\":800},{\"x\":1350,\"y\":789}],\"text\":\"Max-pooling\"},{\"boundingBox\":[{\"x\":1697,\"y\":742},{\"x\":1864,\"y\":749},{\"x\":1864,\"y\":809},{\"x\":1697,\"y\":802}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":1875,\"y\":749},{\"x\":1998,\"y\":756},{\"x\":1998,\"y\":815},{\"x\":1875,\"y\":809}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":198,\"y\":780},{\"x\":317,\"y\":779},{\"x\":316,\"y\":833},{\"x\":196,\"y\":834}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":328,\"y\":779},{\"x\":497,\"y\":776},{\"x\":498,\"y\":834},{\"x\":327,\"y\":833}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1008,\"y\":804},{\"x\":1324,\"y\":801},{\"x\":1324,\"y\":851},{\"x\":1007,\"y\":856}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1443,\"y\":803},{\"x\":1571,\"y\":804},{\"x\":1570,\"y\":858},{\"x\":1443,\"y\":856}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":1105,\"y\":872},{\"x\":1225,\"y\":873},{\"x\":1224,\"y\":921},{\"x\":1105,\"y\":919}],\"text\":\"layer\"}]}",
        "{\"language\":\"en\",\"text\":\"The existing location- Users' Latent historical factor based location data model marketing resources Time series Features of User feature resources preferences model Training process Input Output CNN A new location-based Features of marketing resources new resources Language model CNN Results process User preferences Recommending\",\"lines\":[{\"boundingBox\":[{\"x\":316,\"y\":40},{\"x\":622,\"y\":46},{\"x\":621,\"y\":102},{\"x\":316,\"y\":95}],\"text\":\"The existing\"},{\"boundingBox\":[{\"x\":354,\"y\":114},{\"x\":577,\"y\":115},{\"x\":577,\"y\":169},{\"x\":354,\"y\":168}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1159,\"y\":79},{\"x\":1315,\"y\":73},{\"x\":1316,\"y\":121},{\"x\":1160,\"y\":129}],\"text\":\"Users'\"},{\"boundingBox\":[{\"x\":1616,\"y\":79},{\"x\":1779,\"y\":81},{\"x\":1778,\"y\":128},{\"x\":1616,\"y\":125}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":1142,\"y\":138},{\"x\":1378,\"y\":138},{\"x\":1378,\"y\":193},{\"x\":1142,\"y\":192}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1629,\"y\":139},{\"x\":1772,\"y\":141},{\"x\":1771,\"y\":191},{\"x\":1628,\"y\":187}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":390,\"y\":187},{\"x\":538,\"y\":187},{\"x\":539,\"y\":239},{\"x\":390,\"y\":238}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1097,\"y\":201},{\"x\":1417,\"y\":203},{\"x\":1417,\"y\":255},{\"x\":1097,\"y\":252}],\"text\":\"location data\"},{\"boundingBox\":[{\"x\":1621,\"y\":208},{\"x\":1770,\"y\":204},{\"x\":1771,\"y\":248},{\"x\":1621,\"y\":249}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":337,\"y\":262},{\"x\":592,\"y\":262},{\"x\":592,\"y\":318},{\"x\":337,\"y\":318}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":344,\"y\":341},{\"x\":584,\"y\":341},{\"x\":585,\"y\":383},{\"x\":344,\"y\":385}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":575,\"y\":389},{\"x\":854,\"y\":392},{\"x\":854,\"y\":440},{\"x\":575,\"y\":437}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":1227,\"y\":389},{\"x\":1511,\"y\":389},{\"x\":1511,\"y\":440},{\"x\":1227,\"y\":440}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":1675,\"y\":392},{\"x\":1795,\"y\":390},{\"x\":1796,\"y\":437},{\"x\":1673,\"y\":438}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":632,\"y\":447},{\"x\":798,\"y\":452},{\"x\":797,\"y\":499},{\"x\":631,\"y\":494}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1246,\"y\":469},{\"x\":1482,\"y\":467},{\"x\":1483,\"y\":509},{\"x\":1246,\"y\":511}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":1582,\"y\":463},{\"x\":1871,\"y\":460},{\"x\":1872,\"y\":514},{\"x\":1583,\"y\":521}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":634,\"y\":516},{\"x\":792,\"y\":506},{\"x\":793,\"y\":557},{\"x\":635,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":50,\"y\":717},{\"x\":55,\"y\":300},{\"x\":117,\"y\":300},{\"x\":112,\"y\":718}],\"text\":\"Training process\"},{\"boundingBox\":[{\"x\":526,\"y\":878},{\"x\":665,\"y\":878},{\"x\":665,\"y\":937},{\"x\":526,\"y\":936}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1396,\"y\":879},{\"x\":1570,\"y\":879},{\"x\":1570,\"y\":937},{\"x\":1396,\"y\":935}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":952,\"y\":1118},{\"x\":1076,\"y\":1116},{\"x\":1076,\"y\":1161},{\"x\":953,\"y\":1160}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":196,\"y\":1275},{\"x\":739,\"y\":1276},{\"x\":739,\"y\":1333},{\"x\":196,\"y\":1332}],\"text\":\"A new location-based\"},{\"boundingBox\":[{\"x\":1348,\"y\":1268},{\"x\":1635,\"y\":1269},{\"x\":1635,\"y\":1320},{\"x\":1348,\"y\":1319}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":224,\"y\":1350},{\"x\":715,\"y\":1350},{\"x\":715,\"y\":1406},{\"x\":224,\"y\":1405}],\"text\":\"marketing resources\"},{\"boundingBox\":[{\"x\":1438,\"y\":1345},{\"x\":1541,\"y\":1345},{\"x\":1540,\"y\":1381},{\"x\":1438,\"y\":1380}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1369,\"y\":1406},{\"x\":1611,\"y\":1407},{\"x\":1610,\"y\":1451},{\"x\":1369,\"y\":1450}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":653,\"y\":1455},{\"x\":897,\"y\":1460},{\"x\":896,\"y\":1521},{\"x\":651,\"y\":1517}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":692,\"y\":1531},{\"x\":847,\"y\":1528},{\"x\":848,\"y\":1582},{\"x\":693,\"y\":1583}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1037,\"y\":1497},{\"x\":1165,\"y\":1496},{\"x\":1165,\"y\":1546},{\"x\":1035,\"y\":1545}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1758,\"y\":1491},{\"x\":1949,\"y\":1491},{\"x\":1950,\"y\":1547},{\"x\":1758,\"y\":1549}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":83,\"y\":1551},{\"x\":83,\"y\":1350},{\"x\":131,\"y\":1350},{\"x\":132,\"y\":1550}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1551,\"y\":1626},{\"x\":1551,\"y\":1674},{\"x\":1431,\"y\":1669}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1348,\"y\":1690},{\"x\":1630,\"y\":1687},{\"x\":1631,\"y\":1741},{\"x\":1348,\"y\":1747}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":5,\"y\":1740},{\"x\":7,\"y\":1350},{\"x\":58,\"y\":1350},{\"x\":53,\"y\":1741}],\"text\":\"Recommending\"}],\"words\":[{\"boundingBox\":[{\"x\":320,\"y\":40},{\"x\":405,\"y\":41},{\"x\":405,\"y\":99},{\"x\":320,\"y\":97}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":417,\"y\":42},{\"x\":613,\"y\":47},{\"x\":613,\"y\":103},{\"x\":417,\"y\":99}],\"text\":\"existing\"},{\"boundingBox\":[{\"x\":356,\"y\":114},{\"x\":574,\"y\":118},{\"x\":576,\"y\":168},{\"x\":354,\"y\":169}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1160,\"y\":79},{\"x\":1316,\"y\":73},{\"x\":1315,\"y\":115},{\"x\":1160,\"y\":129}],\"text\":\"Users'\"},{\"boundingBox\":[{\"x\":1616,\"y\":79},{\"x\":1780,\"y\":82},{\"x\":1779,\"y\":128},{\"x\":1616,\"y\":126}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":1145,\"y\":138},{\"x\":1375,\"y\":142},{\"x\":1376,\"y\":192},{\"x\":1143,\"y\":193}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1629,\"y\":139},{\"x\":1769,\"y\":141},{\"x\":1768,\"y\":191},{\"x\":1628,\"y\":188}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":393,\"y\":187},{\"x\":530,\"y\":187},{\"x\":530,\"y\":239},{\"x\":392,\"y\":239}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1097,\"y\":201},{\"x\":1293,\"y\":203},{\"x\":1294,\"y\":255},{\"x\":1098,\"y\":253}],\"text\":\"location\"},{\"boundingBox\":[{\"x\":1308,\"y\":203},{\"x\":1416,\"y\":203},{\"x\":1416,\"y\":255},{\"x\":1308,\"y\":255}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":1623,\"y\":209},{\"x\":1768,\"y\":205},{\"x\":1767,\"y\":249},{\"x\":1621,\"y\":249}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":340,\"y\":266},{\"x\":586,\"y\":266},{\"x\":587,\"y\":319},{\"x\":337,\"y\":319}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":348,\"y\":342},{\"x\":578,\"y\":344},{\"x\":578,\"y\":380},{\"x\":346,\"y\":386}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":576,\"y\":390},{\"x\":698,\"y\":391},{\"x\":698,\"y\":439},{\"x\":578,\"y\":438}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":708,\"y\":391},{\"x\":853,\"y\":392},{\"x\":850,\"y\":441},{\"x\":708,\"y\":439}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":1228,\"y\":389},{\"x\":1436,\"y\":390},{\"x\":1436,\"y\":441},{\"x\":1228,\"y\":441}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1447,\"y\":390},{\"x\":1510,\"y\":389},{\"x\":1510,\"y\":441},{\"x\":1446,\"y\":441}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1673,\"y\":391},{\"x\":1790,\"y\":390},{\"x\":1790,\"y\":437},{\"x\":1673,\"y\":438}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":633,\"y\":448},{\"x\":796,\"y\":454},{\"x\":795,\"y\":499},{\"x\":631,\"y\":495}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1247,\"y\":469},{\"x\":1477,\"y\":468},{\"x\":1478,\"y\":510},{\"x\":1247,\"y\":512}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":1586,\"y\":467},{\"x\":1870,\"y\":461},{\"x\":1871,\"y\":515},{\"x\":1586,\"y\":522}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":636,\"y\":518},{\"x\":792,\"y\":507},{\"x\":789,\"y\":559},{\"x\":635,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":50,\"y\":713},{\"x\":58,\"y\":512},{\"x\":116,\"y\":511},{\"x\":109,\"y\":713}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":58,\"y\":500},{\"x\":55,\"y\":307},{\"x\":115,\"y\":304},{\"x\":116,\"y\":498}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":526,\"y\":878},{\"x\":664,\"y\":878},{\"x\":663,\"y\":937},{\"x\":526,\"y\":936}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1398,\"y\":879},{\"x\":1568,\"y\":879},{\"x\":1568,\"y\":938},{\"x\":1396,\"y\":934}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":952,\"y\":1116},{\"x\":1058,\"y\":1116},{\"x\":1058,\"y\":1161},{\"x\":952,\"y\":1161}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":208,\"y\":1276},{\"x\":239,\"y\":1276},{\"x\":237,\"y\":1333},{\"x\":206,\"y\":1333}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":254,\"y\":1277},{\"x\":349,\"y\":1277},{\"x\":348,\"y\":1331},{\"x\":253,\"y\":1333}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":374,\"y\":1278},{\"x\":728,\"y\":1278},{\"x\":730,\"y\":1332},{\"x\":374,\"y\":1331}],\"text\":\"location-based\"},{\"boundingBox\":[{\"x\":1349,\"y\":1268},{\"x\":1558,\"y\":1269},{\"x\":1559,\"y\":1319},{\"x\":1349,\"y\":1320}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1568,\"y\":1269},{\"x\":1630,\"y\":1270},{\"x\":1631,\"y\":1321},{\"x\":1569,\"y\":1320}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":226,\"y\":1354},{\"x\":464,\"y\":1351},{\"x\":462,\"y\":1406},{\"x\":224,\"y\":1406}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":479,\"y\":1351},{\"x\":713,\"y\":1360},{\"x\":711,\"y\":1402},{\"x\":477,\"y\":1406}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":1438,\"y\":1345},{\"x\":1516,\"y\":1345},{\"x\":1516,\"y\":1381},{\"x\":1438,\"y\":1380}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1370,\"y\":1407},{\"x\":1602,\"y\":1410},{\"x\":1601,\"y\":1450},{\"x\":1371,\"y\":1451}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":654,\"y\":1456},{\"x\":896,\"y\":1461},{\"x\":897,\"y\":1521},{\"x\":652,\"y\":1517}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":695,\"y\":1536},{\"x\":847,\"y\":1528},{\"x\":848,\"y\":1583},{\"x\":693,\"y\":1582}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1035,\"y\":1496},{\"x\":1143,\"y\":1496},{\"x\":1143,\"y\":1546},{\"x\":1035,\"y\":1546}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1764,\"y\":1492},{\"x\":1950,\"y\":1492},{\"x\":1950,\"y\":1547},{\"x\":1762,\"y\":1550}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":83,\"y\":1546},{\"x\":83,\"y\":1356},{\"x\":128,\"y\":1353},{\"x\":132,\"y\":1541}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1551,\"y\":1626},{\"x\":1549,\"y\":1674},{\"x\":1431,\"y\":1669}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1350,\"y\":1693},{\"x\":1630,\"y\":1689},{\"x\":1630,\"y\":1742},{\"x\":1348,\"y\":1748}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":5,\"y\":1738},{\"x\":7,\"y\":1359},{\"x\":58,\"y\":1359},{\"x\":52,\"y\":1737}],\"text\":\"Recommending\"}]}",
        "{\"language\":\"en\",\"text\":\"User behavior data Pageviews> 30,000 No- Retain data Yes Purchase ≥3 Yes Retain data 0 No Eliminate data\",\"lines\":[{\"boundingBox\":[{\"x\":77,\"y\":41},{\"x\":727,\"y\":38},{\"x\":727,\"y\":114},{\"x\":77,\"y\":116}],\"text\":\"User behavior data\"},{\"boundingBox\":[{\"x\":172,\"y\":322},{\"x\":617,\"y\":318},{\"x\":618,\"y\":402},{\"x\":172,\"y\":406}],\"text\":\"Pageviews>\"},{\"boundingBox\":[{\"x\":279,\"y\":425},{\"x\":516,\"y\":427},{\"x\":513,\"y\":507},{\"x\":276,\"y\":505}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":867,\"y\":373},{\"x\":986,\"y\":375},{\"x\":987,\"y\":444},{\"x\":865,\"y\":445}],\"text\":\"No-\"},{\"boundingBox\":[{\"x\":1151,\"y\":374},{\"x\":1549,\"y\":374},{\"x\":1549,\"y\":448},{\"x\":1151,\"y\":449}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":336,\"y\":632},{\"x\":469,\"y\":637},{\"x\":469,\"y\":700},{\"x\":335,\"y\":694}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":180,\"y\":884},{\"x\":613,\"y\":880},{\"x\":614,\"y\":955},{\"x\":180,\"y\":959}],\"text\":\"Purchase ≥3\"},{\"boundingBox\":[{\"x\":851,\"y\":908},{\"x\":978,\"y\":908},{\"x\":977,\"y\":977},{\"x\":850,\"y\":979}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":1153,\"y\":909},{\"x\":1545,\"y\":908},{\"x\":1545,\"y\":978},{\"x\":1153,\"y\":979}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":377,\"y\":996},{\"x\":426,\"y\":996},{\"x\":425,\"y\":1056},{\"x\":380,\"y\":1053}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":343,\"y\":1147},{\"x\":450,\"y\":1147},{\"x\":450,\"y\":1210},{\"x\":341,\"y\":1207}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":151,\"y\":1305},{\"x\":652,\"y\":1306},{\"x\":652,\"y\":1375},{\"x\":151,\"y\":1374}],\"text\":\"Eliminate data\"}],\"words\":[{\"boundingBox\":[{\"x\":78,\"y\":43},{\"x\":237,\"y\":43},{\"x\":237,\"y\":116},{\"x\":77,\"y\":115}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":252,\"y\":42},{\"x\":558,\"y\":41},{\"x\":559,\"y\":116},{\"x\":252,\"y\":116}],\"text\":\"behavior\"},{\"boundingBox\":[{\"x\":572,\"y\":40},{\"x\":722,\"y\":39},{\"x\":724,\"y\":114},{\"x\":574,\"y\":116}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":174,\"y\":322},{\"x\":595,\"y\":319},{\"x\":598,\"y\":403},{\"x\":173,\"y\":406}],\"text\":\"Pageviews>\"},{\"boundingBox\":[{\"x\":280,\"y\":425},{\"x\":516,\"y\":427},{\"x\":515,\"y\":507},{\"x\":279,\"y\":505}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":869,\"y\":373},{\"x\":985,\"y\":373},{\"x\":985,\"y\":445},{\"x\":868,\"y\":444}],\"text\":\"No-\"},{\"boundingBox\":[{\"x\":1153,\"y\":374},{\"x\":1372,\"y\":375},{\"x\":1372,\"y\":447},{\"x\":1151,\"y\":450}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1393,\"y\":375},{\"x\":1545,\"y\":375},{\"x\":1547,\"y\":448},{\"x\":1393,\"y\":447}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":339,\"y\":632},{\"x\":468,\"y\":637},{\"x\":466,\"y\":700},{\"x\":337,\"y\":695}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":180,\"y\":885},{\"x\":486,\"y\":884},{\"x\":486,\"y\":958},{\"x\":180,\"y\":959}],\"text\":\"Purchase\"},{\"boundingBox\":[{\"x\":500,\"y\":884},{\"x\":613,\"y\":880},{\"x\":613,\"y\":956},{\"x\":500,\"y\":957}],\"text\":\"≥3\"},{\"boundingBox\":[{\"x\":850,\"y\":908},{\"x\":977,\"y\":908},{\"x\":978,\"y\":978},{\"x\":850,\"y\":979}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":1154,\"y\":910},{\"x\":1369,\"y\":912},{\"x\":1369,\"y\":977},{\"x\":1153,\"y\":979}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1393,\"y\":911},{\"x\":1543,\"y\":909},{\"x\":1543,\"y\":979},{\"x\":1392,\"y\":977}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":377,\"y\":996},{\"x\":411,\"y\":996},{\"x\":409,\"y\":1055},{\"x\":377,\"y\":1054}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":344,\"y\":1147},{\"x\":440,\"y\":1147},{\"x\":439,\"y\":1210},{\"x\":343,\"y\":1209}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":152,\"y\":1306},{\"x\":483,\"y\":1306},{\"x\":481,\"y\":1376},{\"x\":151,\"y\":1375}],\"text\":\"Eliminate\"},{\"boundingBox\":[{\"x\":498,\"y\":1306},{\"x\":647,\"y\":1307},{\"x\":645,\"y\":1376},{\"x\":496,\"y\":1376}],\"text\":\"data\"}]}",
        "{\"language\":\"en\",\"text\":\"Validation accuracy A-Testing accuary Training accuracy 98 96 94 92 90 88 86 84 Accuracy rate/% 82 1 2 3 4 5 6 7 Number of iterations\",\"lines\":[{\"boundingBox\":[{\"x\":136,\"y\":2},{\"x\":488,\"y\":5},{\"x\":487,\"y\":36},{\"x\":136,\"y\":32}],\"text\":\"Validation accuracy\"},{\"boundingBox\":[{\"x\":488,\"y\":3},{\"x\":823,\"y\":4},{\"x\":823,\"y\":36},{\"x\":488,\"y\":34}],\"text\":\"A-Testing accuary\"},{\"boundingBox\":[{\"x\":186,\"y\":55},{\"x\":457,\"y\":58},{\"x\":457,\"y\":94},{\"x\":186,\"y\":89}],\"text\":\"Training accuracy\"},{\"boundingBox\":[{\"x\":47,\"y\":121},{\"x\":81,\"y\":121},{\"x\":83,\"y\":152},{\"x\":48,\"y\":153}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":48,\"y\":159},{\"x\":82,\"y\":161},{\"x\":81,\"y\":193},{\"x\":46,\"y\":191}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":49,\"y\":204},{\"x\":79,\"y\":203},{\"x\":78,\"y\":231},{\"x\":47,\"y\":230}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":47,\"y\":242},{\"x\":77,\"y\":242},{\"x\":77,\"y\":270},{\"x\":46,\"y\":270}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":52,\"y\":285},{\"x\":78,\"y\":284},{\"x\":77,\"y\":311},{\"x\":50,\"y\":310}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":51,\"y\":326},{\"x\":79,\"y\":325},{\"x\":77,\"y\":353},{\"x\":50,\"y\":353}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":48,\"y\":367},{\"x\":79,\"y\":366},{\"x\":78,\"y\":393},{\"x\":48,\"y\":393}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":48,\"y\":407},{\"x\":81,\"y\":407},{\"x\":79,\"y\":434},{\"x\":47,\"y\":434}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":2,\"y\":425},{\"x\":3,\"y\":173},{\"x\":34,\"y\":173},{\"x\":33,\"y\":425}],\"text\":\"Accuracy rate/%\"},{\"boundingBox\":[{\"x\":49,\"y\":450},{\"x\":79,\"y\":449},{\"x\":80,\"y\":474},{\"x\":49,\"y\":476}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":97,\"y\":493},{\"x\":111,\"y\":493},{\"x\":113,\"y\":512},{\"x\":98,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":225,\"y\":487},{\"x\":249,\"y\":488},{\"x\":249,\"y\":518},{\"x\":224,\"y\":518}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":491},{\"x\":377,\"y\":491},{\"x\":377,\"y\":515},{\"x\":360,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":498,\"y\":491},{\"x\":514,\"y\":492},{\"x\":513,\"y\":516},{\"x\":497,\"y\":515}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":630,\"y\":492},{\"x\":645,\"y\":493},{\"x\":645,\"y\":515},{\"x\":630,\"y\":513}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":762,\"y\":492},{\"x\":778,\"y\":493},{\"x\":779,\"y\":516},{\"x\":763,\"y\":516}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":893,\"y\":491},{\"x\":909,\"y\":493},{\"x\":908,\"y\":515},{\"x\":892,\"y\":513}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":348,\"y\":537},{\"x\":663,\"y\":537},{\"x\":663,\"y\":568},{\"x\":348,\"y\":567}],\"text\":\"Number of iterations\"}],\"words\":[{\"boundingBox\":[{\"x\":188,\"y\":3},{\"x\":340,\"y\":4},{\"x\":339,\"y\":34},{\"x\":188,\"y\":32}],\"text\":\"Validation\"},{\"boundingBox\":[{\"x\":352,\"y\":4},{\"x\":484,\"y\":6},{\"x\":483,\"y\":37},{\"x\":352,\"y\":34}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":529,\"y\":4},{\"x\":696,\"y\":4},{\"x\":696,\"y\":36},{\"x\":529,\"y\":35}],\"text\":\"A-Testing\"},{\"boundingBox\":[{\"x\":705,\"y\":4},{\"x\":823,\"y\":7},{\"x\":822,\"y\":37},{\"x\":705,\"y\":37}],\"text\":\"accuary\"},{\"boundingBox\":[{\"x\":189,\"y\":55},{\"x\":314,\"y\":56},{\"x\":313,\"y\":93},{\"x\":188,\"y\":90}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":321,\"y\":56},{\"x\":455,\"y\":61},{\"x\":455,\"y\":94},{\"x\":321,\"y\":93}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":47,\"y\":121},{\"x\":77,\"y\":121},{\"x\":78,\"y\":153},{\"x\":47,\"y\":153}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":47,\"y\":159},{\"x\":80,\"y\":161},{\"x\":78,\"y\":193},{\"x\":46,\"y\":191}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":47,\"y\":203},{\"x\":79,\"y\":203},{\"x\":79,\"y\":231},{\"x\":47,\"y\":231}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":46,\"y\":242},{\"x\":76,\"y\":242},{\"x\":76,\"y\":270},{\"x\":46,\"y\":270}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":50,\"y\":284},{\"x\":76,\"y\":284},{\"x\":76,\"y\":311},{\"x\":50,\"y\":311}],\"text\":\"90\"},{\"boundingBox\":[{\"x\":50,\"y\":325},{\"x\":77,\"y\":325},{\"x\":77,\"y\":353},{\"x\":50,\"y\":353}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":48,\"y\":366},{\"x\":77,\"y\":366},{\"x\":78,\"y\":393},{\"x\":48,\"y\":393}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":47,\"y\":407},{\"x\":78,\"y\":407},{\"x\":78,\"y\":434},{\"x\":47,\"y\":434}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":2,\"y\":424},{\"x\":4,\"y\":290},{\"x\":34,\"y\":290},{\"x\":33,\"y\":425}],\"text\":\"Accuracy\"},{\"boundingBox\":[{\"x\":4,\"y\":279},{\"x\":3,\"y\":175},{\"x\":33,\"y\":174},{\"x\":34,\"y\":279}],\"text\":\"rate/%\"},{\"boundingBox\":[{\"x\":49,\"y\":450},{\"x\":77,\"y\":449},{\"x\":78,\"y\":474},{\"x\":49,\"y\":475}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":99,\"y\":493},{\"x\":110,\"y\":493},{\"x\":111,\"y\":513},{\"x\":99,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":228,\"y\":487},{\"x\":246,\"y\":487},{\"x\":245,\"y\":518},{\"x\":227,\"y\":518}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":491},{\"x\":375,\"y\":491},{\"x\":374,\"y\":515},{\"x\":360,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":498,\"y\":491},{\"x\":513,\"y\":492},{\"x\":512,\"y\":516},{\"x\":497,\"y\":515}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":630,\"y\":492},{\"x\":644,\"y\":493},{\"x\":642,\"y\":515},{\"x\":630,\"y\":513}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":762,\"y\":492},{\"x\":777,\"y\":492},{\"x\":776,\"y\":516},{\"x\":762,\"y\":515}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":896,\"y\":491},{\"x\":908,\"y\":493},{\"x\":905,\"y\":514},{\"x\":894,\"y\":513}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":349,\"y\":540},{\"x\":473,\"y\":537},{\"x\":472,\"y\":568},{\"x\":348,\"y\":567}],\"text\":\"Number\"},{\"boundingBox\":[{\"x\":479,\"y\":537},{\"x\":514,\"y\":537},{\"x\":513,\"y\":568},{\"x\":478,\"y\":568}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":520,\"y\":537},{\"x\":663,\"y\":539},{\"x\":662,\"y\":569},{\"x\":519,\"y\":568}],\"text\":\"iterations\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 01 May 2019\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":14},{\"x\":890,\"y\":15},{\"x\":890,\"y\":74},{\"x\":4,\"y\":70}],\"text\":\"Published online: 01 May 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":270,\"y\":15},{\"x\":270,\"y\":72},{\"x\":4,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":289,\"y\":15},{\"x\":493,\"y\":15},{\"x\":493,\"y\":73},{\"x\":289,\"y\":72}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":504,\"y\":15},{\"x\":578,\"y\":15},{\"x\":578,\"y\":74},{\"x\":504,\"y\":73}],\"text\":\"01\"},{\"boundingBox\":[{\"x\":594,\"y\":15},{\"x\":725,\"y\":15},{\"x\":725,\"y\":74},{\"x\":594,\"y\":74}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":745,\"y\":15},{\"x\":883,\"y\":16},{\"x\":883,\"y\":74},{\"x\":745,\"y\":74}],\"text\":\"2019\"}]}"
      ]
    },
    {
      "@search.score": 1.9367393,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNTM3LTAyMC0wMDI5Mi15LnBkZg2",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter",
      "people": [
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "ket",
        "dancy",
        "Narayanan",
        "Makridakis",
        "Hao",
        "Salakhutdinov",
        "Wietsma",
        "Jianguo Chen",
        "Asha",
        "Gini",
        "Luo",
        "Liu",
        "log2Ef",
        "�G"
      ],
      "keyphrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "big data processing technologies",
        "A Feature Information Gain",
        "other online shopping sites",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "RESEARCH Narayanan",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "prediction accuracy",
        "Introduction",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "iveco",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "org",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "product pre-launch prediction",
        "Consumer product success",
        "user rating matrix",
        "future work” section",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "prediction classifiers",
        "Related work",
        "marketing strategies",
        "embedded process",
        "Page",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "15Narayanan",
        "MF",
        "Hao",
        "item",
        "low",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "related matrix factorization",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "customer review Ri",
        "product rating scale",
        "Regression Testing Dataset",
        "categorical, real",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "best information",
        "prediction classifier",
        "multivariate data",
        "data pre",
        "pre-launch prediction",
        "input dataset",
        "Product feature",
        "Product categories",
        "feature selection",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "processing Feature",
        "user requirements",
        "particular review",
        "pre‑processing",
        "feature impurity",
        "market industry",
        "Battery life",
        "user features",
        "Review Content",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "�G value",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "path",
        "map",
        "groupBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "t training feature vectors",
        "Support Vector Machine classifiers",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "+ b",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "mth review",
        "ith review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2 G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "Spark python API",
        "several case studies",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "system response time",
        "redundant customer reviews",
        "system design factors",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "separate servers",
        "proposed system",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "other gini-index",
        "Gini-index model",
        "Product price",
        "scalability requirements",
        "other state",
        "application development",
        "16 GB",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "LSA-based meth- ods",
        "Classifier Logistic regression",
        "LSA-based DMRDF Gini-index",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "significant features",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "Gini-index approaches",
        "Dataset Size",
        "large dataset",
        "LSA-based approaches",
        "many features",
        "The DMRDF",
        "Performance comparison",
        "future work",
        "PA measures",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "18",
        "7",
        "other product feature identification",
        "different reliable online websites",
        "prediction model- ling",
        "data processing domains",
        "information fusion approach",
        "statistical prop- erties",
        "memory computation method",
        "time streaming predictions",
        "dataset model performance",
        "important role",
        "DMRDF model",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "unified API",
        "customer comments",
        "learning algorithms",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "machine"
      ],
      "merged_content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n  \n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F  ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T  of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope w",
      "text": [
        "",
        "Published online: 28 February 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2020\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":16},{\"x\":1018,\"y\":18},{\"x\":1018,\"y\":73},{\"x\":4,\"y\":70}],\"text\":\"Published online: 28 February 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":269,\"y\":17},{\"x\":270,\"y\":70},{\"x\":5,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":498,\"y\":17},{\"x\":498,\"y\":71},{\"x\":292,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":508,\"y\":17},{\"x\":575,\"y\":17},{\"x\":575,\"y\":71},{\"x\":508,\"y\":71}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":593,\"y\":17},{\"x\":852,\"y\":18},{\"x\":852,\"y\":73},{\"x\":594,\"y\":71}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":871,\"y\":18},{\"x\":1008,\"y\":18},{\"x\":1008,\"y\":74},{\"x\":871,\"y\":73}],\"text\":\"2020\"}]}"
      ]
    },
    {
      "@search.score": 1.6248871,
      "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\n\n\n\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNzA4LTAxNS0wMDE3LXoucGRm0",
      "metadata_author": "Zengda Guan",
      "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction",
      "people": [
        "Zengda Guan",
        "Ang Li",
        "Tingshao Zhu",
        "Z. Guan",
        "A. Li",
        "T. Zhu",
        "YÞ",
        "PteðxtrÞ",
        "PtrðxtrÞ",
        "Sugiyama",
        "Kanamori",
        "Huang",
        "Hilbert",
        "Dai",
        "Pardoe",
        "Loog",
        "Gaussian",
        "WeigðxtrÞ",
        "jj22Þ",
        "Enas",
        "Choi",
        "WeigðxðiÞtr",
        "Weig",
        "Sina Weibo",
        "mogorov",
        "Smirnov",
        "Kolmogorov",
        "Clust",
        "KMM"
      ],
      "keyphrases": [
        "China A. Li Black Dog Institute",
        "weighted risk regression model",
        "Z. Guan Business School",
        "Local regression transfer learning",
        "Adaptive parameter-setting method",
        "New South Wales",
        "Local transfer learning",
        "clustering reweighting methods",
        "Shandong Jianzhu University",
        "T. Zhu Institute",
        "individual psychological characteristics",
        "online user experience",
        "transfer learning methods",
        "personal psychological characteristics",
        "independent test dataset",
        "labelled training dataset",
        "psychological characteristics prediction",
        "Beijing Forestry University",
        "Zengda Guan",
        "Ang Li",
        "personal preference",
        "online services",
        "Tingshao Zhu",
        "web user",
        "The Author",
        "open access",
        "Recent studies",
        "training instance",
        "different genders",
        "ferent districts",
        "� Covariate shift",
        "recent decades",
        "important role",
        "human lives",
        "consumer preference",
        "consistent traits",
        "changeable status",
        "key factors",
        "one type",
        "human behaviour",
        "web behaviour",
        "Many studies",
        "Most studies",
        "many cases",
        "Chinese Academy",
        "Computing Technology",
        "Brain Informatics",
        "Previous studies",
        "little attention",
        "innovative approach",
        "learning models",
        "tional models",
        "generalization capability",
        "demographic variation",
        "identical distribution",
        "users’ personality",
        "behaviours",
        "applications",
        "article",
        "Springerlink",
        "Abstract",
        "differences",
        "problem",
        "bour",
        "importance",
        "situation",
        "labels",
        "experiments",
        "depression",
        "results",
        "Keywords",
        "1 Introduction",
        "people",
        "time",
        "Internet",
        "help",
        "assumption",
        "Jinan",
        "mail",
        "guanzengda",
        "Department",
        "Psychology",
        "Sydney",
        "Australia",
        "Sciences",
        "DOI",
        "org",
        "low",
        "performance",
        "paper",
        "3",
        "14",
        "new local regression transfer learning methods",
        "Most existing covariate shift methods",
        "local transfer learning methods",
        "local learning views",
        "adaptive k-NN methods",
        "general solution framework",
        "local neighbour region",
        "risk regression model",
        "training-test k-NN method",
        "pre- diction functions",
        "covariate shift problems",
        "training data instances",
        "test dataset label",
        "empirical risk form",
        "test dataset population",
        "test dataset model",
        "psychological characteristics labels",
        "learning model",
        "regression form",
        "local region",
        "risk model",
        "prediction functions",
        "simple covariate",
        "data points",
        "parameter-setting method",
        "risk function",
        "expected risk",
        "prediction accuracy",
        "training dataset",
        "entire dataset",
        "input dataset",
        "resampling weight",
        "computation cost",
        "continual values",
        "unknown parameter",
        "exper- iments",
        "experiment results",
        "last section",
        "different distributions",
        "key point",
        "loss function",
        "ntr Xntr",
        "PtrðX",
        "PtrðyjxÞ",
        "¼ PteðXÞ",
        "Pteðxtr",
        "¼ PteðyjxÞ",
        "probability density",
        "� X � Y",
        "previous work",
        "probability distribution",
        "�Ptr",
        "�Pte",
        "Y.",
        "YÞ",
        "purpose",
        "researches",
        "procedure",
        "addition",
        "parameters",
        "domains",
        "dictions",
        "rest",
        "Sect.",
        "background",
        "Ztr",
        "Zte",
        "inputs",
        "process",
        "hÞ",
        "datasets",
        "ratio",
        "2",
        "Gaussian kernel basis functions",
        "kernel mean matching method",
        "many machine learning tasks",
        "adaptive K-NN reweighting method",
        "training-test K-NN reweighting method",
        "weighted machine learning model",
        "complete covariate shift process",
        "many density estimation methods",
        "Gaussian kernel functions",
        "Kullback–Leibler divergence",
        "reproducing-kernel Hilbert space",
        "2.3.1 K-NN reweighting method",
        "density estima- tion",
        "Local machine learning",
        "nearest-neighbour approximation method",
        "local classification method",
        "k-nearest test set",
        "many neighbour points",
        "less neighbour points",
        "k-nearest-neighbour reweighting methods",
        "squares importance biases",
        "kernel methods",
        "learning process",
        "k-NN methods",
        "many kinds",
        "nearest-neighbour methods",
        "local learning",
        "density similarity",
        "transfer learning",
        "neighbour region",
        "histogram methods",
        "clustering methods",
        "PteðxtrÞ",
        "PtrðxtrÞ",
        "prediction model",
        "146 Z. Guan",
        "significant effect",
        "one hand",
        "little relationship",
        "target point",
        "unnecessary interferences",
        "similar property",
        "small region",
        "big region",
        "other hand",
        "strong noise",
        "small proportion",
        "given point",
        "PðxÞ",
        "region volume",
        "one volume",
        "two stages",
        "test dataset",
        "first stage",
        "origin form",
        "test points",
        "long-distance points",
        "target data",
        "data point",
        "training distributions",
        "nearest neighbours",
        "training instances",
        "training point",
        "training data",
        "input densities",
        "common situations",
        "total number",
        "errors",
        "researchers",
        "forms",
        "formula",
        "Sugiyama",
        "series",
        "Kanamori",
        "Huang",
        "KMM",
        "Dai",
        "Pardoe",
        "list",
        "boosting",
        "rithms",
        "size",
        "effects",
        "way",
        "ktr",
        "kte",
        "nV",
        "Loog",
        "work",
        "formance",
        "neighbourhood",
        "fact",
        "knowledge",
        "extension",
        "adaptation",
        "Clustering-based",
        "view",
        "clustering-based reweighting meth- ods",
        "3.2 Training-test K-NN reweighting method",
        "Cross validation tech- nique",
        "Gaussian kernel function ratio",
        "empirical parameter estimation way",
        "Clustering-based reweighting method",
        "users’ psychological characteristics",
        "nearest-neighbour reweighting method",
        "other big values",
        "nearest test set",
        "other test points",
        "covariate shift methods",
        "labelled test data",
        "training data points",
        "k-NN method",
        "training data part",
        "empirical way",
        "togram method",
        "reweighting formula",
        "Gaussian kernels",
        "validation technique",
        "big bias",
        "nearest-neighbour point",
        "WeigðxtrÞ",
        "exponential term",
        "exponential law",
        "appropriate neighbour",
        "computation time",
        "comparable probability",
        "two parts",
        "important issue",
        "smaller value",
        "similar probability",
        "expð�cjjxtar",
        "small value",
        "2nneig Pnneig",
        "clustering algorithm",
        "test neighbours",
        "validation dataset",
        "k value",
        "mine k",
        "k � n",
        "density distance",
        "population size",
        "ktr Pktr",
        "k.",
        "¼ ktr",
        "Xk",
        "bandwidth",
        "effect",
        "denominator",
        "c.",
        "Enas",
        "Choi",
        "threshold",
        "reason",
        "1 jjxtr",
        "kind",
        "histograms",
        "147",
        "The Big Five personality model",
        "accurate density-region division methods",
        "lðyðiÞtr",
        "Big Five personality framework",
        "jClusteðxðiÞtr",
        "adaptive stepwise regression method",
        "WeigðxðiÞtr",
        "local transfer methods",
        "small enough region",
        "approximate computation way",
        "local reweighting methods",
        "2.4 Weighted regression model",
        "weighted learning model",
        "web users’ depression",
        "big influence",
        "five dimensions",
        "hjðxÞ",
        "regression function",
        "C � xÞ",
        "human personality",
        "histogram method",
        "high-dimensional difficulty",
        "high-dimensional situation",
        "previous stage",
        "real value",
        "prediction value",
        "unweighted models",
        "sion splines",
        "constant denoted",
        "hinge function",
        "total steps",
        "optimal performance",
        "logical characteristics",
        "three parts",
        "148 Z. Guan",
        "different districts",
        "unpleasant emotions",
        "intellectual curiosity",
        "CES-T scale",
        "many data",
        "one histogram",
        "same cluster",
        "Clustering method",
        "probability estimation",
        "lowing formula",
        "one cluster",
        "different weight",
        "importance weighting",
        "� CÞ",
        "instances",
        "detail",
        "number",
        "idea",
        "equal",
        "risk",
        "Xntr",
        "bias",
        "MARS",
        "Xm",
        "bjhj",
        "product",
        "two",
        "cients",
        "3 Experiments",
        "psychology",
        "agreeableness",
        "conscientiousness",
        "extraversion",
        "neuroticism",
        "openness",
        "tendency",
        "degree",
        "creativity",
        "preference",
        "novelty",
        "famous microblog service providers",
        "male users’ depression degree",
        "many feature dimensions",
        "mogorov–Smirnov test",
        "503 million registered users",
        "high dimension curse",
        "Big Five dimensions",
        "online self-report questionnaire",
        "users’ web behaviours",
        "time series data",
        "additional linguistic features",
        "Guangdong users’ personality",
        "istration time",
        "test set",
        "T test",
        "two-sample test",
        "Weibo users",
        "different users",
        "female users",
        "important factor",
        "training set",
        "3.1 Experiment setup",
        "Sina Weibo",
        "depression scales",
        "digital records",
        "pression behaviours",
        "screen name",
        "facial picture",
        "privacy settings",
        "individual privacy",
        "vate messages",
        "interpersonal behaviours",
        "social interaction",
        "punctuation marks",
        "perceptual processing",
        "stepwisefit method",
        "Matlab toolbox",
        "gender-personality experiment",
        "district-personality experiment",
        "depression experiment",
        "same distribution",
        "Gaussian distribution",
        "unknown distribution",
        "five categories",
        "online behaviours",
        "behavioural data",
        "personal image",
        "personal page",
        "sentiment words",
        "cognitive words",
        "Guangdong dataset",
        "female dataset",
        "sion experiments",
        "dynamic features",
        "personality experiments",
        "845 features",
        "20 features",
        "relationship",
        "personality/depression",
        "example",
        "suitability",
        "difference",
        "China",
        "research",
        "consent",
        "prediction",
        "May",
        "August",
        "562 participants",
        "profiles",
        "demographics",
        "expression",
        "self-statement",
        "concern",
        "comments",
        "strangers",
        "outcomes",
        "friends",
        "microblogs",
        "period",
        "apps",
        "June",
        "1000 participants",
        "characters",
        "numerals",
        "pronouns",
        "relevant",
        "Both",
        "22",
        "other local trans- fer learning methods",
        "adaptive k-NN transfer learning methods",
        "test k-NN transfer learning method",
        "other transfer learning methods",
        "famous transfer learning method",
        "regression transfer learning methods",
        "adaptive k-NN reweighting method",
        "Local kNN reweighting methods",
        "transfer learning way",
        "mean square error",
        "Kol- mogorov–Smirnov",
        "weighted risk model",
        "weighted risk form",
        "Kolmogorov–Smirnov test",
        "global transfer method",
        "clustering-based reweighting method",
        "single-dimension T test",
        "fixed k value",
        "other k-NN",
        "KMM reweighting method",
        "male users’ personality",
        "global k-NN",
        "regression software",
        "risk models",
        "training- test",
        "traditional method",
        "baseline method",
        "test results",
        "open source",
        "female datasets",
        "situa- tions",
        "trait A",
        "total size",
        "perfor- mances",
        "5 personality dimensions",
        "MARS method",
        "best results",
        "c value",
        "GkNN method",
        "result comparisons",
        "distribution divergence",
        "comparable performance",
        "parameter values",
        "C.",
        "4 dimensions",
        "MARS.",
        "neighbours",
        "Matlab/Octave",
        "rtu",
        "lv",
        "jekabsons",
        "tables",
        "figures",
        "nation",
        "AkNN1",
        "AkNN2",
        "MSE",
        "genders",
        "task",
        "2 features",
        "1 features",
        "probability",
        "confidence",
        "situations",
        "others",
        "advantage",
        "impact",
        "Fig.",
        "noise",
        "149",
        "25",
        "test results Condition A C E N O",
        "Local regression transfer learning results",
        "other local transfer learning methods",
        "stable density estimation method",
        "male users’ depression level",
        "k-NN transfer methods",
        "regression transfer methods",
        "clustering transfer methods",
        "other two traits",
        "least k value",
        "five personality traits",
        "other districts",
        "five traits",
        "clustering method",
        "0, outlier point",
        "strong influence",
        "optimization result",
        "three traits",
        "M S",
        "150 Z. Guan",
        "cs.rtu",
        "regular rule",
        "person- ality",
        "mogorov–Smirnov",
        "one trait",
        "3 feature dimensions",
        "5 feature dimensions",
        "The method",
        "best method",
        "different-gender datasets",
        "different-district datasets",
        "best performance",
        "optimal value",
        "Weibo data",
        "gender experiment",
        "most situations",
        "cluster number",
        "clusters increases",
        "prediction performance",
        "5 traits",
        "exception",
        "shake-up",
        "factor",
        "GkNN",
        "constant",
        "diversity",
        "Guangdong",
        "province",
        "model",
        "total",
        "95% confidence",
        "performances",
        "Table 2",
        "local k-NN family transfer learning methods",
        "global k-NN transfer learning method",
        "predic- tion bias",
        "different- distribution datasets",
        "appropriate k value",
        "transfer method",
        "test data",
        "different-distribution feature",
        "subtle nature",
        "density estimation",
        "training",
        "experiment",
        "Table",
        "result",
        "non",
        "Clust",
        "other",
        "3.5 Discussion",
        "conclusion"
      ],
      "merged_content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\n  \n\n  \n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods",
      "text": [
        "",
        ""
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}"
      ]
    },
    {
      "@search.score": 1.5471878,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\n\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\n\n\n\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\n\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\n\n\n\n\n\n\n\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwMDY0LTAxNS0xNTE1LTQucGRm0",
      "metadata_author": "Rik Das",
      "metadata_title": "Multi technique amalgamation for enhanced information identification with content based image data",
      "people": [
        "Rik Das1",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Raventós",
        "Madireddy",
        "Oliva Torralba",
        "Das",
        "rikdas78",
        "Camil",
        "Wang",
        "Annadurai",
        "Shanmugalakshmi",
        "Kekre",
        "Prakash",
        "Luo",
        "Valizadeh",
        "Thepade",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Ramírez",
        "Ortegón",
        "Rojas",
        "Otsu",
        "Shaikh",
        "Flickner",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Lu",
        "ary",
        "Fourier",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "Kim",
        "Zhu",
        "Shyu",
        "ElAlami",
        "Hiremath",
        "Pujari",
        "Yue",
        "Banerjee",
        "Gabor",
        "Jalab",
        "Shen",
        "Wu",
        "Irtaza",
        "Rahimi",
        "Moghaddam",
        "Subrahmanyam",
        "Walia",
        "Douze",
        "Zhao",
        "Txjixif",
        "Pratt",
        "Sridhar",
        "xmean",
        "xhi",
        "xlomeanxlo",
        "Qi",
        "Alsmadi"
      ],
      "keyphrases": [
        "multi tech- nique feature extraction process",
        "content based image data Rik Das1",
        "Creative Commons Attribution 4.0 International License",
        "traditional text based annotation technique",
        "content based feature extraction",
        "Dr. Camil Bulcke Path",
        "customary text based process",
        "Diverse low level features",
        "content based image identification",
        "Creative Commons license",
        "robust feature vectors",
        "Multi technique amalgamation",
        "low cost storage",
        "digital photo-capture devices",
        "four public datasets",
        "test Open Access",
        "P.O. Box",
        "digital image acquisition",
        "huge image repositories",
        "gigantic image datasets",
        "image capturing devices",
        "new research challenges",
        "three different techniques",
        "exist- ing techniques",
        "original author(s",
        "image identification technique",
        "enhanced information identification",
        "manual process",
        "image content",
        "Diverse applications",
        "image recognition",
        "image binarization",
        "image transform",
        "Image classification",
        "Image retrieval",
        "Raventós",
        "RESEARCH Das",
        "image information",
        "author information",
        "Conventional techniques",
        "art techniques",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Recent years",
        "computer power",
        "accessible internet",
        "computer vision",
        "machine learning",
        "Automatic derivation",
        "severe limitations",
        "aforesaid limitations",
        "resourceful foundation",
        "social media",
        "Oliva Torralba",
        "Corel Dataset",
        "Caltech Dataset",
        "recognition rate",
        "Classification result",
        "average increase",
        "retrieval result",
        "Slant transform",
        "unrestricted use",
        "appropriate credit",
        "Xavier Institute",
        "Social Service",
        "Purulia Road",
        "Full list",
        "effective alternative",
        "meaningful information",
        "Information Technology",
        "Wang Dataset",
        "decision fusion",
        "Morphological operator",
        "analogous images",
        "mass",
        "popularity",
        "indexing",
        "Madireddy",
        "Walia",
        "words",
        "mapping",
        "perception",
        "vocabulary",
        "person",
        "nature",
        "Abstract",
        "proliferation",
        "areas",
        "biomedicine",
        "military",
        "commerce",
        "education",
        "means",
        "success",
        "paper",
        "Precision",
        "state",
        "Otsu",
        "threshold",
        "article",
        "terms",
        "creativecommons",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "SpringerPlus",
        "DOI",
        "Correspondence",
        "1 Department",
        "Ranchi",
        "Jharkhand",
        "India",
        "end",
        "org",
        "Page",
        "26Das",
        "competence",
        "color",
        "content based image recog",
        "binarization based feature extraction",
        "content based image classification",
        "feature extraction Feature extraction",
        "Z score normalization",
        "orthogonal unitary matrices",
        "Appropriate threshold selection",
        "fusion based method",
        "main contribu- tion",
        "fractional energy coefficient",
        "fifteen fractional coefficients",
        "high dimensional data",
        "multi-technique feature extraction",
        "efficient feature extraction",
        "efficient image binarization",
        "multi technique fusion",
        "image trans- formation",
        "feature vector dimension",
        "Image binarization techniques",
        "energy spectrum",
        "image data",
        "subband coefficients",
        "novel techniques",
        "Diverse techniques",
        "single feature",
        "feature vectors",
        "feature spaces",
        "recognition decision",
        "hybrid architecture",
        "image identification",
        "fusion architecture",
        "Statistical validation",
        "trans- forms",
        "archi- tecture",
        "four subsections",
        "earlier works",
        "four topics",
        "image elements",
        "basis functions",
        "one representation",
        "two aspects",
        "critical components",
        "image patterns",
        "compact structure",
        "cient storage",
        "radical reduction",
        "seven image",
        "multiple scales",
        "tropic transform",
        "various factors",
        "uneven illumination",
        "research objectives",
        "research results",
        "transformation process",
        "aforesaid properties",
        "basis images",
        "Original images",
        "shape",
        "texture",
        "number",
        "features",
        "Comparison",
        "operators",
        "retrieval",
        "correlation",
        "connection",
        "contemporary",
        "Change",
        "domain",
        "set",
        "series",
        "Annadurai",
        "Shanmugalakshmi",
        "advantages",
        "waveforms",
        "use",
        "analysis",
        "transmission",
        "Kekre",
        "Thepade",
        "subbands",
        "Prakash",
        "Luo",
        "execution",
        "Ramírez- Ortegón",
        "Gradient vector flow fields",
        "multi technique feature extraction",
        "fusion based image identification",
        "superior prediction accuracy",
        "image signature extraction",
        "feature extraction technique",
        "feature level correlations",
        "Popular contour-based descriptors",
        "Enhanced classification results",
        "curvature scale space",
        "feature selection technique",
        "diverse extraction techniques",
        "region- based descriptors",
        "3 D color histogram",
        "shape feature extraction",
        "Two different categorization",
        "global threshold selection",
        "multilevel mean threshold",
        "image binarization techniques",
        "local threshold selection",
        "retrieval process complexity",
        "feature vector",
        "identification rate",
        "Image Content",
        "different techniques",
        "shape descriptors",
        "semantic retrieval",
        "Local descriptors",
        "local decisions",
        "shape information",
        "region-based descriptors",
        "information fusion",
        "early fusion",
        "late fusion",
        "hybrid fusion",
        "intermediate fusion",
        "adverse effect",
        "Contemporary literatures",
        "unfavourable influences",
        "binarized images",
        "standard deviation",
        "gray values",
        "Commercial viability",
        "existing literatures",
        "ary lines",
        "Fourier descriptor",
        "chain codes",
        "complex shapes",
        "Information recognition",
        "Recent studies",
        "four classes",
        "higher dimensions",
        "joint model",
        "genetic algorithm",
        "optimum boundaries",
        "numerical intervals",
        "memory con",
        "filter responses",
        "distinct features",
        "concentrated features",
        "multiple features",
        "Fusion methodologies",
        "rate learner",
        "Gabor filters",
        "Color moments",
        "texture features",
        "contrast",
        "computation",
        "Valizadeh",
        "average",
        "advantage",
        "spread",
        "calculation",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Rojas",
        "Shaikh",
        "Use",
        "systems",
        "Flickner",
        "PicToSeek",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Emphasize",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "area",
        "object",
        "Kim",
        "single",
        "input",
        "sepa",
        "combiner",
        "scalability",
        "comparison",
        "mix",
        "Zhu",
        "Shyu",
        "sumption",
        "ElAlami",
        "back propagation neural net- work",
        "fuzzy set theoretic approach",
        "angular radial transform descriptor",
        "artificial neural network",
        "spatial structure descriptors",
        "spatial orientation tree",
        "lesser computational overhead",
        "neural network architecture",
        "correct semantic retrieval",
        "Higher retrieval results",
        "semantic retrieval results",
        "HSV color space",
        "significant point features",
        "Three different techniques",
        "local edge bins",
        "non directional edge",
        "color layout descriptor",
        "greater feature dimension",
        "content-based image retrieval",
        "tex- ture features",
        "inter-class feature extraction",
        "fusion based classifier",
        "invariant color features",
        "Gabor texture descriptor",
        "invariant moments",
        "retrieval decisions",
        "retrieval performance",
        "retrieval rate",
        "retrieval purpose",
        "color histogram",
        "color co",
        "color motif",
        "color moment",
        "Feature vectors",
        "popular feature",
        "32 feature maps",
        "feature values",
        "feature size",
        "occurrence matrix",
        "Recognition process",
        "image signatures",
        "Multi view",
        "Wavelet packets",
        "Eigen values",
        "sub repository",
        "main image",
        "right neighbourhood",
        "query image",
        "scan pattern",
        "equal weights",
        "Precision values",
        "individual techniques",
        "Six semantics",
        "sub- image",
        "tion techniques",
        "GIST descriptor",
        "same size",
        "16 average value",
        "other hand",
        "horizontal edge",
        "shape features",
        "EHD) features",
        "vertical edge",
        "edge images",
        "45° edge",
        "135° edge",
        "Hiremath",
        "Pujari",
        "Yue",
        "points",
        "similarity",
        "Banerjee",
        "Jalab",
        "increased",
        "Shen",
        "Wu",
        "authors",
        "Irtaza",
        "kind",
        "training",
        "response",
        "intra-class",
        "Rahimi",
        "Moghaddam",
        "CCM",
        "difference",
        "pixels",
        "DBPSP",
        "ANN",
        "Subrahmanyam",
        "MCMCM",
        "colour",
        "CMs",
        "sub-image",
        "unique",
        "Methods",
        "operator",
        "4 scales",
        "8 orientations",
        "Douze",
        "region",
        "Red Component Green Component Blue Component",
        "local threshold selec- tion method",
        "popular global threshold selection method",
        "image feature point extraction",
        "different gray level pixels",
        "four subsec- tions",
        "three color components",
        "necessary image information",
        "squared Euclidian distance",
        "document image binarzation",
        "Conventional BoW model",
        "feature extraction",
        "thresholding method",
        "parametric method",
        "optimal threshold",
        "words model",
        "information losses",
        "fifth subsection",
        "test images",
        "binari- zation",
        "dant details",
        "two classes",
        "foreground pixels",
        "background pixels",
        "class variance",
        "The separation",
        "Comprehensive investigation",
        "class probabilities",
        "Total variance",
        "two terms",
        "SIFT algorithm",
        "SIFT descriptors",
        "cluster members",
        "codebook generation",
        "final step",
        "huge computational",
        "BoW generation",
        "class means",
        "tering process",
        "descriptor dimension",
        "classification",
        "techniques",
        "methods",
        "fusion",
        "architecture",
        "following",
        "description",
        "datasets",
        "Fig.",
        "way",
        "combined",
        "sum",
        "variances",
        "Eq.",
        "Eqs.",
        "q1",
        "effect",
        "contributions",
        "bag",
        "Zhao",
        "size",
        "problem",
        "omissions",
        "stability",
        "clustering",
        "codewords",
        "overhead",
        "∑",
        "σ",
        "N*N feature vector Feature Vector Dimension Reduction",
        "Compute binary image maps",
        "N unitary slant matrix",
        "Blue (B) color component",
        "N × N matrix",
        "higher intensity feature vectors",
        "two dimensional slant transform",
        "N × 1 vector",
        "higher intensity group",
        "three different color",
        "efficient algorithm design",
        "discrete Fourier transform",
        "One dimensional transform",
        "lower intensity group",
        "Slant transform matrices",
        "frequency domain information",
        "local threshold value",
        "spatial domain information",
        "insignificant transform coefficients",
        "drastic reduction",
        "smaller dimension",
        "Energy compaction property",
        "unitary transform",
        "spatial information",
        "matrix multiplication",
        "two groups",
        "orthogonal transform",
        "inverse transform",
        "transform operation",
        "grey values",
        "two codewords",
        "large fraction",
        "faster execution",
        "tion operation",
        "computational overhead",
        "energy conservation",
        "signal energy",
        "significant role",
        "Partial Coefficients",
        "sequential transformations",
        "average energy",
        "test image",
        "image features",
        "monochrome image",
        "image quality",
        "image line",
        "components R",
        "real components",
        "pixel values",
        "average coding",
        "Transforms",
        "approach",
        "cluster",
        "mean",
        "codebook",
        "Tx",
        "Method",
        "End",
        "cer",
        "operations",
        "capacity",
        "example",
        "characteristic",
        "Green",
        "8 bits",
        "1 bit",
        "images",
        "24–2 bits",
        "Pratt",
        "less",
        "column",
        "row",
        "forward",
        "Eqs",
        "The energy compaction property",
        "gray scale opening operation",
        "lower intensity feature vector",
        "higher intensity feature vector",
        "entire feature vector set",
        "feature vector extraction process",
        "shorter dis- tance",
        "feature vector formation",
        "highest classification result",
        "Blue color components",
        "top-hat transform technique",
        "feature vector database",
        "significant image information",
        "image transform End",
        "image similarity measures",
        "Higher similarity",
        "database images",
        "working process",
        "Similar process",
        "classification results",
        "slant transform",
        "complete set",
        "partial coefficient",
        "fractional coefficient",
        "small patch",
        "Human perception",
        "shape context",
        "point correspondences",
        "considerable contribution",
        "closing operations",
        "hat transformation",
        "bright peaks",
        "code- book",
        "noteworthy image",
        "original image",
        "visual words",
        "series form",
        "cal operator",
        "tophat operator",
        "one cluster",
        "codebook size",
        "peak detector",
        "Red, Green",
        "coefficients",
        "2. Peak",
        "algorithm",
        "query",
        "block",
        "12 elements",
        "dimension",
        "variant",
        "Sridhar",
        "Exit",
        "BoW",
        "methodology",
        "clusters",
        "Determination",
        "distance",
        "Dunham",
        "σ   Red Component Green Component Blue Component",
        "Three different distance measures",
        "forward Neural Network Classifier",
        "support vector machine",
        "nearest neigh- bors",
        "city block distance",
        "MSE) distance metric",
        "Canberra Distance measure",
        "fusion based classification",
        "first hidden layer",
        "input feature vectors",
        "mean squared error",
        "Data standardization technique",
        "back propagation technique",
        "supervised learning procedure",
        "multi layer perceptron",
        "database image T",
        "query image Q",
        "color component",
        "SVM) classifier",
        "classifier types",
        "network prediction",
        "Euclidian distance",
        "final distance",
        "classification decision",
        "Multilayer Perceptron",
        "input layer",
        "input features",
        "input units",
        "classified query",
        "following sections",
        "tophat transform",
        "xloF.V.",
        "greater effect",
        "common range",
        "weighted sum",
        "appropriate output",
        "sification performance",
        "training tuple",
        "backward direction",
        "normalization process",
        "classified image",
        "Top-Hat operator",
        "higher values",
        "precision values",
        "qp background",
        "individual distances",
        "class majority",
        "similar images",
        "top 20 images",
        "target value",
        "foreground",
        "operation",
        "xmean",
        "stdev",
        "VF",
        "xlomeanxlo",
        "Qi",
        "attributes",
        "possibilities",
        "Dcityblock",
        "Deuclidian",
        "DMSE",
        "distn",
        "disti",
        "weights",
        "interest",
        "Ranking",
        "feed",
        "MLP",
        "Alsmadi",
        "optimization",
        "modifications",
        "The",
        "µ",
        "feature vector dataset",
        "Support vector machine",
        "original training data",
        "Optimal separating hyperplane",
        "two different classes",
        "bias node",
        "subsequent layer",
        "hidden layer",
        "Weighted output",
        "final layer",
        "output layer",
        "forward property",
        "higher dimension",
        "nonlinear mapping",
        "new dimension",
        "class labels",
        "summation",
        "utes",
        "nodes",
        "half",
        "inputs",
        "processing"
      ],
      "merged_content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\n  \n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n  \n\n  \n\n  \n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n  \n\n  \n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n  \n\n  \n\n  \n\n  \n\n - \n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an",
      "text": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "-",
        "O O O O - O Large Margin - - O",
        "",
        "Lins LaBern TES VA",
        "",
        "IRE-USE THINGS mezanina SITED SERIES OP AMI MATE",
        "",
        "",
        "Published online: 01 December 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"-\",\"lines\":[{\"boundingBox\":[{\"x\":32,\"y\":165},{\"x\":34,\"y\":177},{\"x\":28,\"y\":178},{\"x\":26,\"y\":167}],\"text\":\"-\"}],\"words\":[{\"boundingBox\":[{\"x\":33,\"y\":168},{\"x\":33,\"y\":172},{\"x\":27,\"y\":173},{\"x\":27,\"y\":169}],\"text\":\"-\"}]}",
        "{\"language\":\"en\",\"text\":\"O O O O - O Large Margin - - O\",\"lines\":[{\"boundingBox\":[{\"x\":498,\"y\":25},{\"x\":532,\"y\":20},{\"x\":533,\"y\":48},{\"x\":500,\"y\":49}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":564,\"y\":107},{\"x\":599,\"y\":106},{\"x\":596,\"y\":137},{\"x\":571,\"y\":138}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":675,\"y\":102},{\"x\":711,\"y\":102},{\"x\":709,\"y\":136},{\"x\":676,\"y\":137}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":560,\"y\":201},{\"x\":598,\"y\":190},{\"x\":600,\"y\":213},{\"x\":575,\"y\":223}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":503,\"y\":225},{\"x\":517,\"y\":235},{\"x\":512,\"y\":241},{\"x\":498,\"y\":230}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":672,\"y\":248},{\"x\":708,\"y\":237},{\"x\":711,\"y\":262},{\"x\":684,\"y\":268}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":254,\"y\":315},{\"x\":356,\"y\":213},{\"x\":374,\"y\":231},{\"x\":269,\"y\":333}],\"text\":\"Large Margin\"},{\"boundingBox\":[{\"x\":591,\"y\":312},{\"x\":603,\"y\":326},{\"x\":598,\"y\":330},{\"x\":586,\"y\":317}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":607,\"y\":329},{\"x\":623,\"y\":345},{\"x\":617,\"y\":350},{\"x\":601,\"y\":335}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":407,\"y\":680},{\"x\":444,\"y\":679},{\"x\":444,\"y\":717},{\"x\":407,\"y\":718}],\"text\":\"O\"}],\"words\":[{\"boundingBox\":[{\"x\":498,\"y\":23},{\"x\":513,\"y\":22},{\"x\":516,\"y\":50},{\"x\":500,\"y\":50}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":565,\"y\":107},{\"x\":584,\"y\":106},{\"x\":585,\"y\":137},{\"x\":566,\"y\":138}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":675,\"y\":102},{\"x\":694,\"y\":102},{\"x\":695,\"y\":137},{\"x\":675,\"y\":137}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":562,\"y\":200},{\"x\":576,\"y\":195},{\"x\":585,\"y\":219},{\"x\":571,\"y\":224}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":505,\"y\":225},{\"x\":510,\"y\":229},{\"x\":506,\"y\":235},{\"x\":501,\"y\":232}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":672,\"y\":246},{\"x\":686,\"y\":242},{\"x\":693,\"y\":267},{\"x\":679,\"y\":269}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":254,\"y\":314},{\"x\":296,\"y\":273},{\"x\":313,\"y\":291},{\"x\":269,\"y\":332}],\"text\":\"Large\"},{\"boundingBox\":[{\"x\":300,\"y\":269},{\"x\":355,\"y\":214},{\"x\":372,\"y\":232},{\"x\":317,\"y\":287}],\"text\":\"Margin\"},{\"boundingBox\":[{\"x\":592,\"y\":312},{\"x\":595,\"y\":315},{\"x\":590,\"y\":320},{\"x\":587,\"y\":317}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":608,\"y\":330},{\"x\":611,\"y\":333},{\"x\":606,\"y\":339},{\"x\":603,\"y\":336}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":407,\"y\":680},{\"x\":427,\"y\":679},{\"x\":428,\"y\":716},{\"x\":407,\"y\":717}],\"text\":\"O\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Lins LaBern TES VA\",\"lines\":[{\"boundingBox\":[{\"x\":29,\"y\":276},{\"x\":114,\"y\":266},{\"x\":116,\"y\":278},{\"x\":31,\"y\":288}],\"text\":\"Lins LaBern\"},{\"boundingBox\":[{\"x\":154,\"y\":392},{\"x\":143,\"y\":462},{\"x\":126,\"y\":460},{\"x\":135,\"y\":390}],\"text\":\"TES VA\"}],\"words\":[{\"boundingBox\":[{\"x\":33,\"y\":277},{\"x\":60,\"y\":274},{\"x\":62,\"y\":285},{\"x\":34,\"y\":289}],\"text\":\"Lins\"},{\"boundingBox\":[{\"x\":67,\"y\":273},{\"x\":110,\"y\":268},{\"x\":111,\"y\":279},{\"x\":68,\"y\":284}],\"text\":\"LaBern\"},{\"boundingBox\":[{\"x\":154,\"y\":393},{\"x\":149,\"y\":421},{\"x\":132,\"y\":418},{\"x\":136,\"y\":390}],\"text\":\"TES\"},{\"boundingBox\":[{\"x\":147,\"y\":435},{\"x\":144,\"y\":458},{\"x\":127,\"y\":456},{\"x\":130,\"y\":433}],\"text\":\"VA\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"IRE-USE THINGS mezanina SITED SERIES OP AMI MATE\",\"lines\":[{\"boundingBox\":[{\"x\":620,\"y\":22},{\"x\":659,\"y\":24},{\"x\":658,\"y\":38},{\"x\":619,\"y\":37}],\"text\":\"IRE-USE\"},{\"boundingBox\":[{\"x\":666,\"y\":28},{\"x\":715,\"y\":30},{\"x\":714,\"y\":45},{\"x\":666,\"y\":42}],\"text\":\"THINGS\"},{\"boundingBox\":[{\"x\":774,\"y\":240},{\"x\":823,\"y\":234},{\"x\":825,\"y\":243},{\"x\":775,\"y\":249}],\"text\":\"mezanina\"},{\"boundingBox\":[{\"x\":770,\"y\":505},{\"x\":886,\"y\":517},{\"x\":885,\"y\":527},{\"x\":769,\"y\":514}],\"text\":\"SITED SERIES OP AMI\"},{\"boundingBox\":[{\"x\":358,\"y\":515},{\"x\":399,\"y\":522},{\"x\":397,\"y\":535},{\"x\":355,\"y\":528}],\"text\":\"MATE\"}],\"words\":[{\"boundingBox\":[{\"x\":619,\"y\":22},{\"x\":659,\"y\":23},{\"x\":658,\"y\":38},{\"x\":619,\"y\":36}],\"text\":\"IRE-USE\"},{\"boundingBox\":[{\"x\":681,\"y\":29},{\"x\":714,\"y\":31},{\"x\":714,\"y\":45},{\"x\":681,\"y\":44}],\"text\":\"THINGS\"},{\"boundingBox\":[{\"x\":775,\"y\":240},{\"x\":819,\"y\":235},{\"x\":820,\"y\":244},{\"x\":776,\"y\":249}],\"text\":\"mezanina\"},{\"boundingBox\":[{\"x\":771,\"y\":505},{\"x\":801,\"y\":509},{\"x\":800,\"y\":518},{\"x\":770,\"y\":514}],\"text\":\"SITED\"},{\"boundingBox\":[{\"x\":806,\"y\":509},{\"x\":845,\"y\":513},{\"x\":843,\"y\":522},{\"x\":805,\"y\":518}],\"text\":\"SERIES\"},{\"boundingBox\":[{\"x\":849,\"y\":514},{\"x\":862,\"y\":515},{\"x\":861,\"y\":525},{\"x\":848,\"y\":523}],\"text\":\"OP\"},{\"boundingBox\":[{\"x\":866,\"y\":515},{\"x\":886,\"y\":517},{\"x\":885,\"y\":527},{\"x\":865,\"y\":525}],\"text\":\"AMI\"},{\"boundingBox\":[{\"x\":358,\"y\":516},{\"x\":396,\"y\":522},{\"x\":394,\"y\":535},{\"x\":356,\"y\":528}],\"text\":\"MATE\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 01 December 2015\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":1051,\"y\":15},{\"x\":1051,\"y\":70},{\"x\":4,\"y\":70}],\"text\":\"Published online: 01 December 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":273,\"y\":15},{\"x\":272,\"y\":71},{\"x\":4,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":15},{\"x\":496,\"y\":15},{\"x\":495,\"y\":71},{\"x\":291,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":507,\"y\":15},{\"x\":579,\"y\":15},{\"x\":578,\"y\":71},{\"x\":506,\"y\":71}],\"text\":\"01\"},{\"boundingBox\":[{\"x\":595,\"y\":15},{\"x\":897,\"y\":15},{\"x\":895,\"y\":71},{\"x\":593,\"y\":71}],\"text\":\"December\"},{\"boundingBox\":[{\"x\":909,\"y\":15},{\"x\":1047,\"y\":16},{\"x\":1044,\"y\":70},{\"x\":906,\"y\":71}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 1.5471878,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNzA4LTAyMC0wMDEwOS14LnBkZg2",
      "metadata_author": "Ferdousi Sabera Rawnaque ",
      "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review",
      "people": [
        "Rawnaque",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "Khondaker Abdullah Al Mamun",
        "Taylor Francis",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "Price Çakar",
        "Gong",
        "Pilelienė",
        "Grigaliūnaitė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "Soria Morillo",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "katraman",
        "Touchette",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Missagila",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "Nemorin",
        "Davidson",
        "Krugman",
        "Missaglia"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Khondaker Abdullah Al Mamun",
        "high time resolution advantages",
        "consumer emotion recognition-based experiments",
        "other third party material",
        "video advertisement-based Neuromarketing experiments",
        "Physiological response measuring techniques",
        "other machine learning algorithms",
        "Creative Commons licence",
        "Support Vector Machine",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "magnetic resonance imaging",
        "heart rate monitoring",
        "traditional filtering methods",
        "independent component analysis",
        "Linear Discriminant Analysis",
        "highest average accuracy",
        "consumer response prediction",
        "prefrontal alpha band",
        "skin conductance recording",
        "Brain computer interface",
        "brain–computer interface",
        "Artificial Neural Network",
        "empirical research findings",
        "prevalent marketing stimuli",
        "first systematic review",
        "unspoken response",
        "Neural recording",
        "consumer goods",
        "consumer preferences",
        "Brain Inf.",
        "brain recordings",
        "neural signal",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "commercial area",
        "last 5 years",
        "valid databases",
        "promotion forms",
        "asymmetry theory",
        "many researchers",
        "low cost",
        "eye tracking",
        "facial mapping",
        "artifact removal",
        "future researchers",
        "vital information",
        "novel contributions",
        "The Author",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "BCI) technology",
        "interdisciplinary bridge",
        "ultimate sale",
        "targeted audiences",
        "expanding economy",
        "effective marketing",
        "57 relevant literatures",
        "intended use",
        "permitted use",
        "good product",
        "new businesses",
        "Neuromarketing field",
        "Technological advancements",
        "opportunities",
        "Abstract",
        "academic",
        "interest",
        "interpreting",
        "tool",
        "consumers",
        "article",
        "purpose",
        "authors",
        "total",
        "basic",
        "trend",
        "nals",
        "electroencephalogram",
        "EEG",
        "functional",
        "fMRI",
        "parallel",
        "classification",
        "ANN",
        "SVM",
        "LDA",
        "Keywords",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "iveco",
        "1 Introduction",
        "application",
        "invasive",
        "neuroscience",
        "perception",
        "changing",
        "non-invasive brain signal record- ing techniques",
        "1 Advanced Intelligent Multidisciplinary Systems Lab",
        "transcranial mag- netic stimulator",
        "Functional mag- netic resonance",
        "Open Access Brain Informatics",
        "image processing techniques",
        "nal recording techniques",
        "functional near-infrared spectroscopy",
        "com- petitive market",
        "United International University",
        "positron emission tomography",
        "machine learning algorithms",
        "skin conductance measurements",
        "neuronal activ- ity",
        "marketing research techniques",
        "ventional market research",
        "traditional survey methods",
        "neural recording devices",
        "Different stimuli trigger",
        "consumers’ purchase decisions",
        "various marketing stimuli",
        "Neuromarketing World Forum",
        "Advanced Research",
        "human brain",
        "traditional marketing",
        "Neuro- marketing",
        "neuronal signals",
        "academic research",
        "qualitative assessment",
        "posteriori analysis",
        "Full list",
        "author information",
        "group discussion",
        "personal interviews",
        "consumer feedback",
        "time requirement",
        "high cost",
        "unreliable information",
        "inaccurate results",
        "possible meanings",
        "new door",
        "buying behavior",
        "tal states",
        "heart rate",
        "spectral analysis",
        "accurate depiction",
        "Early years",
        "contro- versy",
        "high promises",
        "consumer mind",
        "buy buttons",
        "consumer behavior",
        "Business Association",
        "annual event",
        "neurophysiological signals",
        "Neuromarketing research",
        "field trials",
        "unspoken cognitive",
        "Neuromarketing Science",
        "consumer response",
        "associated response",
        "emotional responses",
        "quantitative",
        "products",
        "surveys",
        "Correspondence",
        "Institute",
        "Dhaka",
        "Bangladesh",
        "end",
        "org",
        "Page",
        "19Rawnaque",
        "observations",
        "approaches",
        "limitations",
        "contrast",
        "customer",
        "electroencephalography",
        "magnetoencephalography",
        "MEG",
        "TMS",
        "fNIRS",
        "examples",
        "withdrawal",
        "change",
        "brainwaves",
        "researchers",
        "help",
        "excitement",
        "engagement",
        "stress",
        "insight",
        "audience",
        "preferences",
        "dislikes",
        "academician",
        "marketers",
        "due",
        "lack",
        "groundwork",
        "claim",
        "scrutiny",
        "scope",
        "NMSBA",
        "gap",
        "dialogue",
        "other physiological signal recording device",
        "best brain signal recording tool",
        "address inclusion–exclusion criteria",
        "relevant academic journal articles",
        "neural recording tools",
        "industry–academia collaboration",
        "brief intro- duction",
        "unbiased evidence collection",
        "clear con- clusions",
        "Numerous literature reviews",
        "other literature review",
        "date research scopes",
        "systematic literature review",
        "new research field",
        "signal processing",
        "inclusion criteria",
        "Academic research",
        "relevant inference",
        "new perspective",
        "brain signals",
        "cal tools",
        "systematic review",
        "research question",
        "brain regions",
        "150 consumer neuroscience",
        "big brands",
        "efficient way",
        "lytical accuracy",
        "building blocks",
        "theoretical aspect",
        "business ethics",
        "current methods",
        "art findings",
        "impartial conclusion",
        "exhaustive search",
        "experimental findings",
        "neu- ral",
        "biometric data",
        "Book chapters",
        "empirical experiments",
        "engineering part",
        "pretational methodologies",
        "important results",
        "lowing questions",
        "marketing stimuli",
        "Neuromarketing experiments",
        "platform",
        "panies",
        "globe",
        "Google",
        "Microsoft",
        "Unilever",
        "insights",
        "tailored",
        "breakthrough",
        "acceptance",
        "world",
        "capacities",
        "management",
        "psychology",
        "focus",
        "regard",
        "premises",
        "types",
        "analysis",
        "techniques",
        "comprehensive",
        "knowledge",
        "methodology",
        "state",
        "synthesis",
        "recommendation",
        "body",
        "objective",
        "obligation",
        "investigation",
        "existing",
        "literatures",
        "evance",
        "studies",
        "analytical",
        "BCI",
        "language",
        "English",
        "brain signal recording techniques",
        "Neural response recording techniques",
        "Brain signal processing",
        "relevant existing literatures",
        "Machine learning applications",
        "autonomous nervous system",
        "different brain regions",
        "galvanic skin response",
        "Wiley Online Library",
        "Neuromarketing Product Chew",
        "neural data",
        "3  Systematic review",
        "database source",
        "book chapters",
        "six databases",
        "Taylor Francis",
        "initial article",
        "action coding",
        "successful means",
        "withdrawal actions",
        "engineering perspective",
        "lyze activities",
        "Price Çakar",
        "Grigaliūnaitė",
        "Soria Morillo",
        "marketing strategies",
        "Neuromarketing questions",
        "search item",
        "Science Direct",
        "IEEE Xplore",
        "Table 1 Number",
        "database Results",
        "Emerald insight",
        "Neuromarketing studies",
        "following dimensions",
        "Marketing stimuli",
        "Table 2 Studies",
        "57 research articles",
        "931 articles",
        "internet",
        "Neuro-marketing",
        "publications",
        "aggregation",
        "Sage",
        "mulation",
        "title",
        "abstract",
        "keywords",
        "critical",
        "terms",
        "trends",
        "advancements",
        "Activation",
        "facial",
        "consumer",
        "arousal",
        "attention",
        "study",
        "Name",
        "tools",
        "statistical",
        "sis",
        "paper",
        "form",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Touchette",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "al.",
        "Gong",
        "Pilelienė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Promotion",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "neural recording systems",
        "signal capturing technologies",
        "heart rate Cherubino",
        "EEG Soria Morillo",
        "brain region",
        "brain functions",
        "brain anatomy",
        "brain areas",
        "brain activity",
        "EMG Missagila",
        "Eye tracking",
        "Galvanic skin",
        "atic review",
        "human behavior",
        "cal functions",
        "true essence",
        "anatomical functionality",
        "high accuracy",
        "external stimuli",
        "fNIRS Çakir",
        "Neuromarketing Cherubino",
        "fMRI Venkatraman",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "subjects",
        "matter",
        "interconnection",
        "actions",
        "hidden",
        "ences",
        "section",
        "targeted",
        "signals",
        "associated",
        "number",
        "3D (three dimensional) image",
        "machine learning techniques",
        "tral nervous system",
        "predictive modeling framework",
        "3D visualiza- tion",
        "other marketing models",
        "consumer behavior researchers",
        "E-commerce prod- ucts",
        "gift card limit",
        "virtual 3D bracelet",
        "30 existing image",
        "3D shapes",
        "consumer demand",
        "Consumer response",
        "consumer choice",
        "different strategies",
        "market- ing",
        "quantitative assessment",
        "rological data",
        "thinking procedures",
        "last 5  years",
        "two forms",
        "physical object",
        "important role",
        "user esthetics",
        "new area",
        "mathematical model",
        "lis superformula",
        "school bags",
        "first-time buyers",
        "partici- pants",
        "commerce website",
        "negative emotion",
        "necessary buttons",
        "sorting options",
        "Retail businesses",
        "large amount",
        "large number",
        "mock shop",
        "right-handed participants",
        "test participants",
        "different designs",
        "shoe designs",
        "E-commerce products",
        "retail products",
        "test subjects",
        "normal vision",
        "brain response",
        "user experience",
        "42 product images",
        "product search",
        "wrong product",
        "possible preference",
        "60 bracelet",
        "medications",
        "history",
        "neuropathology",
        "Nemorin",
        "price",
        "promotions",
        "service",
        "beverage",
        "decision",
        "objects",
        "recognition",
        "bracelets",
        "advantage",
        "apparels",
        "accessory",
        "items",
        "sweaters",
        "shoes",
        "wrist",
        "watches",
        "view",
        "maximum",
        "money",
        "thousands",
        "blueprints",
        "manufacturing",
        "failures",
        "3.1",
        "corporate social respon- sibilities",
        "frontal asymmetry theory",
        "female undergraduate students",
        "capital white letter",
        "luxury) prod- ucts",
        "high emotional value",
        "higher emotional value",
        "shoe selection time",
        "high-resolution computer screen",
        "60 basic brand items",
        "wine tasting experiment",
        "regular brand products",
        "luxury brand products",
        "brain response-based prediction",
        "consumer brain response",
        "60 luxury items",
        "gift value",
        "brand images",
        "brand names",
        "brand logos",
        "national brand",
        "social atmosphere",
        "shoe experiment",
        "EEG signals",
        "Likert scale",
        "self-report-based methods",
        "sales data",
        "12.1% profit growth",
        "36.4% profit growth",
        "apparel products",
        "young adults",
        "random order",
        "alone atmosphere",
        "brain responses",
        "consumer attitude",
        "Tahoma font",
        "black background",
        "other hand",
        "implicit response",
        "different types",
        "important factor",
        "interesting concept",
        "tional companies",
        "responsible companies",
        "decision-making time",
        "passive role",
        "keting strategies",
        "lower degree",
        "con- trol",
        "pivotal role",
        "purchase decision",
        "two wines",
        "lesser price",
        "rank",
        "simulation",
        "choice",
        "Davidson",
        "comparison",
        "brands",
        "blocks",
        "popular",
        "experiments",
        "branding",
        "user",
        "first-time",
        "promotional",
        "relation",
        "author",
        "company",
        "socially",
        "influence",
        "discounts",
        "gifts",
        "gift-giving",
        "ambiguity",
        "40",
        "mon experimental design procedure",
        "merchandise product advertisement clips",
        "wine tasting session",
        "older adults’ reaction",
        "mated review systems",
        "different aged population",
        "facial biometric sensors",
        "TV commer- cials",
        "six smartphone commercials",
        "super bowl commercials",
        "promotion-based Neuromarketing experiments",
        "The Neuromarketing experiments",
        "sustainable product designs",
        "seven TV commercials",
        "unfavorable TV commercials",
        "consecutive advertise- ments",
        "cognitive neurophysiological indices",
        "experimental setting",
        "conventional product",
        "14 TV commercials",
        "100 TV commercials",
        "different brands",
        "Nestle advertisement",
        "television commercials",
        "consecutive advertisements",
        "TV advertisements",
        "equal price",
        "short movies",
        "key focus",
        "higher rate",
        "memory rate",
        "time distance",
        "neutral stimuli",
        "white screen",
        "green scenario",
        "electrical activity",
        "advanced algorithms",
        "emotional changes",
        "cerebral activity",
        "strong images",
        "pulse analysis",
        "print media",
        "verbal narrative",
        "visual narrative",
        "graduate students",
        "class rank",
        "emotional states",
        "visual attentions",
        "lit- eratures",
        "standard time",
        "video advertisements",
        "neural response",
        "consumer engagement",
        "audience brain",
        "animated commercial",
        "web page",
        "user preference",
        "audiences’ taste",
        "user attention",
        "favorable advertisements",
        "static advertisements",
        "50 commercials",
        "communication",
        "Neuromarketers",
        "air",
        "extract",
        "happiness",
        "surprise",
        "10 s",
        "participants",
        "observation",
        "Krugman",
        "success",
        "neuroimaging",
        "gies",
        "TVC",
        "30 s",
        "other",
        "videos",
        "social adver- tisements",
        "social advertisement stimuli",
        "social groups",
        "documentary film",
        "gaming video",
        "true response",
        "different purposes",
        "gender-related advertisements",
        "appli- cation",
        "cigarette commercials",
        "smoking cessa",
        "atten- tion",
        "substantial role",
        "advertisement industry",
        "gender-targeted marketing",
        "tion frames",
        "drama",
        "Neuromarketing",
        "The",
        "ads",
        "messages",
        "smokers",
        "celebrity",
        "endorsement",
        "Missaglia",
        "research"
      ],
      "merged_content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research o",
      "text": [
        "36.0 a 6.0 10.0 4.0 22.0 10.0 Hz 1 20 10 10 -20 -30 -40 Power 10*log . 0(/V/Hz) -50 -60 5 10 15 20 25 30 35 40 45 50 Frequency (Hz) a (a) ----- Servicntermin-Buchung & . -- - 1 (b) 3 Intensity of the view: low high b C",
        "Published online: 21 September 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"36.0 a 6.0 10.0 4.0 22.0 10.0 Hz 1 20 10 10 -20 -30 -40 Power 10*log . 0(/V/Hz) -50 -60 5 10 15 20 25 30 35 40 45 50 Frequency (Hz) a (a) ----- Servicntermin-Buchung & . -- - 1 (b) 3 Intensity of the view: low high b C\",\"lines\":[{\"boundingBox\":[{\"x\":1397,\"y\":0},{\"x\":1432,\"y\":3},{\"x\":1431,\"y\":25},{\"x\":1396,\"y\":21}],\"text\":\"36.0\"},{\"boundingBox\":[{\"x\":25,\"y\":4},{\"x\":60,\"y\":5},{\"x\":58,\"y\":37},{\"x\":25,\"y\":36}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":200,\"y\":4},{\"x\":224,\"y\":4},{\"x\":223,\"y\":23},{\"x\":200,\"y\":23}],\"text\":\"6.0\"},{\"boundingBox\":[{\"x\":500,\"y\":0},{\"x\":532,\"y\":3},{\"x\":532,\"y\":22},{\"x\":501,\"y\":19}],\"text\":\"10.0\"},{\"boundingBox\":[{\"x\":805,\"y\":1},{\"x\":833,\"y\":4},{\"x\":832,\"y\":22},{\"x\":804,\"y\":21}],\"text\":\"4.0\"},{\"boundingBox\":[{\"x\":1098,\"y\":2},{\"x\":1127,\"y\":2},{\"x\":1128,\"y\":25},{\"x\":1099,\"y\":25}],\"text\":\"22.0\"},{\"boundingBox\":[{\"x\":1688,\"y\":0},{\"x\":1749,\"y\":2},{\"x\":1749,\"y\":21},{\"x\":1688,\"y\":19}],\"text\":\"10.0 Hz\"},{\"boundingBox\":[{\"x\":137,\"y\":342},{\"x\":137,\"y\":404},{\"x\":92,\"y\":401},{\"x\":92,\"y\":340}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":62,\"y\":345},{\"x\":90,\"y\":345},{\"x\":88,\"y\":376},{\"x\":62,\"y\":375}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":58,\"y\":409},{\"x\":92,\"y\":412},{\"x\":92,\"y\":441},{\"x\":59,\"y\":438}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":61,\"y\":533},{\"x\":98,\"y\":535},{\"x\":93,\"y\":564},{\"x\":59,\"y\":561}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":58,\"y\":594},{\"x\":91,\"y\":597},{\"x\":87,\"y\":625},{\"x\":56,\"y\":623}],\"text\":\"-20\"},{\"boundingBox\":[{\"x\":55,\"y\":657},{\"x\":95,\"y\":656},{\"x\":90,\"y\":686},{\"x\":52,\"y\":686}],\"text\":\"-30\"},{\"boundingBox\":[{\"x\":56,\"y\":721},{\"x\":94,\"y\":721},{\"x\":91,\"y\":746},{\"x\":54,\"y\":745}],\"text\":\"-40\"},{\"boundingBox\":[{\"x\":5,\"y\":775},{\"x\":5,\"y\":491},{\"x\":40,\"y\":491},{\"x\":38,\"y\":775}],\"text\":\"Power 10*log . 0(/V/Hz)\"},{\"boundingBox\":[{\"x\":56,\"y\":779},{\"x\":96,\"y\":780},{\"x\":93,\"y\":812},{\"x\":53,\"y\":811}],\"text\":\"-50\"},{\"boundingBox\":[{\"x\":58,\"y\":839},{\"x\":93,\"y\":839},{\"x\":89,\"y\":866},{\"x\":54,\"y\":866}],\"text\":\"-60\"},{\"boundingBox\":[{\"x\":199,\"y\":919},{\"x\":221,\"y\":918},{\"x\":219,\"y\":945},{\"x\":199,\"y\":947}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":375,\"y\":918},{\"x\":407,\"y\":917},{\"x\":407,\"y\":945},{\"x\":377,\"y\":945}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":553,\"y\":919},{\"x\":585,\"y\":918},{\"x\":586,\"y\":946},{\"x\":555,\"y\":948}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":732,\"y\":917},{\"x\":766,\"y\":917},{\"x\":765,\"y\":945},{\"x\":731,\"y\":945}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":912,\"y\":916},{\"x\":949,\"y\":917},{\"x\":946,\"y\":945},{\"x\":910,\"y\":945}],\"text\":\"25\"},{\"boundingBox\":[{\"x\":1092,\"y\":918},{\"x\":1126,\"y\":919},{\"x\":1125,\"y\":945},{\"x\":1092,\"y\":944}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":1269,\"y\":916},{\"x\":1304,\"y\":915},{\"x\":1303,\"y\":943},{\"x\":1269,\"y\":944}],\"text\":\"35\"},{\"boundingBox\":[{\"x\":1450,\"y\":917},{\"x\":1487,\"y\":918},{\"x\":1487,\"y\":943},{\"x\":1451,\"y\":943}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1632,\"y\":918},{\"x\":1668,\"y\":918},{\"x\":1667,\"y\":944},{\"x\":1632,\"y\":942}],\"text\":\"45\"},{\"boundingBox\":[{\"x\":1811,\"y\":920},{\"x\":1842,\"y\":920},{\"x\":1842,\"y\":941},{\"x\":1811,\"y\":942}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":842,\"y\":947},{\"x\":1031,\"y\":946},{\"x\":1031,\"y\":971},{\"x\":842,\"y\":972}],\"text\":\"Frequency (Hz)\"},{\"boundingBox\":[{\"x\":916,\"y\":1017},{\"x\":956,\"y\":1017},{\"x\":956,\"y\":1056},{\"x\":918,\"y\":1057}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":129,\"y\":1137},{\"x\":154,\"y\":1137},{\"x\":153,\"y\":1159},{\"x\":129,\"y\":1160}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":989,\"y\":1147},{\"x\":1085,\"y\":1148},{\"x\":1085,\"y\":1159},{\"x\":989,\"y\":1158}],\"text\":\"-----\"},{\"boundingBox\":[{\"x\":1077,\"y\":1221},{\"x\":1261,\"y\":1225},{\"x\":1261,\"y\":1246},{\"x\":1077,\"y\":1242}],\"text\":\"Servicntermin-Buchung\"},{\"boundingBox\":[{\"x\":818,\"y\":1258},{\"x\":834,\"y\":1257},{\"x\":833,\"y\":1278},{\"x\":818,\"y\":1280}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":1082,\"y\":1260},{\"x\":1099,\"y\":1260},{\"x\":1097,\"y\":1273},{\"x\":1081,\"y\":1274}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1480,\"y\":1263},{\"x\":1526,\"y\":1264},{\"x\":1526,\"y\":1271},{\"x\":1480,\"y\":1269}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":1613,\"y\":1261},{\"x\":1667,\"y\":1261},{\"x\":1667,\"y\":1276},{\"x\":1613,\"y\":1275}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":825,\"y\":1428},{\"x\":838,\"y\":1430},{\"x\":837,\"y\":1446},{\"x\":825,\"y\":1445}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":132,\"y\":1450},{\"x\":155,\"y\":1451},{\"x\":153,\"y\":1473},{\"x\":132,\"y\":1473}],\"text\":\"(b)\"},{\"boundingBox\":[{\"x\":818,\"y\":1633},{\"x\":832,\"y\":1631},{\"x\":832,\"y\":1647},{\"x\":818,\"y\":1649}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1093,\"y\":1681},{\"x\":1491,\"y\":1681},{\"x\":1491,\"y\":1720},{\"x\":1093,\"y\":1720}],\"text\":\"Intensity of the view: low\"},{\"boundingBox\":[{\"x\":1605,\"y\":1680},{\"x\":1678,\"y\":1680},{\"x\":1678,\"y\":1719},{\"x\":1605,\"y\":1718}],\"text\":\"high\"},{\"boundingBox\":[{\"x\":399,\"y\":1758},{\"x\":443,\"y\":1761},{\"x\":440,\"y\":1813},{\"x\":397,\"y\":1809}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1388,\"y\":1767},{\"x\":1431,\"y\":1768},{\"x\":1429,\"y\":1808},{\"x\":1388,\"y\":1807}],\"text\":\"C\"}],\"words\":[{\"boundingBox\":[{\"x\":1397,\"y\":0},{\"x\":1432,\"y\":3},{\"x\":1430,\"y\":25},{\"x\":1396,\"y\":21}],\"text\":\"36.0\"},{\"boundingBox\":[{\"x\":27,\"y\":4},{\"x\":45,\"y\":5},{\"x\":44,\"y\":37},{\"x\":26,\"y\":36}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":200,\"y\":4},{\"x\":223,\"y\":4},{\"x\":223,\"y\":23},{\"x\":200,\"y\":23}],\"text\":\"6.0\"},{\"boundingBox\":[{\"x\":500,\"y\":0},{\"x\":533,\"y\":3},{\"x\":531,\"y\":22},{\"x\":500,\"y\":19}],\"text\":\"10.0\"},{\"boundingBox\":[{\"x\":807,\"y\":1},{\"x\":833,\"y\":3},{\"x\":831,\"y\":23},{\"x\":806,\"y\":21}],\"text\":\"4.0\"},{\"boundingBox\":[{\"x\":1098,\"y\":2},{\"x\":1127,\"y\":2},{\"x\":1127,\"y\":25},{\"x\":1098,\"y\":25}],\"text\":\"22.0\"},{\"boundingBox\":[{\"x\":1688,\"y\":1},{\"x\":1717,\"y\":2},{\"x\":1717,\"y\":21},{\"x\":1688,\"y\":20}],\"text\":\"10.0\"},{\"boundingBox\":[{\"x\":1723,\"y\":2},{\"x\":1749,\"y\":3},{\"x\":1749,\"y\":21},{\"x\":1723,\"y\":21}],\"text\":\"Hz\"},{\"boundingBox\":[{\"x\":137,\"y\":347},{\"x\":137,\"y\":376},{\"x\":92,\"y\":376},{\"x\":92,\"y\":347}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":62,\"y\":345},{\"x\":89,\"y\":345},{\"x\":88,\"y\":376},{\"x\":62,\"y\":375}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":61,\"y\":409},{\"x\":90,\"y\":412},{\"x\":87,\"y\":441},{\"x\":59,\"y\":438}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":64,\"y\":533},{\"x\":91,\"y\":534},{\"x\":89,\"y\":563},{\"x\":62,\"y\":561}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":58,\"y\":594},{\"x\":91,\"y\":596},{\"x\":89,\"y\":625},{\"x\":56,\"y\":622}],\"text\":\"-20\"},{\"boundingBox\":[{\"x\":52,\"y\":656},{\"x\":91,\"y\":656},{\"x\":91,\"y\":686},{\"x\":52,\"y\":686}],\"text\":\"-30\"},{\"boundingBox\":[{\"x\":54,\"y\":721},{\"x\":90,\"y\":721},{\"x\":89,\"y\":746},{\"x\":54,\"y\":745}],\"text\":\"-40\"},{\"boundingBox\":[{\"x\":8,\"y\":775},{\"x\":9,\"y\":702},{\"x\":37,\"y\":701},{\"x\":33,\"y\":775}],\"text\":\"Power\"},{\"boundingBox\":[{\"x\":9,\"y\":695},{\"x\":9,\"y\":631},{\"x\":39,\"y\":630},{\"x\":37,\"y\":695}],\"text\":\"10*log\"},{\"boundingBox\":[{\"x\":8,\"y\":625},{\"x\":8,\"y\":621},{\"x\":39,\"y\":620},{\"x\":39,\"y\":624}],\"text\":\".\"},{\"boundingBox\":[{\"x\":8,\"y\":615},{\"x\":5,\"y\":496},{\"x\":39,\"y\":495},{\"x\":39,\"y\":614}],\"text\":\"0(/V/Hz)\"},{\"boundingBox\":[{\"x\":53,\"y\":779},{\"x\":92,\"y\":780},{\"x\":92,\"y\":812},{\"x\":53,\"y\":811}],\"text\":\"-50\"},{\"boundingBox\":[{\"x\":54,\"y\":839},{\"x\":89,\"y\":839},{\"x\":89,\"y\":866},{\"x\":54,\"y\":866}],\"text\":\"-60\"},{\"boundingBox\":[{\"x\":199,\"y\":919},{\"x\":214,\"y\":918},{\"x\":216,\"y\":945},{\"x\":200,\"y\":946}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":375,\"y\":917},{\"x\":402,\"y\":917},{\"x\":402,\"y\":945},{\"x\":375,\"y\":945}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":554,\"y\":919},{\"x\":581,\"y\":918},{\"x\":582,\"y\":947},{\"x\":555,\"y\":948}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":734,\"y\":917},{\"x\":764,\"y\":917},{\"x\":764,\"y\":945},{\"x\":734,\"y\":945}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":913,\"y\":916},{\"x\":944,\"y\":916},{\"x\":943,\"y\":945},{\"x\":913,\"y\":944}],\"text\":\"25\"},{\"boundingBox\":[{\"x\":1095,\"y\":918},{\"x\":1122,\"y\":919},{\"x\":1122,\"y\":945},{\"x\":1094,\"y\":944}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":1271,\"y\":916},{\"x\":1301,\"y\":915},{\"x\":1302,\"y\":943},{\"x\":1272,\"y\":944}],\"text\":\"35\"},{\"boundingBox\":[{\"x\":1451,\"y\":917},{\"x\":1480,\"y\":917},{\"x\":1480,\"y\":943},{\"x\":1451,\"y\":942}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1633,\"y\":918},{\"x\":1662,\"y\":918},{\"x\":1662,\"y\":944},{\"x\":1633,\"y\":943}],\"text\":\"45\"},{\"boundingBox\":[{\"x\":1812,\"y\":920},{\"x\":1839,\"y\":920},{\"x\":1840,\"y\":942},{\"x\":1812,\"y\":942}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":844,\"y\":947},{\"x\":970,\"y\":947},{\"x\":970,\"y\":972},{\"x\":844,\"y\":972}],\"text\":\"Frequency\"},{\"boundingBox\":[{\"x\":975,\"y\":947},{\"x\":1031,\"y\":946},{\"x\":1031,\"y\":972},{\"x\":976,\"y\":972}],\"text\":\"(Hz)\"},{\"boundingBox\":[{\"x\":917,\"y\":1017},{\"x\":943,\"y\":1017},{\"x\":944,\"y\":1057},{\"x\":918,\"y\":1057}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":129,\"y\":1137},{\"x\":153,\"y\":1137},{\"x\":153,\"y\":1159},{\"x\":129,\"y\":1160}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":990,\"y\":1148},{\"x\":1071,\"y\":1148},{\"x\":1070,\"y\":1159},{\"x\":989,\"y\":1157}],\"text\":\"-----\"},{\"boundingBox\":[{\"x\":1077,\"y\":1221},{\"x\":1262,\"y\":1225},{\"x\":1261,\"y\":1246},{\"x\":1077,\"y\":1243}],\"text\":\"Servicntermin-Buchung\"},{\"boundingBox\":[{\"x\":818,\"y\":1258},{\"x\":828,\"y\":1257},{\"x\":830,\"y\":1278},{\"x\":818,\"y\":1279}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":1082,\"y\":1260},{\"x\":1090,\"y\":1260},{\"x\":1091,\"y\":1274},{\"x\":1083,\"y\":1274}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1481,\"y\":1263},{\"x\":1508,\"y\":1264},{\"x\":1508,\"y\":1270},{\"x\":1481,\"y\":1270}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":1636,\"y\":1262},{\"x\":1644,\"y\":1262},{\"x\":1643,\"y\":1276},{\"x\":1636,\"y\":1275}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":827,\"y\":1428},{\"x\":837,\"y\":1429},{\"x\":835,\"y\":1446},{\"x\":825,\"y\":1444}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":132,\"y\":1450},{\"x\":155,\"y\":1450},{\"x\":154,\"y\":1473},{\"x\":132,\"y\":1472}],\"text\":\"(b)\"},{\"boundingBox\":[{\"x\":818,\"y\":1633},{\"x\":827,\"y\":1632},{\"x\":829,\"y\":1647},{\"x\":820,\"y\":1648}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1095,\"y\":1682},{\"x\":1223,\"y\":1683},{\"x\":1222,\"y\":1721},{\"x\":1094,\"y\":1721}],\"text\":\"Intensity\"},{\"boundingBox\":[{\"x\":1231,\"y\":1683},{\"x\":1263,\"y\":1683},{\"x\":1261,\"y\":1721},{\"x\":1229,\"y\":1721}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1270,\"y\":1683},{\"x\":1325,\"y\":1684},{\"x\":1323,\"y\":1721},{\"x\":1269,\"y\":1721}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":1332,\"y\":1684},{\"x\":1419,\"y\":1684},{\"x\":1417,\"y\":1721},{\"x\":1330,\"y\":1721}],\"text\":\"view:\"},{\"boundingBox\":[{\"x\":1426,\"y\":1684},{\"x\":1477,\"y\":1684},{\"x\":1476,\"y\":1721},{\"x\":1424,\"y\":1721}],\"text\":\"low\"},{\"boundingBox\":[{\"x\":1607,\"y\":1680},{\"x\":1676,\"y\":1680},{\"x\":1676,\"y\":1719},{\"x\":1607,\"y\":1719}],\"text\":\"high\"},{\"boundingBox\":[{\"x\":399,\"y\":1758},{\"x\":428,\"y\":1760},{\"x\":424,\"y\":1812},{\"x\":397,\"y\":1809}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1388,\"y\":1767},{\"x\":1411,\"y\":1767},{\"x\":1410,\"y\":1807},{\"x\":1388,\"y\":1807}],\"text\":\"C\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 21 September 2020\",\"lines\":[{\"boundingBox\":[{\"x\":4,\"y\":15},{\"x\":1066,\"y\":15},{\"x\":1066,\"y\":74},{\"x\":4,\"y\":72}],\"text\":\"Published online: 21 September 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":5,\"y\":16},{\"x\":270,\"y\":15},{\"x\":271,\"y\":72},{\"x\":6,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":290,\"y\":15},{\"x\":494,\"y\":15},{\"x\":494,\"y\":74},{\"x\":291,\"y\":73}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":505,\"y\":15},{\"x\":579,\"y\":15},{\"x\":579,\"y\":74},{\"x\":505,\"y\":74}],\"text\":\"21\"},{\"boundingBox\":[{\"x\":595,\"y\":15},{\"x\":907,\"y\":16},{\"x\":906,\"y\":73},{\"x\":594,\"y\":74}],\"text\":\"September\"},{\"boundingBox\":[{\"x\":918,\"y\":16},{\"x\":1058,\"y\":16},{\"x\":1057,\"y\":71},{\"x\":917,\"y\":73}],\"text\":\"2020\"}]}"
      ]
    },
    {
      "@search.score": 1.4910595,
      "content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\n\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\n\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A r",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNDkzLTAxNS0wMDE5LXoucGRm0",
      "metadata_author": "Partheeban Chandrasekaran",
      "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis",
      "people": [
        "Chandrasekaran",
        "Esfandiari",
        "Partheeban Chandrasekaran",
        "Babak Esfandiari",
        "Aberer",
        "Despotovic",
        "Abdul-Rahman",
        "Marsh",
        "Tij",
        "Guha",
        "Hazard",
        "Singh"
      ],
      "keyphrases": [
        "new classi- fication scheme",
        "peer file-sharing applications",
        "social networking websites",
        "comprehensive test tool",
        "Attribution 4.0 International License",
        "original author(s",
        "three trust algorithms",
        "various trust properties",
        "computational trust mechanisms",
        "online community-based systems",
        "Creative Commons license",
        "computational trust models",
        "social trust models",
        "three reputation systems",
        "different trust models",
        "RESEARCH Open Access",
        "trust assessment workflow",
        "existing reputation systems",
        "existing models",
        "Many models",
        "different stages",
        "Trust Management",
        "trust relationship",
        "Multi-agent systems",
        "Computer Engineering",
        "Introduction Motivation",
        "simple averaging",
        "based scores",
        "Advogato website",
        "application-dependent metrics",
        "transformation stage",
        "unrestricted use",
        "appropriate credit",
        "first contribution",
        "second contribution",
        "evaluation schemes",
        "Trust testbed",
        "Esfandiari Journal",
        "latest model",
        "single model",
        "generic testbed",
        "increasing need",
        "past transactions",
        "graph transformations",
        "Carleton University",
        "larger number",
        "Partheeban Chandrasekaran",
        "slandering attacks",
        "two agents",
        "Babak Esfandiari",
        "DOI",
        "experiments",
        "analysis",
        "Correspondence",
        "Department",
        "1125 Colonel",
        "Drive",
        "Ottawa",
        "Ontario",
        "Canada",
        "Abstract",
        "tesbed",
        "flexibility",
        "design",
        "prototype",
        "EigenTrust",
        "PeerTrust",
        "Appleseed",
        "vulnerabilites",
        "compliance",
        "example",
        "discrepancies",
        "trade-offs",
        "resistance",
        "self-promotion",
        "Keywords",
        "growth",
        "users",
        "history",
        "interactions",
        "calculations",
        "ratings",
        "eBay",
        "researcher",
        "tools",
        "evaluations",
        "set",
        "vulnerabilities",
        "types",
        "Overview",
        "solution",
        "contributions",
        "paper",
        "family",
        "process",
        "succession",
        "vertices",
        "meaning",
        "edges",
        "article",
        "terms",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "org",
        "mailto",
        "Page",
        "presence",
        "existence",
        "variety",
        "certificate authority-based PKI trust models",
        "Trust management systems aid agents",
        "reputation-based trust management systems",
        "three different trust algorithms",
        "public key infras",
        "Social trust models",
        "basic trust properties",
        "social trust algorithms",
        "existing trust algorithms",
        "Beta Reputation System",
        "three systems",
        "reputation systems",
        "chaining algorithms",
        "mutual trust",
        "trust assertions",
        "trust score",
        "existing testbeds",
        "subtle interplay",
        "higher sensitivity",
        "lower sensitivity",
        "literature review",
        "implementation details",
        "actual mechanisms",
        "earlier direct",
        "indirect interactions",
        "e-commerce website",
        "third party",
        "Various inputs",
        "file-sharing network",
        "blogging website",
        "particular level",
        "specific context",
        "simple attacks",
        "self-promoting attacks",
        "Problem description",
        "research problem",
        "particular agent",
        "one attack",
        "friend chain",
        "past experiences",
        "testbed prototype",
        "aggregated trustworthiness",
        "evaluation results",
        "range",
        "differences",
        "way",
        "slandering",
        "Organization",
        "section",
        "Background",
        "state",
        "discussion",
        "Conclusions",
        "limitations",
        "tructures",
        "certificates",
        "The",
        "means",
        "order",
        "instance",
        "sellers",
        "trustworthy",
        "product",
        "drawbacks",
        "Chandrasekaran",
        "root",
        "recommendations",
        "turn",
        "recommenders",
        "Nature",
        "TRAVOS",
        "BRS",
        "satisfaction",
        "transaction",
        "P2P",
        "Aberer",
        "Despotovic",
        "plaints",
        "Advogato",
        "goal",
        "spam",
        "community",
        "TidalTrust",
        "beta probability density function",
        "late indirect trust scores",
        "other agents’ past experiences",
        "Spreading Activation model",
        "local trust scores",
        "global trust algorithms",
        "global trust score",
        "local trust algorithms",
        "three algorithms",
        "general trust",
        "Eigen- Trust",
        "Trust decisions",
        "similarity score",
        "file authenticity",
        "recommenda- tions",
        "different inputs",
        "thresholding techniques",
        "edge weights",
        "maximum “flow",
        "direct experience",
        "party recommendations",
        "different families",
        "next sections",
        "detailed descriptions",
        "respective sections",
        "satisfaction ratings",
        "trust graph",
        "direct interactions",
        "truster agent",
        "Credence",
        "votes",
        "files",
        "trustee",
        "gossiping",
        "information",
        "perspective",
        "survey",
        "Marsh",
        "Abdul-Rahman",
        "case",
        "percentage",
        "top",
        "value",
        "energy",
        "tor",
        "short",
        "third",
        "one",
        "scope",
        "testbed",
        "details",
        "output",
        "readers",
        "high local trust values",
        "low local trust values",
        "local direct trust score",
        "global trust score computations",
        "local trust score",
        "current trust scores",
        "trans- action ratings",
        "local trust vector",
        "global trust scores",
        "fellow malicious agents",
        "global value",
        "trust relationships",
        "trust propagation",
        "trust graphs",
        "flow-based algorithm",
        "path length",
        "termination condition",
        "trust seed",
        "weighted graph",
        "cij elements",
        "factor decay",
        "gence threshold",
        "decay factor",
        "honest agents",
        "agent u",
        "agent j",
        "satisfaction rating",
        "transaction rating",
        "raters",
        "Eq.",
        "transactions",
        "system",
        "method",
        "input",
        "tij",
        "sij",
        "sum",
        "truster",
        "friends",
        "matrix",
        "opinion",
        "number",
        "hops",
        "i.",
        "rank",
        "nodes",
        "trustworthiness",
        "sink",
        "amount",
        "places",
        "loops",
        "Ai",
        "step",
        "Tc",
        "∑",
        "∈",
        "open-source message- driven simulation engine",
        "two different targets",
        "two testbed implementations",
        "document recommendation systems",
        "beta probability distribution",
        "local trust algorithm",
        "reputation estimation errors",
        "two testbed models",
        "general-purpose trust models",
        "previous reputation value",
        "art painting sales",
        "general trust systems",
        "The Agent Reputation",
        "beta distribution",
        "two roles",
        "initial value",
        "real systems",
        "trust assessment",
        "brief survey",
        "important role",
        "case studies",
        "direct feedbacks",
        "current reputation",
        "single fixpoint",
        "important stage",
        "one trustworthiness",
        "dom variables",
        "various attacks",
        "particular era",
        "market values",
        "specific era",
        "other eras",
        "reputation scores",
        "trust algorithms",
        "next section",
        "rating function",
        "desirable properties",
        "negative experiences",
        "mutual ratings",
        "target pairing",
        "other agents",
        "Guha Guha",
        "seed",
        "methods",
        "look",
        "investigation",
        "graph",
        "documents",
        "many",
        "specialization",
        "subclass",
        "Macau",
        "Hazard",
        "Singh",
        "authors",
        "rater",
        "favor",
        "payoff",
        "belief",
        "likelihood",
        "future",
        "definitions",
        "Monotonicity",
        "computed",
        "b.",
        "Unambiguity",
        "convergence",
        "time",
        "Accuracy",
        "total",
        "update",
        "positive",
        "comparison",
        "resilience",
        "performance",
        "domain",
        "client",
        "paintings",
        "appraisals",
        "expert",
        "dedicated distributed infras- tructure",
        "online product review websites",
        "trust score rep- resentation",
        "graphical user interface",
        "first person point",
        "P2P file-sharing networks",
        "honest sales ratio",
        "trust calculation schemes",
        "decentralized trust algorithms",
        "highest bank balance",
        "Value Imbalance attack",
        "general marketplace scenario",
        "various reputation systems",
        "Reputation Lag attack",
        "honest profit ratio",
        "multiple buyer accounts",
        "Multiple seller accounts",
        "The ART testbed",
        "distributed version",
        "utility value",
        "Proliferation attack",
        "cheater sales",
        "other systems",
        "marketplace domain",
        "Reputation Experimentation",
        "trust mechanism",
        "trust management",
        "The Trust",
        "future interactions",
        "possible messages",
        "time interval",
        "The engine",
        "new clients",
        "correct ability",
        "problem domain",
        "Evaluation Testbed",
        "varying prices",
        "inexpensive items",
        "market competition",
        "selling price",
        "future transactions",
        "following metrics",
        "evaluation metrics",
        "simulation engine",
        "simulation days",
        "trustworthy agents",
        "right agents",
        "participating agents",
        "different agents",
        "centralized version",
        "sale price",
        "random number",
        "collusion attacks",
        "winning agent",
        "1,000 different products",
        "departing agent",
        "protocol",
        "Themessages",
        "track",
        "results",
        "database",
        "GUI",
        "runtime",
        "platform",
        "service",
        "cooperation",
        "additional",
        "model",
        "TREET",
        "buyers",
        "influence",
        "cost",
        "purchase",
        "probability",
        "initialization",
        "offers",
        "feedback",
        "experience",
        "White-Washing",
        "Self-Promoting",
        "limitation",
        "restriction",
        "others",
        "knowledge",
        "100",
        "many trust management systems",
        "obtain feedback history graph",
        "discrete event simulator",
        "generic evaluation metrics",
        "particular trust algorithm",
        "A Reputation Graph",
        "trust assessment process",
        "new systems",
        "decision-making process",
        "evaluation framework",
        "labelled graph",
        "agent A",
        "Summary Trust",
        "social trust",
        "trust properties",
        "nth-hand trust",
        "application domain",
        "rational decisions",
        "abstraction layer",
        "n feedbacks",
        "ith satisfaction",
        "different ranges",
        "labelled arcs",
        "other things",
        "next step",
        "transitive closure",
        "target agent",
        "agent B",
        "indirect feedback",
        "feedback histories",
        "trust models",
        "following stages",
        "past behavior",
        "R N",
        "common set",
        "final stage",
        "abstract model",
        "feedback value",
        "output requirements",
        "R.",
        "tool",
        "manymodels",
        "attacks",
        "developers",
        "existing",
        "foundation",
        "expectation",
        "individual",
        "observations",
        "estimation",
        "combination",
        "rest",
        "graphs",
        "group",
        "actions",
        "list",
        "FHG",
        "downloader",
        "uploader",
        "values",
        "timestamps",
        "directed",
        "second",
        "E.",
        "∅",
        "single reputa- tion score",
        "absolute global reputation score",
        "new classification scheme",
        "continuous reputation values",
        "global reputation scores",
        "existing classification criteria",
        "stage transi- tions",
        "top k agents",
        "local reputation scores",
        "obtain trust graph",
        "Global algorithms",
        "existing literature",
        "one scores",
        "Reputation algorithms",
        "particular task",
        "two groups",
        "Two methods",
        "above sections",
        "other hand",
        "same codomain",
        "other words",
        "reputation graph",
        "various algorithms",
        "different algorithms",
        "edge labels",
        "local algorithms",
        "other method",
        "intermediate graph",
        "reflexive property",
        "incoming arcs",
        "among agents",
        "ranking algorithms",
        "One method",
        "trusting agent",
        "EigenTrust outputs",
        "one stage",
        "looping",
        "degree",
        "Figs",
        "weights",
        "Fig.",
        "Examples",
        "decision",
        "TG",
        "stages",
        "workflow",
        "Classifying",
        "nisms",
        "threshold",
        "transitions",
        "addition",
        "Table",
        "semantics",
        "relative",
        "Trust Algorithm Transitions Input",
        "satisfaction local absoluteratings Ranking",
        "continuous feedback values",
        "trust models Stage",
        "graph transformation function",
        "Stage transitions",
        "global absoluteratings",
        "local absolutescores",
        "Trust algorithms",
        "functional view",
        "evaluation mechanisms",
        "global relativeratings",
        "N/A r",
        "Reputation Scores",
        "experimenter",
        "inputs",
        "outputs",
        "classification",
        "AppleSeed",
        "3 complaints",
        "3 certificates"
      ],
      "merged_content": "\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\n  \n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n R b R b 0.7 0.8 0.8 1.0 0.2 0.2 0.2 0.8 a 0.6 0.4 C a C 1.0 1.0 a b \n\n G R b (0.7, 0.8, 0.9) b 2 (1.0, 1.0) 0.8 1.0 C a C a 0.7 1a 1b b 0.8 1.0 a C \n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n Start End Feedback history Trust Graph Stage 1: Obtain FHG Stage 3: Obtain TG Preconditions: Preconditions: {fı(a, b), f2(a, b), ..., fn (a, b)} Ax A - R Post-conditions: Post-conditions: Ax A - RR\" A x A -+ [0, 1] Stage 2: Obtain RG Feedback Preconditions: History Ax A - R\" Reputation Graph Post-conditions: Graph Ax A - R \n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management  (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A r",
      "text": [
        "",
        "R b R b 0.7 0.8 0.8 1.0 0.2 0.2 0.2 0.8 a 0.6 0.4 C a C 1.0 1.0 a b",
        "G R b (0.7, 0.8, 0.9) b 2 (1.0, 1.0) 0.8 1.0 C a C a 0.7 1a 1b b 0.8 1.0 a C",
        "Start End Feedback history Trust Graph Stage 1: Obtain FHG Stage 3: Obtain TG Preconditions: Preconditions: {fı(a, b), f2(a, b), ..., fn (a, b)} Ax A - R Post-conditions: Post-conditions: Ax A - RR\" A x A -+ [0, 1] Stage 2: Obtain RG Feedback Preconditions: History Ax A - R\" Reputation Graph Post-conditions: Graph Ax A - R",
        "Stage transitions Feedback History Output: A x A => [0, 1] Input: A x A => {-1, 1]* FHG RG TG fully connected global relative rep. scores Input: A x A => [0, 1]* Ouput: A x A => [0, 1] Feedback History FHG RG TG fully connected global absolute rep. scores Input: A x A => [0, 1] Output: Ax A => [0, 1] local Feedback History FHG RG TG partially connected local absolute rep.scores Appleseed | PeerTrust | EigenTrust Input: A x A => {-1, 1]* Feedback History FHG DO TG Output: Ax A => {0, 1} fully connected global Aberer & Despotovic Input: Ax A => {0, 1} Feedback History Ouput: Ax A => {0, 1} FHG RG TG partially connected, local Input: A x A => {0, 1]* Feedback History FHG RG TG Ouput: Ax A => [0, 1] local absolute rep. scores TRAVOS |Advogato Input: A x A => [0, 1] relative rep. scores Feedback History FHG RG TG Output: A x A => {0, 1} partially connected Input: A x A => [0, 1] absolute rep. scores Feedback History FHG RG TG Output: A x A => {0, 1} partially connected Thresholding Ranking",
        "FHGO PeerTrust Preconditions: A x A + [0,1]* Post-conditions: RG2 Discretizer Fully connected, global, Preconditions: A x A + [0, 1]* absolute r(a,b) A x A ++ [0, 1] Post-conditions Normalizer Ax A ++ {-1,1}* Preconditions: A x A ++ [0, 1] Post-conditions: Fully connected, global, FHG1 relative r(a, b) A x A ++ [0, 1] EigenTrust Preconditions: RG3 Ax A ++ {-1,1}* RG1 Post-conditions: Fully connected, global, relative r(a, b) Spearman A x A ++ [0, 1] Correlation coefficient",
        "0 0 {1.0, 1.0, 1.0} 1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} (1.0, 1.0, 0.0} 2 2 [0.0, 0.0, 0.0} 10.0, 0.0, 0.0} 3 3",
        "0 0 0 0.22 (1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 0.3 0.22 1 1 1 0.3 0.22 0.12 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} K1.0, 1.0, 1.0} 0.3 0.12 0.36 0.22 2 2 0.36 0.3 3 0.12 K0.0, 0.0, 0.0} {0.0, 0.0, 0.0} 0.36 /0.12 3 3 2 00.36",
        "0 0 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} K0.0, 0.0, 0.0} 2 2 (0.0, 0.0, 0.0} {0.0, 0.0, 0.0} 3 3",
        "0 0 (1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 2 2 [0.0, 0.0, 0.0} {0.0, 0.0, 0.0}[0.0, 0.0. 0.0} 3 3",
        "1.0 1.0 2 0.0 1.0 3",
        "0 1.0 1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 6 4 3 11 5 12 9 13 8 10",
        "1.0 1 1.0 1.0 2 3",
        "? 1 2 3",
        "0 1.0 1.0 1 4 1.0 1.0 1.0 2 3",
        "0 1 2 3 5",
        "1.0 1 1.0 2 1.0 1.0 3",
        "0 1 2 3",
        "4 2 r 1.5 1 0.5 Feedback value,f & Reputation,r 0 2 3 4 15 1 Feedback, i",
        "Published online: 07 September 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"R b R b 0.7 0.8 0.8 1.0 0.2 0.2 0.2 0.8 a 0.6 0.4 C a C 1.0 1.0 a b\",\"lines\":[{\"boundingBox\":[{\"x\":135,\"y\":88},{\"x\":165,\"y\":87},{\"x\":166,\"y\":116},{\"x\":135,\"y\":117}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":272,\"y\":110},{\"x\":299,\"y\":109},{\"x\":299,\"y\":135},{\"x\":274,\"y\":135}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":720,\"y\":88},{\"x\":747,\"y\":87},{\"x\":749,\"y\":115},{\"x\":721,\"y\":116}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":860,\"y\":100},{\"x\":883,\"y\":101},{\"x\":883,\"y\":130},{\"x\":861,\"y\":129}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":417,\"y\":138},{\"x\":466,\"y\":142},{\"x\":466,\"y\":170},{\"x\":415,\"y\":168}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":109,\"y\":170},{\"x\":157,\"y\":169},{\"x\":157,\"y\":198},{\"x\":110,\"y\":197}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":723,\"y\":148},{\"x\":770,\"y\":147},{\"x\":771,\"y\":174},{\"x\":723,\"y\":176}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":1005,\"y\":139},{\"x\":1050,\"y\":141},{\"x\":1050,\"y\":169},{\"x\":1004,\"y\":168}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":204,\"y\":206},{\"x\":252,\"y\":205},{\"x\":251,\"y\":234},{\"x\":202,\"y\":233}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":779,\"y\":201},{\"x\":831,\"y\":201},{\"x\":830,\"y\":232},{\"x\":779,\"y\":232}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":863,\"y\":245},{\"x\":912,\"y\":245},{\"x\":912,\"y\":273},{\"x\":862,\"y\":274}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":926,\"y\":220},{\"x\":975,\"y\":220},{\"x\":975,\"y\":248},{\"x\":926,\"y\":249}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":96,\"y\":284},{\"x\":115,\"y\":283},{\"x\":117,\"y\":306},{\"x\":98,\"y\":306}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":268,\"y\":269},{\"x\":389,\"y\":260},{\"x\":391,\"y\":292},{\"x\":269,\"y\":301}],\"text\":\"0.6 0.4\"},{\"boundingBox\":[{\"x\":471,\"y\":284},{\"x\":494,\"y\":283},{\"x\":494,\"y\":307},{\"x\":471,\"y\":307}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":684,\"y\":284},{\"x\":701,\"y\":283},{\"x\":703,\"y\":307},{\"x\":686,\"y\":307}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1065,\"y\":283},{\"x\":1086,\"y\":284},{\"x\":1086,\"y\":308},{\"x\":1065,\"y\":308}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":254,\"y\":395},{\"x\":301,\"y\":395},{\"x\":301,\"y\":424},{\"x\":253,\"y\":424}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":863,\"y\":370},{\"x\":912,\"y\":370},{\"x\":912,\"y\":398},{\"x\":863,\"y\":397}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":295,\"y\":508},{\"x\":327,\"y\":506},{\"x\":327,\"y\":539},{\"x\":295,\"y\":541}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":845,\"y\":495},{\"x\":881,\"y\":494},{\"x\":883,\"y\":542},{\"x\":847,\"y\":543}],\"text\":\"b\"}],\"words\":[{\"boundingBox\":[{\"x\":135,\"y\":87},{\"x\":153,\"y\":87},{\"x\":154,\"y\":116},{\"x\":136,\"y\":117}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":275,\"y\":109},{\"x\":290,\"y\":109},{\"x\":290,\"y\":135},{\"x\":275,\"y\":135}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":720,\"y\":87},{\"x\":738,\"y\":87},{\"x\":739,\"y\":115},{\"x\":721,\"y\":116}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":861,\"y\":100},{\"x\":878,\"y\":101},{\"x\":877,\"y\":130},{\"x\":860,\"y\":129}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":416,\"y\":138},{\"x\":463,\"y\":141},{\"x\":461,\"y\":171},{\"x\":415,\"y\":168}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":109,\"y\":169},{\"x\":152,\"y\":169},{\"x\":152,\"y\":198},{\"x\":109,\"y\":198}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":723,\"y\":148},{\"x\":766,\"y\":147},{\"x\":767,\"y\":175},{\"x\":724,\"y\":176}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":1004,\"y\":139},{\"x\":1046,\"y\":140},{\"x\":1045,\"y\":169},{\"x\":1004,\"y\":167}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":202,\"y\":205},{\"x\":245,\"y\":205},{\"x\":245,\"y\":234},{\"x\":202,\"y\":234}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":779,\"y\":201},{\"x\":828,\"y\":201},{\"x\":828,\"y\":232},{\"x\":779,\"y\":232}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":862,\"y\":245},{\"x\":907,\"y\":245},{\"x\":907,\"y\":274},{\"x\":862,\"y\":274}],\"text\":\"0.2\"},{\"boundingBox\":[{\"x\":926,\"y\":220},{\"x\":971,\"y\":220},{\"x\":971,\"y\":249},{\"x\":926,\"y\":249}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":97,\"y\":283},{\"x\":110,\"y\":283},{\"x\":111,\"y\":306},{\"x\":97,\"y\":306}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":268,\"y\":270},{\"x\":313,\"y\":266},{\"x\":314,\"y\":298},{\"x\":270,\"y\":301}],\"text\":\"0.6\"},{\"boundingBox\":[{\"x\":337,\"y\":265},{\"x\":386,\"y\":261},{\"x\":387,\"y\":293},{\"x\":338,\"y\":296}],\"text\":\"0.4\"},{\"boundingBox\":[{\"x\":472,\"y\":283},{\"x\":486,\"y\":283},{\"x\":486,\"y\":307},{\"x\":472,\"y\":307}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":684,\"y\":283},{\"x\":698,\"y\":283},{\"x\":699,\"y\":307},{\"x\":685,\"y\":307}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1066,\"y\":283},{\"x\":1081,\"y\":283},{\"x\":1080,\"y\":308},{\"x\":1066,\"y\":307}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":253,\"y\":395},{\"x\":295,\"y\":395},{\"x\":295,\"y\":424},{\"x\":253,\"y\":424}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":864,\"y\":370},{\"x\":905,\"y\":370},{\"x\":905,\"y\":398},{\"x\":864,\"y\":397}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":297,\"y\":508},{\"x\":315,\"y\":507},{\"x\":317,\"y\":539},{\"x\":299,\"y\":540}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":845,\"y\":495},{\"x\":875,\"y\":494},{\"x\":876,\"y\":542},{\"x\":846,\"y\":543}],\"text\":\"b\"}]}",
        "{\"language\":\"en\",\"text\":\"G R b (0.7, 0.8, 0.9) b 2 (1.0, 1.0) 0.8 1.0 C a C a 0.7 1a 1b b 0.8 1.0 a C\",\"lines\":[{\"boundingBox\":[{\"x\":307,\"y\":48},{\"x\":331,\"y\":48},{\"x\":333,\"y\":77},{\"x\":309,\"y\":77}],\"text\":\"G\"},{\"boundingBox\":[{\"x\":1035,\"y\":48},{\"x\":1056,\"y\":48},{\"x\":1057,\"y\":76},{\"x\":1036,\"y\":75}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":1175,\"y\":71},{\"x\":1194,\"y\":72},{\"x\":1194,\"y\":95},{\"x\":1175,\"y\":94}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":28,\"y\":96},{\"x\":251,\"y\":94},{\"x\":252,\"y\":129},{\"x\":28,\"y\":131}],\"text\":\"(0.7, 0.8, 0.9) b\"},{\"boundingBox\":[{\"x\":695,\"y\":90},{\"x\":715,\"y\":88},{\"x\":716,\"y\":116},{\"x\":697,\"y\":117}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":311,\"y\":126},{\"x\":442,\"y\":128},{\"x\":441,\"y\":161},{\"x\":310,\"y\":160}],\"text\":\"(1.0, 1.0)\"},{\"boundingBox\":[{\"x\":1011,\"y\":122},{\"x\":1055,\"y\":122},{\"x\":1055,\"y\":148},{\"x\":1011,\"y\":147}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":1316,\"y\":123},{\"x\":1361,\"y\":122},{\"x\":1362,\"y\":147},{\"x\":1317,\"y\":149}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":370,\"y\":228},{\"x\":389,\"y\":229},{\"x\":388,\"y\":252},{\"x\":369,\"y\":251}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1031,\"y\":209},{\"x\":1049,\"y\":209},{\"x\":1051,\"y\":231},{\"x\":1032,\"y\":231}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1311,\"y\":208},{\"x\":1330,\"y\":208},{\"x\":1332,\"y\":230},{\"x\":1313,\"y\":231}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":85,\"y\":231},{\"x\":101,\"y\":231},{\"x\":102,\"y\":250},{\"x\":85,\"y\":250}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1163,\"y\":291},{\"x\":1207,\"y\":291},{\"x\":1208,\"y\":318},{\"x\":1162,\"y\":317}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":489,\"y\":322},{\"x\":524,\"y\":322},{\"x\":524,\"y\":348},{\"x\":489,\"y\":346}],\"text\":\"1a\"},{\"boundingBox\":[{\"x\":910,\"y\":322},{\"x\":941,\"y\":320},{\"x\":944,\"y\":345},{\"x\":911,\"y\":347}],\"text\":\"1b\"},{\"boundingBox\":[{\"x\":692,\"y\":372},{\"x\":708,\"y\":374},{\"x\":708,\"y\":398},{\"x\":691,\"y\":396}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":573,\"y\":418},{\"x\":619,\"y\":420},{\"x\":618,\"y\":447},{\"x\":572,\"y\":445}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":783,\"y\":417},{\"x\":829,\"y\":412},{\"x\":834,\"y\":443},{\"x\":786,\"y\":448}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":580,\"y\":505},{\"x\":614,\"y\":502},{\"x\":616,\"y\":538},{\"x\":582,\"y\":540}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":797,\"y\":505},{\"x\":832,\"y\":504},{\"x\":833,\"y\":535},{\"x\":800,\"y\":534}],\"text\":\"C\"}],\"words\":[{\"boundingBox\":[{\"x\":307,\"y\":48},{\"x\":325,\"y\":48},{\"x\":325,\"y\":77},{\"x\":307,\"y\":77}],\"text\":\"G\"},{\"boundingBox\":[{\"x\":1035,\"y\":48},{\"x\":1051,\"y\":48},{\"x\":1050,\"y\":76},{\"x\":1035,\"y\":75}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":1176,\"y\":71},{\"x\":1190,\"y\":72},{\"x\":1188,\"y\":95},{\"x\":1175,\"y\":94}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":29,\"y\":96},{\"x\":85,\"y\":96},{\"x\":84,\"y\":131},{\"x\":28,\"y\":131}],\"text\":\"(0.7,\"},{\"boundingBox\":[{\"x\":92,\"y\":96},{\"x\":146,\"y\":96},{\"x\":145,\"y\":130},{\"x\":91,\"y\":131}],\"text\":\"0.8,\"},{\"boundingBox\":[{\"x\":153,\"y\":96},{\"x\":218,\"y\":96},{\"x\":217,\"y\":130},{\"x\":152,\"y\":130}],\"text\":\"0.9)\"},{\"boundingBox\":[{\"x\":228,\"y\":95},{\"x\":247,\"y\":95},{\"x\":246,\"y\":130},{\"x\":227,\"y\":130}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":695,\"y\":89},{\"x\":712,\"y\":88},{\"x\":715,\"y\":116},{\"x\":697,\"y\":117}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":314,\"y\":127},{\"x\":380,\"y\":127},{\"x\":379,\"y\":161},{\"x\":312,\"y\":160}],\"text\":\"(1.0,\"},{\"boundingBox\":[{\"x\":387,\"y\":128},{\"x\":442,\"y\":129},{\"x\":441,\"y\":162},{\"x\":385,\"y\":161}],\"text\":\"1.0)\"},{\"boundingBox\":[{\"x\":1011,\"y\":122},{\"x\":1050,\"y\":122},{\"x\":1050,\"y\":148},{\"x\":1011,\"y\":147}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":1316,\"y\":123},{\"x\":1354,\"y\":122},{\"x\":1355,\"y\":148},{\"x\":1317,\"y\":149}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":370,\"y\":228},{\"x\":383,\"y\":229},{\"x\":382,\"y\":252},{\"x\":369,\"y\":251}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":1032,\"y\":209},{\"x\":1045,\"y\":209},{\"x\":1045,\"y\":231},{\"x\":1032,\"y\":231}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1312,\"y\":208},{\"x\":1325,\"y\":208},{\"x\":1326,\"y\":231},{\"x\":1312,\"y\":231}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":86,\"y\":231},{\"x\":97,\"y\":231},{\"x\":97,\"y\":250},{\"x\":86,\"y\":250}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1162,\"y\":291},{\"x\":1204,\"y\":291},{\"x\":1204,\"y\":318},{\"x\":1162,\"y\":317}],\"text\":\"0.7\"},{\"boundingBox\":[{\"x\":490,\"y\":322},{\"x\":519,\"y\":322},{\"x\":519,\"y\":348},{\"x\":490,\"y\":347}],\"text\":\"1a\"},{\"boundingBox\":[{\"x\":910,\"y\":321},{\"x\":938,\"y\":320},{\"x\":939,\"y\":345},{\"x\":911,\"y\":347}],\"text\":\"1b\"},{\"boundingBox\":[{\"x\":693,\"y\":372},{\"x\":707,\"y\":374},{\"x\":704,\"y\":398},{\"x\":691,\"y\":396}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":573,\"y\":418},{\"x\":615,\"y\":420},{\"x\":614,\"y\":447},{\"x\":572,\"y\":445}],\"text\":\"0.8\"},{\"boundingBox\":[{\"x\":784,\"y\":416},{\"x\":826,\"y\":412},{\"x\":829,\"y\":443},{\"x\":787,\"y\":448}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":588,\"y\":503},{\"x\":609,\"y\":502},{\"x\":612,\"y\":538},{\"x\":591,\"y\":539}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":799,\"y\":504},{\"x\":817,\"y\":504},{\"x\":817,\"y\":535},{\"x\":799,\"y\":535}],\"text\":\"C\"}]}",
        "{\"language\":\"en\",\"text\":\"Start End Feedback history Trust Graph Stage 1: Obtain FHG Stage 3: Obtain TG Preconditions: Preconditions: {fı(a, b), f2(a, b), ..., fn (a, b)} Ax A - R Post-conditions: Post-conditions: Ax A - RR\\\" A x A -+ [0, 1] Stage 2: Obtain RG Feedback Preconditions: History Ax A - R\\\" Reputation Graph Post-conditions: Graph Ax A - R\",\"lines\":[{\"boundingBox\":[{\"x\":156,\"y\":36},{\"x\":230,\"y\":37},{\"x\":229,\"y\":65},{\"x\":154,\"y\":64}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":1205,\"y\":49},{\"x\":1266,\"y\":50},{\"x\":1266,\"y\":79},{\"x\":1205,\"y\":79}],\"text\":\"End\"},{\"boundingBox\":[{\"x\":121,\"y\":185},{\"x\":268,\"y\":186},{\"x\":268,\"y\":216},{\"x\":121,\"y\":215}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":141,\"y\":231},{\"x\":249,\"y\":233},{\"x\":248,\"y\":265},{\"x\":140,\"y\":261}],\"text\":\"history\"},{\"boundingBox\":[{\"x\":1197,\"y\":226},{\"x\":1279,\"y\":227},{\"x\":1280,\"y\":257},{\"x\":1197,\"y\":254}],\"text\":\"Trust\"},{\"boundingBox\":[{\"x\":1190,\"y\":268},{\"x\":1286,\"y\":271},{\"x\":1287,\"y\":306},{\"x\":1189,\"y\":304}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":39,\"y\":406},{\"x\":347,\"y\":403},{\"x\":347,\"y\":439},{\"x\":39,\"y\":441}],\"text\":\"Stage 1: Obtain FHG\"},{\"boundingBox\":[{\"x\":1061,\"y\":411},{\"x\":1345,\"y\":410},{\"x\":1345,\"y\":445},{\"x\":1061,\"y\":447}],\"text\":\"Stage 3: Obtain TG\"},{\"boundingBox\":[{\"x\":78,\"y\":449},{\"x\":310,\"y\":450},{\"x\":310,\"y\":484},{\"x\":78,\"y\":483}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1095,\"y\":457},{\"x\":1317,\"y\":457},{\"x\":1317,\"y\":490},{\"x\":1094,\"y\":489}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":15,\"y\":487},{\"x\":371,\"y\":488},{\"x\":371,\"y\":526},{\"x\":15,\"y\":525}],\"text\":\"{fı(a, b), f2(a, b), ..., fn (a, b)}\"},{\"boundingBox\":[{\"x\":1136,\"y\":496},{\"x\":1311,\"y\":495},{\"x\":1311,\"y\":529},{\"x\":1136,\"y\":530}],\"text\":\"Ax A - R\"},{\"boundingBox\":[{\"x\":70,\"y\":539},{\"x\":324,\"y\":539},{\"x\":324,\"y\":575},{\"x\":70,\"y\":574}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1079,\"y\":545},{\"x\":1328,\"y\":546},{\"x\":1328,\"y\":579},{\"x\":1079,\"y\":578}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":89,\"y\":592},{\"x\":292,\"y\":591},{\"x\":292,\"y\":626},{\"x\":89,\"y\":629}],\"text\":\"Ax A - RR\\\"\"},{\"boundingBox\":[{\"x\":1122,\"y\":590},{\"x\":1327,\"y\":590},{\"x\":1327,\"y\":626},{\"x\":1121,\"y\":625}],\"text\":\"A x A -+ [0, 1]\"},{\"boundingBox\":[{\"x\":569,\"y\":738},{\"x\":856,\"y\":736},{\"x\":856,\"y\":772},{\"x\":569,\"y\":774}],\"text\":\"Stage 2: Obtain RG\"},{\"boundingBox\":[{\"x\":120,\"y\":785},{\"x\":264,\"y\":785},{\"x\":264,\"y\":818},{\"x\":120,\"y\":818}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":604,\"y\":784},{\"x\":826,\"y\":784},{\"x\":825,\"y\":818},{\"x\":604,\"y\":816}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":136,\"y\":833},{\"x\":250,\"y\":836},{\"x\":249,\"y\":865},{\"x\":136,\"y\":862}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":611,\"y\":831},{\"x\":807,\"y\":830},{\"x\":808,\"y\":861},{\"x\":611,\"y\":863}],\"text\":\"Ax A - R\\\"\"},{\"boundingBox\":[{\"x\":1117,\"y\":805},{\"x\":1286,\"y\":805},{\"x\":1286,\"y\":841},{\"x\":1117,\"y\":840}],\"text\":\"Reputation\"},{\"boundingBox\":[{\"x\":144,\"y\":875},{\"x\":239,\"y\":877},{\"x\":239,\"y\":910},{\"x\":143,\"y\":909}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":589,\"y\":872},{\"x\":839,\"y\":873},{\"x\":839,\"y\":906},{\"x\":589,\"y\":905}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1154,\"y\":851},{\"x\":1251,\"y\":851},{\"x\":1251,\"y\":882},{\"x\":1154,\"y\":882}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":613,\"y\":923},{\"x\":786,\"y\":922},{\"x\":787,\"y\":954},{\"x\":613,\"y\":956}],\"text\":\"Ax A - R\"}],\"words\":[{\"boundingBox\":[{\"x\":155,\"y\":36},{\"x\":229,\"y\":37},{\"x\":228,\"y\":65},{\"x\":155,\"y\":64}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":1206,\"y\":49},{\"x\":1262,\"y\":49},{\"x\":1262,\"y\":79},{\"x\":1206,\"y\":78}],\"text\":\"End\"},{\"boundingBox\":[{\"x\":121,\"y\":186},{\"x\":264,\"y\":186},{\"x\":265,\"y\":217},{\"x\":121,\"y\":216}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":142,\"y\":231},{\"x\":245,\"y\":235},{\"x\":245,\"y\":265},{\"x\":141,\"y\":262}],\"text\":\"history\"},{\"boundingBox\":[{\"x\":1199,\"y\":226},{\"x\":1278,\"y\":227},{\"x\":1277,\"y\":257},{\"x\":1198,\"y\":255}],\"text\":\"Trust\"},{\"boundingBox\":[{\"x\":1191,\"y\":268},{\"x\":1281,\"y\":270},{\"x\":1280,\"y\":306},{\"x\":1190,\"y\":303}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":42,\"y\":406},{\"x\":126,\"y\":407},{\"x\":126,\"y\":441},{\"x\":42,\"y\":442}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":133,\"y\":407},{\"x\":163,\"y\":407},{\"x\":163,\"y\":440},{\"x\":133,\"y\":441}],\"text\":\"1:\"},{\"boundingBox\":[{\"x\":170,\"y\":407},{\"x\":272,\"y\":405},{\"x\":273,\"y\":440},{\"x\":170,\"y\":440}],\"text\":\"Obtain\"},{\"boundingBox\":[{\"x\":279,\"y\":405},{\"x\":340,\"y\":404},{\"x\":341,\"y\":440},{\"x\":280,\"y\":440}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":1063,\"y\":411},{\"x\":1146,\"y\":412},{\"x\":1145,\"y\":447},{\"x\":1062,\"y\":447}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":1152,\"y\":412},{\"x\":1182,\"y\":412},{\"x\":1181,\"y\":447},{\"x\":1152,\"y\":447}],\"text\":\"3:\"},{\"boundingBox\":[{\"x\":1188,\"y\":412},{\"x\":1294,\"y\":412},{\"x\":1294,\"y\":445},{\"x\":1188,\"y\":447}],\"text\":\"Obtain\"},{\"boundingBox\":[{\"x\":1301,\"y\":412},{\"x\":1339,\"y\":411},{\"x\":1340,\"y\":444},{\"x\":1302,\"y\":445}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":81,\"y\":449},{\"x\":310,\"y\":452},{\"x\":311,\"y\":484},{\"x\":80,\"y\":484}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1095,\"y\":458},{\"x\":1316,\"y\":457},{\"x\":1315,\"y\":491},{\"x\":1095,\"y\":489}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":15,\"y\":488},{\"x\":82,\"y\":489},{\"x\":82,\"y\":526},{\"x\":15,\"y\":525}],\"text\":\"{fı(a,\"},{\"boundingBox\":[{\"x\":90,\"y\":489},{\"x\":121,\"y\":490},{\"x\":121,\"y\":526},{\"x\":90,\"y\":526}],\"text\":\"b),\"},{\"boundingBox\":[{\"x\":128,\"y\":490},{\"x\":184,\"y\":490},{\"x\":184,\"y\":526},{\"x\":128,\"y\":526}],\"text\":\"f2(a,\"},{\"boundingBox\":[{\"x\":191,\"y\":490},{\"x\":223,\"y\":490},{\"x\":222,\"y\":526},{\"x\":191,\"y\":526}],\"text\":\"b),\"},{\"boundingBox\":[{\"x\":230,\"y\":490},{\"x\":259,\"y\":490},{\"x\":259,\"y\":526},{\"x\":230,\"y\":526}],\"text\":\"...,\"},{\"boundingBox\":[{\"x\":266,\"y\":490},{\"x\":288,\"y\":490},{\"x\":288,\"y\":526},{\"x\":266,\"y\":526}],\"text\":\"fn\"},{\"boundingBox\":[{\"x\":295,\"y\":490},{\"x\":327,\"y\":489},{\"x\":326,\"y\":526},{\"x\":295,\"y\":526}],\"text\":\"(a,\"},{\"boundingBox\":[{\"x\":334,\"y\":489},{\"x\":371,\"y\":489},{\"x\":370,\"y\":526},{\"x\":333,\"y\":526}],\"text\":\"b)}\"},{\"boundingBox\":[{\"x\":1142,\"y\":497},{\"x\":1190,\"y\":497},{\"x\":1189,\"y\":529},{\"x\":1140,\"y\":530}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":1203,\"y\":497},{\"x\":1224,\"y\":497},{\"x\":1223,\"y\":528},{\"x\":1202,\"y\":529}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1239,\"y\":497},{\"x\":1258,\"y\":497},{\"x\":1257,\"y\":529},{\"x\":1238,\"y\":529}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1279,\"y\":496},{\"x\":1298,\"y\":496},{\"x\":1297,\"y\":529},{\"x\":1278,\"y\":529}],\"text\":\"R\"},{\"boundingBox\":[{\"x\":72,\"y\":540},{\"x\":323,\"y\":540},{\"x\":324,\"y\":576},{\"x\":70,\"y\":574}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1080,\"y\":545},{\"x\":1327,\"y\":548},{\"x\":1327,\"y\":579},{\"x\":1080,\"y\":579}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":96,\"y\":593},{\"x\":148,\"y\":593},{\"x\":147,\"y\":628},{\"x\":94,\"y\":629}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":165,\"y\":593},{\"x\":185,\"y\":593},{\"x\":184,\"y\":628},{\"x\":164,\"y\":628}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":198,\"y\":593},{\"x\":218,\"y\":593},{\"x\":216,\"y\":627},{\"x\":196,\"y\":628}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":242,\"y\":593},{\"x\":290,\"y\":591},{\"x\":288,\"y\":627},{\"x\":240,\"y\":627}],\"text\":\"RR\\\"\"},{\"boundingBox\":[{\"x\":1126,\"y\":591},{\"x\":1146,\"y\":591},{\"x\":1146,\"y\":624},{\"x\":1126,\"y\":624}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1156,\"y\":591},{\"x\":1176,\"y\":591},{\"x\":1176,\"y\":625},{\"x\":1156,\"y\":624}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1188,\"y\":591},{\"x\":1208,\"y\":591},{\"x\":1208,\"y\":625},{\"x\":1188,\"y\":625}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1218,\"y\":591},{\"x\":1250,\"y\":591},{\"x\":1250,\"y\":626},{\"x\":1218,\"y\":625}],\"text\":\"-+\"},{\"boundingBox\":[{\"x\":1256,\"y\":591},{\"x\":1291,\"y\":591},{\"x\":1291,\"y\":626},{\"x\":1256,\"y\":626}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1297,\"y\":591},{\"x\":1325,\"y\":590},{\"x\":1326,\"y\":627},{\"x\":1298,\"y\":626}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":571,\"y\":740},{\"x\":653,\"y\":740},{\"x\":652,\"y\":774},{\"x\":569,\"y\":774}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":660,\"y\":740},{\"x\":690,\"y\":740},{\"x\":689,\"y\":774},{\"x\":659,\"y\":774}],\"text\":\"2:\"},{\"boundingBox\":[{\"x\":697,\"y\":740},{\"x\":800,\"y\":738},{\"x\":801,\"y\":774},{\"x\":696,\"y\":774}],\"text\":\"Obtain\"},{\"boundingBox\":[{\"x\":807,\"y\":738},{\"x\":850,\"y\":737},{\"x\":851,\"y\":773},{\"x\":808,\"y\":774}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":121,\"y\":788},{\"x\":263,\"y\":785},{\"x\":264,\"y\":819},{\"x\":120,\"y\":818}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":606,\"y\":785},{\"x\":826,\"y\":784},{\"x\":826,\"y\":819},{\"x\":604,\"y\":815}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":137,\"y\":834},{\"x\":246,\"y\":836},{\"x\":246,\"y\":866},{\"x\":136,\"y\":863}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":614,\"y\":831},{\"x\":666,\"y\":831},{\"x\":665,\"y\":863},{\"x\":613,\"y\":864}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":684,\"y\":831},{\"x\":702,\"y\":831},{\"x\":701,\"y\":863},{\"x\":683,\"y\":863}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":719,\"y\":831},{\"x\":738,\"y\":831},{\"x\":737,\"y\":862},{\"x\":718,\"y\":863}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":761,\"y\":831},{\"x\":807,\"y\":830},{\"x\":806,\"y\":862},{\"x\":760,\"y\":862}],\"text\":\"R\\\"\"},{\"boundingBox\":[{\"x\":1118,\"y\":805},{\"x\":1285,\"y\":806},{\"x\":1284,\"y\":842},{\"x\":1117,\"y\":841}],\"text\":\"Reputation\"},{\"boundingBox\":[{\"x\":143,\"y\":875},{\"x\":235,\"y\":876},{\"x\":234,\"y\":910},{\"x\":143,\"y\":908}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":590,\"y\":873},{\"x\":838,\"y\":873},{\"x\":838,\"y\":907},{\"x\":590,\"y\":906}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1157,\"y\":851},{\"x\":1244,\"y\":852},{\"x\":1244,\"y\":883},{\"x\":1155,\"y\":883}],\"text\":\"Graph\"},{\"boundingBox\":[{\"x\":617,\"y\":923},{\"x\":666,\"y\":923},{\"x\":665,\"y\":956},{\"x\":615,\"y\":957}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":680,\"y\":923},{\"x\":699,\"y\":923},{\"x\":698,\"y\":956},{\"x\":678,\"y\":956}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":714,\"y\":923},{\"x\":733,\"y\":923},{\"x\":732,\"y\":955},{\"x\":713,\"y\":955}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":755,\"y\":923},{\"x\":774,\"y\":923},{\"x\":774,\"y\":955},{\"x\":755,\"y\":955}],\"text\":\"R\"}]}",
        "{\"language\":\"en\",\"text\":\"Stage transitions Feedback History Output: A x A => [0, 1] Input: A x A => {-1, 1]* FHG RG TG fully connected global relative rep. scores Input: A x A => [0, 1]* Ouput: A x A => [0, 1] Feedback History FHG RG TG fully connected global absolute rep. scores Input: A x A => [0, 1] Output: Ax A => [0, 1] local Feedback History FHG RG TG partially connected local absolute rep.scores Appleseed | PeerTrust | EigenTrust Input: A x A => {-1, 1]* Feedback History FHG DO TG Output: Ax A => {0, 1} fully connected global Aberer & Despotovic Input: Ax A => {0, 1} Feedback History Ouput: Ax A => {0, 1} FHG RG TG partially connected, local Input: A x A => {0, 1]* Feedback History FHG RG TG Ouput: Ax A => [0, 1] local absolute rep. scores TRAVOS |Advogato Input: A x A => [0, 1] relative rep. scores Feedback History FHG RG TG Output: A x A => {0, 1} partially connected Input: A x A => [0, 1] absolute rep. scores Feedback History FHG RG TG Output: A x A => {0, 1} partially connected Thresholding Ranking\",\"lines\":[{\"boundingBox\":[{\"x\":577,\"y\":1},{\"x\":817,\"y\":1},{\"x\":817,\"y\":30},{\"x\":577,\"y\":31}],\"text\":\"Stage transitions\"},{\"boundingBox\":[{\"x\":341,\"y\":112},{\"x\":512,\"y\":112},{\"x\":512,\"y\":136},{\"x\":341,\"y\":136}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":1141,\"y\":90},{\"x\":1367,\"y\":91},{\"x\":1367,\"y\":116},{\"x\":1141,\"y\":115}],\"text\":\"Output: A x A => [0, 1]\"},{\"boundingBox\":[{\"x\":94,\"y\":119},{\"x\":319,\"y\":120},{\"x\":319,\"y\":145},{\"x\":94,\"y\":144}],\"text\":\"Input: A x A => {-1, 1]*\"},{\"boundingBox\":[{\"x\":608,\"y\":114},{\"x\":659,\"y\":112},{\"x\":659,\"y\":132},{\"x\":608,\"y\":133}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":826,\"y\":114},{\"x\":861,\"y\":113},{\"x\":862,\"y\":134},{\"x\":827,\"y\":135}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1020,\"y\":114},{\"x\":1053,\"y\":112},{\"x\":1054,\"y\":133},{\"x\":1021,\"y\":135}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1144,\"y\":117},{\"x\":1428,\"y\":117},{\"x\":1428,\"y\":140},{\"x\":1144,\"y\":140}],\"text\":\"fully connected global relative\"},{\"boundingBox\":[{\"x\":1145,\"y\":145},{\"x\":1254,\"y\":144},{\"x\":1255,\"y\":165},{\"x\":1145,\"y\":168}],\"text\":\"rep. scores\"},{\"boundingBox\":[{\"x\":101,\"y\":257},{\"x\":316,\"y\":257},{\"x\":316,\"y\":284},{\"x\":101,\"y\":283}],\"text\":\"Input: A x A => [0, 1]*\"},{\"boundingBox\":[{\"x\":1154,\"y\":230},{\"x\":1370,\"y\":230},{\"x\":1370,\"y\":255},{\"x\":1154,\"y\":255}],\"text\":\"Ouput: A x A => [0, 1]\"},{\"boundingBox\":[{\"x\":341,\"y\":257},{\"x\":514,\"y\":257},{\"x\":513,\"y\":282},{\"x\":341,\"y\":280}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":608,\"y\":257},{\"x\":658,\"y\":256},{\"x\":658,\"y\":276},{\"x\":609,\"y\":276}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":259},{\"x\":859,\"y\":257},{\"x\":861,\"y\":278},{\"x\":827,\"y\":279}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":258},{\"x\":1050,\"y\":256},{\"x\":1052,\"y\":276},{\"x\":1023,\"y\":279}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1153,\"y\":257},{\"x\":1366,\"y\":257},{\"x\":1366,\"y\":282},{\"x\":1153,\"y\":281}],\"text\":\"fully connected global\"},{\"boundingBox\":[{\"x\":1155,\"y\":283},{\"x\":1351,\"y\":284},{\"x\":1351,\"y\":308},{\"x\":1155,\"y\":307}],\"text\":\"absolute rep. scores\"},{\"boundingBox\":[{\"x\":94,\"y\":408},{\"x\":301,\"y\":408},{\"x\":301,\"y\":434},{\"x\":94,\"y\":433}],\"text\":\"Input: A x A => [0, 1]\"},{\"boundingBox\":[{\"x\":1153,\"y\":395},{\"x\":1385,\"y\":395},{\"x\":1385,\"y\":420},{\"x\":1153,\"y\":420}],\"text\":\"Output: Ax A => [0, 1]\"},{\"boundingBox\":[{\"x\":100,\"y\":436},{\"x\":147,\"y\":436},{\"x\":147,\"y\":456},{\"x\":100,\"y\":455}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":345,\"y\":420},{\"x\":517,\"y\":421},{\"x\":516,\"y\":444},{\"x\":345,\"y\":442}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":609,\"y\":422},{\"x\":659,\"y\":421},{\"x\":659,\"y\":441},{\"x\":609,\"y\":441}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":824,\"y\":423},{\"x\":858,\"y\":420},{\"x\":860,\"y\":440},{\"x\":826,\"y\":442}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1023,\"y\":422},{\"x\":1054,\"y\":420},{\"x\":1056,\"y\":441},{\"x\":1024,\"y\":443}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1158,\"y\":422},{\"x\":1394,\"y\":422},{\"x\":1394,\"y\":444},{\"x\":1158,\"y\":445}],\"text\":\"partially connected local\"},{\"boundingBox\":[{\"x\":1154,\"y\":448},{\"x\":1344,\"y\":448},{\"x\":1344,\"y\":472},{\"x\":1154,\"y\":471}],\"text\":\"absolute rep.scores\"},{\"boundingBox\":[{\"x\":23,\"y\":513},{\"x\":26,\"y\":56},{\"x\":55,\"y\":56},{\"x\":52,\"y\":514}],\"text\":\"Appleseed | PeerTrust | EigenTrust\"},{\"boundingBox\":[{\"x\":95,\"y\":556},{\"x\":320,\"y\":556},{\"x\":320,\"y\":582},{\"x\":95,\"y\":583}],\"text\":\"Input: A x A => {-1, 1]*\"},{\"boundingBox\":[{\"x\":342,\"y\":555},{\"x\":513,\"y\":555},{\"x\":513,\"y\":580},{\"x\":342,\"y\":580}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":611,\"y\":557},{\"x\":658,\"y\":555},{\"x\":658,\"y\":576},{\"x\":611,\"y\":576}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":823,\"y\":555},{\"x\":866,\"y\":556},{\"x\":866,\"y\":569},{\"x\":823,\"y\":567}],\"text\":\"DO\"},{\"boundingBox\":[{\"x\":1024,\"y\":556},{\"x\":1052,\"y\":554},{\"x\":1054,\"y\":575},{\"x\":1025,\"y\":577}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1152,\"y\":546},{\"x\":1386,\"y\":546},{\"x\":1386,\"y\":571},{\"x\":1152,\"y\":571}],\"text\":\"Output: Ax A => {0, 1}\"},{\"boundingBox\":[{\"x\":1155,\"y\":572},{\"x\":1370,\"y\":573},{\"x\":1369,\"y\":597},{\"x\":1154,\"y\":596}],\"text\":\"fully connected global\"},{\"boundingBox\":[{\"x\":10,\"y\":642},{\"x\":12,\"y\":503},{\"x\":40,\"y\":503},{\"x\":37,\"y\":643}],\"text\":\"Aberer &\"},{\"boundingBox\":[{\"x\":40,\"y\":654},{\"x\":42,\"y\":503},{\"x\":69,\"y\":503},{\"x\":67,\"y\":654}],\"text\":\"Despotovic\"},{\"boundingBox\":[{\"x\":94,\"y\":710},{\"x\":314,\"y\":709},{\"x\":314,\"y\":738},{\"x\":94,\"y\":739}],\"text\":\"Input: Ax A => {0, 1}\"},{\"boundingBox\":[{\"x\":338,\"y\":710},{\"x\":515,\"y\":710},{\"x\":515,\"y\":734},{\"x\":338,\"y\":733}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":1156,\"y\":687},{\"x\":1379,\"y\":687},{\"x\":1379,\"y\":712},{\"x\":1156,\"y\":713}],\"text\":\"Ouput: Ax A => {0, 1}\"},{\"boundingBox\":[{\"x\":608,\"y\":712},{\"x\":658,\"y\":709},{\"x\":659,\"y\":731},{\"x\":609,\"y\":732}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":828,\"y\":712},{\"x\":861,\"y\":709},{\"x\":862,\"y\":730},{\"x\":829,\"y\":733}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":712},{\"x\":1052,\"y\":709},{\"x\":1055,\"y\":730},{\"x\":1024,\"y\":733}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1156,\"y\":714},{\"x\":1396,\"y\":714},{\"x\":1396,\"y\":738},{\"x\":1156,\"y\":739}],\"text\":\"partially connected, local\"},{\"boundingBox\":[{\"x\":94,\"y\":870},{\"x\":313,\"y\":868},{\"x\":313,\"y\":896},{\"x\":94,\"y\":897}],\"text\":\"Input: A x A => {0, 1]*\"},{\"boundingBox\":[{\"x\":344,\"y\":869},{\"x\":516,\"y\":870},{\"x\":515,\"y\":896},{\"x\":343,\"y\":892}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":608,\"y\":871},{\"x\":658,\"y\":870},{\"x\":660,\"y\":891},{\"x\":609,\"y\":891}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":872},{\"x\":859,\"y\":869},{\"x\":860,\"y\":889},{\"x\":827,\"y\":892}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1022,\"y\":871},{\"x\":1052,\"y\":869},{\"x\":1054,\"y\":892},{\"x\":1024,\"y\":894}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1143,\"y\":856},{\"x\":1365,\"y\":857},{\"x\":1365,\"y\":881},{\"x\":1143,\"y\":880}],\"text\":\"Ouput: Ax A => [0, 1]\"},{\"boundingBox\":[{\"x\":1144,\"y\":882},{\"x\":1397,\"y\":884},{\"x\":1397,\"y\":908},{\"x\":1144,\"y\":906}],\"text\":\"local absolute rep. scores\"},{\"boundingBox\":[{\"x\":24,\"y\":937},{\"x\":27,\"y\":663},{\"x\":54,\"y\":663},{\"x\":51,\"y\":937}],\"text\":\"TRAVOS |Advogato\"},{\"boundingBox\":[{\"x\":94,\"y\":1002},{\"x\":301,\"y\":1001},{\"x\":301,\"y\":1028},{\"x\":94,\"y\":1028}],\"text\":\"Input: A x A => [0, 1]\"},{\"boundingBox\":[{\"x\":95,\"y\":1029},{\"x\":283,\"y\":1029},{\"x\":283,\"y\":1053},{\"x\":95,\"y\":1052}],\"text\":\"relative rep. scores\"},{\"boundingBox\":[{\"x\":340,\"y\":1013},{\"x\":518,\"y\":1015},{\"x\":518,\"y\":1039},{\"x\":340,\"y\":1036}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":607,\"y\":1015},{\"x\":659,\"y\":1014},{\"x\":659,\"y\":1035},{\"x\":608,\"y\":1035}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":1015},{\"x\":859,\"y\":1013},{\"x\":860,\"y\":1035},{\"x\":826,\"y\":1036}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1020,\"y\":1017},{\"x\":1052,\"y\":1014},{\"x\":1054,\"y\":1035},{\"x\":1023,\"y\":1037}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1142,\"y\":1001},{\"x\":1371,\"y\":1001},{\"x\":1371,\"y\":1028},{\"x\":1142,\"y\":1028}],\"text\":\"Output: A x A => {0, 1}\"},{\"boundingBox\":[{\"x\":1144,\"y\":1029},{\"x\":1329,\"y\":1029},{\"x\":1329,\"y\":1052},{\"x\":1144,\"y\":1053}],\"text\":\"partially connected\"},{\"boundingBox\":[{\"x\":94,\"y\":1160},{\"x\":299,\"y\":1160},{\"x\":299,\"y\":1186},{\"x\":94,\"y\":1186}],\"text\":\"Input: A x A => [0, 1]\"},{\"boundingBox\":[{\"x\":105,\"y\":1187},{\"x\":301,\"y\":1187},{\"x\":301,\"y\":1210},{\"x\":105,\"y\":1210}],\"text\":\"absolute rep. scores\"},{\"boundingBox\":[{\"x\":340,\"y\":1172},{\"x\":514,\"y\":1172},{\"x\":514,\"y\":1195},{\"x\":340,\"y\":1194}],\"text\":\"Feedback History\"},{\"boundingBox\":[{\"x\":609,\"y\":1172},{\"x\":660,\"y\":1172},{\"x\":659,\"y\":1192},{\"x\":610,\"y\":1192}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":826,\"y\":1173},{\"x\":857,\"y\":1171},{\"x\":859,\"y\":1192},{\"x\":827,\"y\":1194}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1020,\"y\":1173},{\"x\":1052,\"y\":1171},{\"x\":1054,\"y\":1191},{\"x\":1023,\"y\":1193}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1155,\"y\":1158},{\"x\":1383,\"y\":1158},{\"x\":1383,\"y\":1186},{\"x\":1154,\"y\":1185}],\"text\":\"Output: A x A => {0, 1}\"},{\"boundingBox\":[{\"x\":1155,\"y\":1187},{\"x\":1338,\"y\":1184},{\"x\":1338,\"y\":1209},{\"x\":1155,\"y\":1212}],\"text\":\"partially connected\"},{\"boundingBox\":[{\"x\":25,\"y\":1264},{\"x\":26,\"y\":962},{\"x\":55,\"y\":962},{\"x\":53,\"y\":1264}],\"text\":\"Thresholding Ranking\"}],\"words\":[{\"boundingBox\":[{\"x\":580,\"y\":1},{\"x\":660,\"y\":2},{\"x\":660,\"y\":30},{\"x\":579,\"y\":32}],\"text\":\"Stage\"},{\"boundingBox\":[{\"x\":670,\"y\":2},{\"x\":817,\"y\":2},{\"x\":817,\"y\":30},{\"x\":669,\"y\":30}],\"text\":\"transitions\"},{\"boundingBox\":[{\"x\":342,\"y\":113},{\"x\":439,\"y\":113},{\"x\":439,\"y\":137},{\"x\":342,\"y\":137}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":443,\"y\":113},{\"x\":512,\"y\":113},{\"x\":513,\"y\":136},{\"x\":444,\"y\":137}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":1143,\"y\":92},{\"x\":1224,\"y\":91},{\"x\":1224,\"y\":114},{\"x\":1142,\"y\":114}],\"text\":\"Output:\"},{\"boundingBox\":[{\"x\":1229,\"y\":91},{\"x\":1241,\"y\":91},{\"x\":1240,\"y\":114},{\"x\":1228,\"y\":114}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1246,\"y\":91},{\"x\":1259,\"y\":91},{\"x\":1259,\"y\":114},{\"x\":1245,\"y\":114}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1264,\"y\":91},{\"x\":1278,\"y\":91},{\"x\":1277,\"y\":114},{\"x\":1264,\"y\":114}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1284,\"y\":91},{\"x\":1309,\"y\":91},{\"x\":1308,\"y\":115},{\"x\":1283,\"y\":115}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1314,\"y\":91},{\"x\":1341,\"y\":92},{\"x\":1340,\"y\":116},{\"x\":1313,\"y\":115}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1346,\"y\":92},{\"x\":1368,\"y\":92},{\"x\":1367,\"y\":117},{\"x\":1345,\"y\":116}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":95,\"y\":120},{\"x\":156,\"y\":120},{\"x\":156,\"y\":145},{\"x\":95,\"y\":145}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":161,\"y\":120},{\"x\":174,\"y\":120},{\"x\":174,\"y\":145},{\"x\":161,\"y\":145}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":179,\"y\":120},{\"x\":192,\"y\":120},{\"x\":192,\"y\":145},{\"x\":179,\"y\":145}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":197,\"y\":120},{\"x\":212,\"y\":120},{\"x\":211,\"y\":145},{\"x\":197,\"y\":145}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":217,\"y\":120},{\"x\":241,\"y\":120},{\"x\":241,\"y\":145},{\"x\":217,\"y\":145}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":246,\"y\":120},{\"x\":282,\"y\":121},{\"x\":282,\"y\":145},{\"x\":246,\"y\":145}],\"text\":\"{-1,\"},{\"boundingBox\":[{\"x\":287,\"y\":121},{\"x\":318,\"y\":121},{\"x\":318,\"y\":145},{\"x\":287,\"y\":145}],\"text\":\"1]*\"},{\"boundingBox\":[{\"x\":610,\"y\":113},{\"x\":651,\"y\":112},{\"x\":651,\"y\":132},{\"x\":610,\"y\":133}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":826,\"y\":114},{\"x\":853,\"y\":113},{\"x\":853,\"y\":134},{\"x\":827,\"y\":135}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":114},{\"x\":1044,\"y\":113},{\"x\":1045,\"y\":133},{\"x\":1022,\"y\":135}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1144,\"y\":118},{\"x\":1182,\"y\":118},{\"x\":1182,\"y\":141},{\"x\":1144,\"y\":141}],\"text\":\"fully\"},{\"boundingBox\":[{\"x\":1187,\"y\":118},{\"x\":1288,\"y\":118},{\"x\":1288,\"y\":140},{\"x\":1187,\"y\":141}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":1293,\"y\":118},{\"x\":1352,\"y\":118},{\"x\":1352,\"y\":140},{\"x\":1293,\"y\":140}],\"text\":\"global\"},{\"boundingBox\":[{\"x\":1357,\"y\":118},{\"x\":1428,\"y\":118},{\"x\":1428,\"y\":141},{\"x\":1357,\"y\":140}],\"text\":\"relative\"},{\"boundingBox\":[{\"x\":1146,\"y\":146},{\"x\":1182,\"y\":145},{\"x\":1182,\"y\":168},{\"x\":1146,\"y\":168}],\"text\":\"rep.\"},{\"boundingBox\":[{\"x\":1187,\"y\":145},{\"x\":1253,\"y\":144},{\"x\":1253,\"y\":166},{\"x\":1186,\"y\":168}],\"text\":\"scores\"},{\"boundingBox\":[{\"x\":101,\"y\":258},{\"x\":162,\"y\":258},{\"x\":163,\"y\":283},{\"x\":102,\"y\":282}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":168,\"y\":258},{\"x\":180,\"y\":257},{\"x\":180,\"y\":284},{\"x\":168,\"y\":283}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":185,\"y\":257},{\"x\":198,\"y\":257},{\"x\":199,\"y\":284},{\"x\":185,\"y\":284}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":204,\"y\":257},{\"x\":217,\"y\":257},{\"x\":217,\"y\":284},{\"x\":204,\"y\":284}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":222,\"y\":257},{\"x\":248,\"y\":257},{\"x\":248,\"y\":285},{\"x\":223,\"y\":284}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":253,\"y\":257},{\"x\":279,\"y\":257},{\"x\":279,\"y\":285},{\"x\":253,\"y\":285}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":284,\"y\":258},{\"x\":315,\"y\":258},{\"x\":315,\"y\":285},{\"x\":284,\"y\":285}],\"text\":\"1]*\"},{\"boundingBox\":[{\"x\":1156,\"y\":232},{\"x\":1225,\"y\":231},{\"x\":1224,\"y\":256},{\"x\":1155,\"y\":256}],\"text\":\"Ouput:\"},{\"boundingBox\":[{\"x\":1230,\"y\":231},{\"x\":1244,\"y\":231},{\"x\":1243,\"y\":256},{\"x\":1229,\"y\":256}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1250,\"y\":231},{\"x\":1262,\"y\":231},{\"x\":1261,\"y\":256},{\"x\":1248,\"y\":256}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1267,\"y\":231},{\"x\":1281,\"y\":231},{\"x\":1280,\"y\":256},{\"x\":1266,\"y\":256}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1287,\"y\":231},{\"x\":1313,\"y\":231},{\"x\":1311,\"y\":256},{\"x\":1285,\"y\":256}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1317,\"y\":231},{\"x\":1343,\"y\":231},{\"x\":1342,\"y\":256},{\"x\":1316,\"y\":256}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1348,\"y\":231},{\"x\":1371,\"y\":231},{\"x\":1369,\"y\":256},{\"x\":1346,\"y\":256}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":342,\"y\":257},{\"x\":440,\"y\":257},{\"x\":440,\"y\":282},{\"x\":342,\"y\":279}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":444,\"y\":257},{\"x\":514,\"y\":258},{\"x\":514,\"y\":283},{\"x\":444,\"y\":282}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":609,\"y\":256},{\"x\":650,\"y\":256},{\"x\":650,\"y\":276},{\"x\":609,\"y\":276}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":258},{\"x\":853,\"y\":257},{\"x\":854,\"y\":278},{\"x\":826,\"y\":279}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":258},{\"x\":1043,\"y\":256},{\"x\":1045,\"y\":277},{\"x\":1023,\"y\":279}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1155,\"y\":258},{\"x\":1193,\"y\":258},{\"x\":1192,\"y\":281},{\"x\":1153,\"y\":281}],\"text\":\"fully\"},{\"boundingBox\":[{\"x\":1198,\"y\":258},{\"x\":1299,\"y\":258},{\"x\":1298,\"y\":281},{\"x\":1196,\"y\":281}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":1304,\"y\":258},{\"x\":1366,\"y\":258},{\"x\":1365,\"y\":282},{\"x\":1303,\"y\":281}],\"text\":\"global\"},{\"boundingBox\":[{\"x\":1157,\"y\":284},{\"x\":1236,\"y\":285},{\"x\":1236,\"y\":308},{\"x\":1156,\"y\":308}],\"text\":\"absolute\"},{\"boundingBox\":[{\"x\":1241,\"y\":285},{\"x\":1280,\"y\":285},{\"x\":1279,\"y\":308},{\"x\":1240,\"y\":308}],\"text\":\"rep.\"},{\"boundingBox\":[{\"x\":1284,\"y\":285},{\"x\":1351,\"y\":286},{\"x\":1350,\"y\":308},{\"x\":1283,\"y\":308}],\"text\":\"scores\"},{\"boundingBox\":[{\"x\":96,\"y\":408},{\"x\":156,\"y\":409},{\"x\":155,\"y\":434},{\"x\":94,\"y\":433}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":161,\"y\":409},{\"x\":175,\"y\":409},{\"x\":174,\"y\":434},{\"x\":160,\"y\":434}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":180,\"y\":409},{\"x\":193,\"y\":409},{\"x\":192,\"y\":434},{\"x\":179,\"y\":434}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":198,\"y\":409},{\"x\":212,\"y\":409},{\"x\":211,\"y\":435},{\"x\":197,\"y\":434}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":218,\"y\":409},{\"x\":242,\"y\":409},{\"x\":242,\"y\":435},{\"x\":217,\"y\":435}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":247,\"y\":409},{\"x\":274,\"y\":409},{\"x\":273,\"y\":434},{\"x\":247,\"y\":434}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":279,\"y\":409},{\"x\":301,\"y\":409},{\"x\":300,\"y\":434},{\"x\":278,\"y\":434}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":1156,\"y\":396},{\"x\":1239,\"y\":396},{\"x\":1238,\"y\":420},{\"x\":1154,\"y\":420}],\"text\":\"Output:\"},{\"boundingBox\":[{\"x\":1244,\"y\":396},{\"x\":1275,\"y\":396},{\"x\":1274,\"y\":420},{\"x\":1243,\"y\":420}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":1280,\"y\":396},{\"x\":1294,\"y\":396},{\"x\":1294,\"y\":420},{\"x\":1280,\"y\":420}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1299,\"y\":396},{\"x\":1325,\"y\":396},{\"x\":1324,\"y\":420},{\"x\":1299,\"y\":420}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1329,\"y\":396},{\"x\":1356,\"y\":396},{\"x\":1356,\"y\":420},{\"x\":1329,\"y\":420}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1361,\"y\":396},{\"x\":1385,\"y\":396},{\"x\":1385,\"y\":420},{\"x\":1361,\"y\":420}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":100,\"y\":436},{\"x\":147,\"y\":436},{\"x\":146,\"y\":456},{\"x\":100,\"y\":455}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":346,\"y\":420},{\"x\":440,\"y\":421},{\"x\":439,\"y\":444},{\"x\":346,\"y\":442}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":444,\"y\":421},{\"x\":516,\"y\":422},{\"x\":515,\"y\":445},{\"x\":444,\"y\":444}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":610,\"y\":421},{\"x\":651,\"y\":421},{\"x\":651,\"y\":441},{\"x\":610,\"y\":441}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":824,\"y\":422},{\"x\":852,\"y\":420},{\"x\":853,\"y\":440},{\"x\":826,\"y\":442}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1023,\"y\":421},{\"x\":1044,\"y\":420},{\"x\":1045,\"y\":442},{\"x\":1024,\"y\":443}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1159,\"y\":422},{\"x\":1234,\"y\":422},{\"x\":1234,\"y\":446},{\"x\":1159,\"y\":446}],\"text\":\"partially\"},{\"boundingBox\":[{\"x\":1239,\"y\":422},{\"x\":1341,\"y\":422},{\"x\":1341,\"y\":445},{\"x\":1239,\"y\":445}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":1346,\"y\":422},{\"x\":1395,\"y\":422},{\"x\":1394,\"y\":445},{\"x\":1345,\"y\":445}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":1156,\"y\":448},{\"x\":1236,\"y\":449},{\"x\":1236,\"y\":472},{\"x\":1156,\"y\":471}],\"text\":\"absolute\"},{\"boundingBox\":[{\"x\":1240,\"y\":449},{\"x\":1344,\"y\":449},{\"x\":1344,\"y\":471},{\"x\":1240,\"y\":472}],\"text\":\"rep.scores\"},{\"boundingBox\":[{\"x\":23,\"y\":495},{\"x\":24,\"y\":372},{\"x\":52,\"y\":372},{\"x\":52,\"y\":496}],\"text\":\"Appleseed\"},{\"boundingBox\":[{\"x\":24,\"y\":366},{\"x\":24,\"y\":350},{\"x\":52,\"y\":350},{\"x\":52,\"y\":366}],\"text\":\"|\"},{\"boundingBox\":[{\"x\":24,\"y\":342},{\"x\":25,\"y\":217},{\"x\":53,\"y\":216},{\"x\":52,\"y\":342}],\"text\":\"PeerTrust\"},{\"boundingBox\":[{\"x\":25,\"y\":209},{\"x\":26,\"y\":202},{\"x\":53,\"y\":201},{\"x\":53,\"y\":208}],\"text\":\"|\"},{\"boundingBox\":[{\"x\":26,\"y\":196},{\"x\":27,\"y\":60},{\"x\":55,\"y\":58},{\"x\":53,\"y\":195}],\"text\":\"EigenTrust\"},{\"boundingBox\":[{\"x\":96,\"y\":557},{\"x\":156,\"y\":557},{\"x\":155,\"y\":582},{\"x\":95,\"y\":582}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":161,\"y\":557},{\"x\":175,\"y\":557},{\"x\":174,\"y\":582},{\"x\":160,\"y\":582}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":180,\"y\":557},{\"x\":193,\"y\":557},{\"x\":192,\"y\":582},{\"x\":179,\"y\":582}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":198,\"y\":557},{\"x\":212,\"y\":557},{\"x\":210,\"y\":582},{\"x\":197,\"y\":582}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":217,\"y\":557},{\"x\":242,\"y\":557},{\"x\":240,\"y\":582},{\"x\":215,\"y\":582}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":247,\"y\":557},{\"x\":282,\"y\":557},{\"x\":281,\"y\":583},{\"x\":245,\"y\":582}],\"text\":\"{-1,\"},{\"boundingBox\":[{\"x\":287,\"y\":557},{\"x\":319,\"y\":556},{\"x\":318,\"y\":583},{\"x\":286,\"y\":583}],\"text\":\"1]*\"},{\"boundingBox\":[{\"x\":344,\"y\":557},{\"x\":440,\"y\":556},{\"x\":439,\"y\":580},{\"x\":342,\"y\":578}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":445,\"y\":556},{\"x\":513,\"y\":556},{\"x\":512,\"y\":579},{\"x\":444,\"y\":580}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":611,\"y\":556},{\"x\":650,\"y\":555},{\"x\":651,\"y\":576},{\"x\":611,\"y\":577}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":556},{\"x\":848,\"y\":556},{\"x\":849,\"y\":569},{\"x\":826,\"y\":568}],\"text\":\"DO\"},{\"boundingBox\":[{\"x\":1024,\"y\":555},{\"x\":1043,\"y\":554},{\"x\":1045,\"y\":576},{\"x\":1025,\"y\":577}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1155,\"y\":546},{\"x\":1239,\"y\":547},{\"x\":1238,\"y\":572},{\"x\":1153,\"y\":572}],\"text\":\"Output:\"},{\"boundingBox\":[{\"x\":1244,\"y\":547},{\"x\":1275,\"y\":547},{\"x\":1274,\"y\":572},{\"x\":1243,\"y\":572}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":1280,\"y\":547},{\"x\":1293,\"y\":547},{\"x\":1293,\"y\":572},{\"x\":1279,\"y\":572}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1298,\"y\":547},{\"x\":1325,\"y\":547},{\"x\":1324,\"y\":572},{\"x\":1298,\"y\":572}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1330,\"y\":547},{\"x\":1358,\"y\":547},{\"x\":1358,\"y\":572},{\"x\":1329,\"y\":572}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":1363,\"y\":547},{\"x\":1386,\"y\":546},{\"x\":1387,\"y\":572},{\"x\":1363,\"y\":572}],\"text\":\"1}\"},{\"boundingBox\":[{\"x\":1156,\"y\":572},{\"x\":1193,\"y\":573},{\"x\":1192,\"y\":596},{\"x\":1155,\"y\":596}],\"text\":\"fully\"},{\"boundingBox\":[{\"x\":1197,\"y\":573},{\"x\":1299,\"y\":574},{\"x\":1298,\"y\":597},{\"x\":1196,\"y\":596}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":1304,\"y\":574},{\"x\":1369,\"y\":574},{\"x\":1369,\"y\":598},{\"x\":1303,\"y\":597}],\"text\":\"global\"},{\"boundingBox\":[{\"x\":11,\"y\":634},{\"x\":11,\"y\":552},{\"x\":38,\"y\":552},{\"x\":37,\"y\":635}],\"text\":\"Aberer\"},{\"boundingBox\":[{\"x\":12,\"y\":546},{\"x\":12,\"y\":532},{\"x\":39,\"y\":531},{\"x\":38,\"y\":546}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":40,\"y\":647},{\"x\":43,\"y\":513},{\"x\":68,\"y\":512},{\"x\":66,\"y\":650}],\"text\":\"Despotovic\"},{\"boundingBox\":[{\"x\":95,\"y\":711},{\"x\":162,\"y\":711},{\"x\":161,\"y\":739},{\"x\":94,\"y\":738}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":168,\"y\":711},{\"x\":198,\"y\":711},{\"x\":197,\"y\":740},{\"x\":167,\"y\":739}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":203,\"y\":711},{\"x\":216,\"y\":711},{\"x\":216,\"y\":740},{\"x\":202,\"y\":740}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":222,\"y\":711},{\"x\":248,\"y\":711},{\"x\":247,\"y\":740},{\"x\":221,\"y\":740}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":254,\"y\":710},{\"x\":280,\"y\":710},{\"x\":279,\"y\":739},{\"x\":253,\"y\":740}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":285,\"y\":710},{\"x\":314,\"y\":710},{\"x\":313,\"y\":739},{\"x\":285,\"y\":739}],\"text\":\"1}\"},{\"boundingBox\":[{\"x\":344,\"y\":711},{\"x\":440,\"y\":710},{\"x\":439,\"y\":734},{\"x\":342,\"y\":732}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":444,\"y\":710},{\"x\":515,\"y\":711},{\"x\":514,\"y\":735},{\"x\":443,\"y\":734}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":1157,\"y\":688},{\"x\":1231,\"y\":688},{\"x\":1230,\"y\":712},{\"x\":1157,\"y\":713}],\"text\":\"Ouput:\"},{\"boundingBox\":[{\"x\":1236,\"y\":688},{\"x\":1268,\"y\":687},{\"x\":1267,\"y\":712},{\"x\":1235,\"y\":712}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":1273,\"y\":687},{\"x\":1286,\"y\":687},{\"x\":1285,\"y\":712},{\"x\":1272,\"y\":712}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1291,\"y\":687},{\"x\":1319,\"y\":687},{\"x\":1317,\"y\":712},{\"x\":1290,\"y\":712}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1324,\"y\":687},{\"x\":1351,\"y\":687},{\"x\":1350,\"y\":713},{\"x\":1322,\"y\":712}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":1356,\"y\":687},{\"x\":1379,\"y\":687},{\"x\":1377,\"y\":713},{\"x\":1354,\"y\":713}],\"text\":\"1}\"},{\"boundingBox\":[{\"x\":608,\"y\":711},{\"x\":650,\"y\":709},{\"x\":651,\"y\":731},{\"x\":609,\"y\":733}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":828,\"y\":712},{\"x\":853,\"y\":710},{\"x\":855,\"y\":731},{\"x\":829,\"y\":733}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":712},{\"x\":1042,\"y\":710},{\"x\":1044,\"y\":731},{\"x\":1023,\"y\":733}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1156,\"y\":716},{\"x\":1229,\"y\":715},{\"x\":1228,\"y\":738},{\"x\":1156,\"y\":740}],\"text\":\"partially\"},{\"boundingBox\":[{\"x\":1233,\"y\":714},{\"x\":1340,\"y\":715},{\"x\":1339,\"y\":738},{\"x\":1233,\"y\":738}],\"text\":\"connected,\"},{\"boundingBox\":[{\"x\":1345,\"y\":715},{\"x\":1396,\"y\":716},{\"x\":1395,\"y\":739},{\"x\":1344,\"y\":738}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":95,\"y\":870},{\"x\":156,\"y\":871},{\"x\":156,\"y\":897},{\"x\":95,\"y\":896}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":162,\"y\":871},{\"x\":174,\"y\":871},{\"x\":173,\"y\":898},{\"x\":161,\"y\":897}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":179,\"y\":871},{\"x\":192,\"y\":871},{\"x\":191,\"y\":898},{\"x\":179,\"y\":898}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":197,\"y\":871},{\"x\":212,\"y\":871},{\"x\":211,\"y\":898},{\"x\":196,\"y\":898}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":218,\"y\":870},{\"x\":243,\"y\":870},{\"x\":242,\"y\":897},{\"x\":217,\"y\":898}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":248,\"y\":870},{\"x\":274,\"y\":870},{\"x\":273,\"y\":897},{\"x\":247,\"y\":897}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":279,\"y\":870},{\"x\":313,\"y\":869},{\"x\":312,\"y\":896},{\"x\":278,\"y\":897}],\"text\":\"1]*\"},{\"boundingBox\":[{\"x\":344,\"y\":870},{\"x\":439,\"y\":870},{\"x\":439,\"y\":894},{\"x\":344,\"y\":891}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":444,\"y\":870},{\"x\":515,\"y\":871},{\"x\":515,\"y\":897},{\"x\":444,\"y\":894}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":609,\"y\":870},{\"x\":652,\"y\":870},{\"x\":652,\"y\":891},{\"x\":609,\"y\":891}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":872},{\"x\":853,\"y\":869},{\"x\":854,\"y\":889},{\"x\":827,\"y\":892}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1023,\"y\":871},{\"x\":1045,\"y\":869},{\"x\":1047,\"y\":892},{\"x\":1024,\"y\":894}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1144,\"y\":857},{\"x\":1222,\"y\":858},{\"x\":1221,\"y\":881},{\"x\":1144,\"y\":880}],\"text\":\"Ouput:\"},{\"boundingBox\":[{\"x\":1227,\"y\":858},{\"x\":1259,\"y\":858},{\"x\":1258,\"y\":881},{\"x\":1226,\"y\":881}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":1264,\"y\":858},{\"x\":1277,\"y\":858},{\"x\":1276,\"y\":881},{\"x\":1263,\"y\":881}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1282,\"y\":858},{\"x\":1309,\"y\":858},{\"x\":1308,\"y\":881},{\"x\":1281,\"y\":881}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1314,\"y\":858},{\"x\":1340,\"y\":858},{\"x\":1338,\"y\":881},{\"x\":1313,\"y\":881}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1344,\"y\":858},{\"x\":1366,\"y\":857},{\"x\":1364,\"y\":881},{\"x\":1343,\"y\":881}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":1145,\"y\":883},{\"x\":1191,\"y\":884},{\"x\":1190,\"y\":907},{\"x\":1144,\"y\":907}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":1195,\"y\":884},{\"x\":1277,\"y\":885},{\"x\":1276,\"y\":907},{\"x\":1194,\"y\":907}],\"text\":\"absolute\"},{\"boundingBox\":[{\"x\":1281,\"y\":885},{\"x\":1321,\"y\":885},{\"x\":1321,\"y\":907},{\"x\":1281,\"y\":907}],\"text\":\"rep.\"},{\"boundingBox\":[{\"x\":1326,\"y\":885},{\"x\":1392,\"y\":886},{\"x\":1392,\"y\":908},{\"x\":1325,\"y\":907}],\"text\":\"scores\"},{\"boundingBox\":[{\"x\":24,\"y\":931},{\"x\":25,\"y\":827},{\"x\":52,\"y\":827},{\"x\":51,\"y\":930}],\"text\":\"TRAVOS\"},{\"boundingBox\":[{\"x\":26,\"y\":807},{\"x\":28,\"y\":671},{\"x\":53,\"y\":671},{\"x\":52,\"y\":807}],\"text\":\"|Advogato\"},{\"boundingBox\":[{\"x\":95,\"y\":1003},{\"x\":156,\"y\":1002},{\"x\":155,\"y\":1028},{\"x\":94,\"y\":1029}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":161,\"y\":1002},{\"x\":175,\"y\":1002},{\"x\":174,\"y\":1028},{\"x\":161,\"y\":1028}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":180,\"y\":1002},{\"x\":193,\"y\":1002},{\"x\":193,\"y\":1028},{\"x\":179,\"y\":1028}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":199,\"y\":1002},{\"x\":212,\"y\":1002},{\"x\":212,\"y\":1028},{\"x\":198,\"y\":1028}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":217,\"y\":1002},{\"x\":243,\"y\":1002},{\"x\":243,\"y\":1028},{\"x\":217,\"y\":1028}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":248,\"y\":1002},{\"x\":274,\"y\":1002},{\"x\":274,\"y\":1028},{\"x\":248,\"y\":1028}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":279,\"y\":1002},{\"x\":301,\"y\":1002},{\"x\":301,\"y\":1029},{\"x\":279,\"y\":1028}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":97,\"y\":1031},{\"x\":167,\"y\":1030},{\"x\":166,\"y\":1053},{\"x\":96,\"y\":1051}],\"text\":\"relative\"},{\"boundingBox\":[{\"x\":172,\"y\":1030},{\"x\":210,\"y\":1030},{\"x\":209,\"y\":1053},{\"x\":171,\"y\":1053}],\"text\":\"rep.\"},{\"boundingBox\":[{\"x\":214,\"y\":1030},{\"x\":280,\"y\":1030},{\"x\":280,\"y\":1053},{\"x\":214,\"y\":1053}],\"text\":\"scores\"},{\"boundingBox\":[{\"x\":343,\"y\":1014},{\"x\":440,\"y\":1014},{\"x\":440,\"y\":1037},{\"x\":341,\"y\":1036}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":445,\"y\":1014},{\"x\":516,\"y\":1015},{\"x\":516,\"y\":1040},{\"x\":444,\"y\":1037}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":609,\"y\":1014},{\"x\":651,\"y\":1014},{\"x\":651,\"y\":1035},{\"x\":609,\"y\":1035}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":825,\"y\":1014},{\"x\":853,\"y\":1013},{\"x\":854,\"y\":1035},{\"x\":826,\"y\":1036}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1020,\"y\":1016},{\"x\":1044,\"y\":1014},{\"x\":1045,\"y\":1035},{\"x\":1022,\"y\":1037}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1145,\"y\":1003},{\"x\":1223,\"y\":1003},{\"x\":1222,\"y\":1027},{\"x\":1144,\"y\":1027}],\"text\":\"Output:\"},{\"boundingBox\":[{\"x\":1228,\"y\":1003},{\"x\":1241,\"y\":1003},{\"x\":1241,\"y\":1028},{\"x\":1227,\"y\":1027}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1246,\"y\":1003},{\"x\":1260,\"y\":1003},{\"x\":1259,\"y\":1028},{\"x\":1246,\"y\":1028}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1265,\"y\":1003},{\"x\":1278,\"y\":1003},{\"x\":1277,\"y\":1028},{\"x\":1264,\"y\":1028}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1283,\"y\":1003},{\"x\":1310,\"y\":1002},{\"x\":1309,\"y\":1028},{\"x\":1282,\"y\":1028}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1315,\"y\":1002},{\"x\":1341,\"y\":1002},{\"x\":1341,\"y\":1029},{\"x\":1314,\"y\":1028}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":1346,\"y\":1002},{\"x\":1371,\"y\":1001},{\"x\":1371,\"y\":1029},{\"x\":1346,\"y\":1029}],\"text\":\"1}\"},{\"boundingBox\":[{\"x\":1144,\"y\":1030},{\"x\":1218,\"y\":1030},{\"x\":1219,\"y\":1053},{\"x\":1145,\"y\":1053}],\"text\":\"partially\"},{\"boundingBox\":[{\"x\":1223,\"y\":1030},{\"x\":1326,\"y\":1029},{\"x\":1327,\"y\":1052},{\"x\":1223,\"y\":1053}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":96,\"y\":1162},{\"x\":158,\"y\":1161},{\"x\":157,\"y\":1185},{\"x\":95,\"y\":1185}],\"text\":\"Input:\"},{\"boundingBox\":[{\"x\":163,\"y\":1161},{\"x\":176,\"y\":1161},{\"x\":175,\"y\":1185},{\"x\":162,\"y\":1185}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":180,\"y\":1161},{\"x\":193,\"y\":1161},{\"x\":193,\"y\":1185},{\"x\":180,\"y\":1185}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":198,\"y\":1161},{\"x\":212,\"y\":1161},{\"x\":211,\"y\":1186},{\"x\":197,\"y\":1186}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":217,\"y\":1161},{\"x\":243,\"y\":1161},{\"x\":242,\"y\":1186},{\"x\":217,\"y\":1186}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":248,\"y\":1161},{\"x\":275,\"y\":1161},{\"x\":275,\"y\":1186},{\"x\":247,\"y\":1186}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":280,\"y\":1161},{\"x\":299,\"y\":1161},{\"x\":299,\"y\":1187},{\"x\":279,\"y\":1186}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":105,\"y\":1188},{\"x\":184,\"y\":1188},{\"x\":184,\"y\":1211},{\"x\":105,\"y\":1209}],\"text\":\"absolute\"},{\"boundingBox\":[{\"x\":189,\"y\":1188},{\"x\":228,\"y\":1188},{\"x\":228,\"y\":1211},{\"x\":189,\"y\":1211}],\"text\":\"rep.\"},{\"boundingBox\":[{\"x\":232,\"y\":1188},{\"x\":299,\"y\":1187},{\"x\":299,\"y\":1210},{\"x\":232,\"y\":1211}],\"text\":\"scores\"},{\"boundingBox\":[{\"x\":343,\"y\":1173},{\"x\":440,\"y\":1172},{\"x\":439,\"y\":1194},{\"x\":341,\"y\":1194}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":444,\"y\":1172},{\"x\":513,\"y\":1173},{\"x\":513,\"y\":1196},{\"x\":444,\"y\":1194}],\"text\":\"History\"},{\"boundingBox\":[{\"x\":610,\"y\":1172},{\"x\":651,\"y\":1172},{\"x\":651,\"y\":1192},{\"x\":610,\"y\":1192}],\"text\":\"FHG\"},{\"boundingBox\":[{\"x\":826,\"y\":1172},{\"x\":853,\"y\":1171},{\"x\":854,\"y\":1192},{\"x\":827,\"y\":1194}],\"text\":\"RG\"},{\"boundingBox\":[{\"x\":1021,\"y\":1173},{\"x\":1043,\"y\":1172},{\"x\":1044,\"y\":1191},{\"x\":1022,\"y\":1193}],\"text\":\"TG\"},{\"boundingBox\":[{\"x\":1156,\"y\":1160},{\"x\":1232,\"y\":1160},{\"x\":1231,\"y\":1186},{\"x\":1155,\"y\":1184}],\"text\":\"Output:\"},{\"boundingBox\":[{\"x\":1237,\"y\":1160},{\"x\":1250,\"y\":1160},{\"x\":1250,\"y\":1186},{\"x\":1237,\"y\":1186}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1255,\"y\":1160},{\"x\":1267,\"y\":1160},{\"x\":1267,\"y\":1186},{\"x\":1255,\"y\":1186}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1272,\"y\":1160},{\"x\":1286,\"y\":1160},{\"x\":1286,\"y\":1186},{\"x\":1272,\"y\":1186}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1291,\"y\":1160},{\"x\":1317,\"y\":1159},{\"x\":1317,\"y\":1187},{\"x\":1291,\"y\":1186}],\"text\":\"=>\"},{\"boundingBox\":[{\"x\":1322,\"y\":1159},{\"x\":1349,\"y\":1159},{\"x\":1349,\"y\":1187},{\"x\":1322,\"y\":1187}],\"text\":\"{0,\"},{\"boundingBox\":[{\"x\":1354,\"y\":1159},{\"x\":1381,\"y\":1159},{\"x\":1381,\"y\":1186},{\"x\":1354,\"y\":1187}],\"text\":\"1}\"},{\"boundingBox\":[{\"x\":1157,\"y\":1188},{\"x\":1229,\"y\":1188},{\"x\":1228,\"y\":1212},{\"x\":1156,\"y\":1210}],\"text\":\"partially\"},{\"boundingBox\":[{\"x\":1234,\"y\":1188},{\"x\":1335,\"y\":1185},{\"x\":1335,\"y\":1208},{\"x\":1233,\"y\":1212}],\"text\":\"connected\"},{\"boundingBox\":[{\"x\":26,\"y\":1259},{\"x\":26,\"y\":1111},{\"x\":54,\"y\":1111},{\"x\":52,\"y\":1259}],\"text\":\"Thresholding\"},{\"boundingBox\":[{\"x\":26,\"y\":1082},{\"x\":26,\"y\":983},{\"x\":55,\"y\":984},{\"x\":54,\"y\":1082}],\"text\":\"Ranking\"}]}",
        "{\"language\":\"en\",\"text\":\"FHGO PeerTrust Preconditions: A x A + [0,1]* Post-conditions: RG2 Discretizer Fully connected, global, Preconditions: A x A + [0, 1]* absolute r(a,b) A x A ++ [0, 1] Post-conditions Normalizer Ax A ++ {-1,1}* Preconditions: A x A ++ [0, 1] Post-conditions: Fully connected, global, FHG1 relative r(a, b) A x A ++ [0, 1] EigenTrust Preconditions: RG3 Ax A ++ {-1,1}* RG1 Post-conditions: Fully connected, global, relative r(a, b) Spearman A x A ++ [0, 1] Correlation coefficient\",\"lines\":[{\"boundingBox\":[{\"x\":100,\"y\":36},{\"x\":182,\"y\":37},{\"x\":182,\"y\":67},{\"x\":98,\"y\":65}],\"text\":\"FHGO\"},{\"boundingBox\":[{\"x\":616,\"y\":60},{\"x\":768,\"y\":60},{\"x\":768,\"y\":93},{\"x\":616,\"y\":92}],\"text\":\"PeerTrust\"},{\"boundingBox\":[{\"x\":503,\"y\":105},{\"x\":728,\"y\":105},{\"x\":728,\"y\":138},{\"x\":503,\"y\":137}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":507,\"y\":147},{\"x\":718,\"y\":148},{\"x\":717,\"y\":185},{\"x\":507,\"y\":182}],\"text\":\"A x A + [0,1]*\"},{\"boundingBox\":[{\"x\":506,\"y\":194},{\"x\":757,\"y\":195},{\"x\":757,\"y\":228},{\"x\":506,\"y\":227}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1036,\"y\":172},{\"x\":1103,\"y\":174},{\"x\":1103,\"y\":203},{\"x\":1037,\"y\":201}],\"text\":\"RG2\"},{\"boundingBox\":[{\"x\":70,\"y\":230},{\"x\":234,\"y\":231},{\"x\":234,\"y\":260},{\"x\":70,\"y\":259}],\"text\":\"Discretizer\"},{\"boundingBox\":[{\"x\":502,\"y\":238},{\"x\":866,\"y\":238},{\"x\":866,\"y\":276},{\"x\":502,\"y\":276}],\"text\":\"Fully connected, global,\"},{\"boundingBox\":[{\"x\":20,\"y\":274},{\"x\":242,\"y\":275},{\"x\":242,\"y\":305},{\"x\":20,\"y\":304}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":24,\"y\":306},{\"x\":237,\"y\":308},{\"x\":236,\"y\":345},{\"x\":23,\"y\":340}],\"text\":\"A x A + [0, 1]*\"},{\"boundingBox\":[{\"x\":503,\"y\":282},{\"x\":733,\"y\":283},{\"x\":732,\"y\":322},{\"x\":503,\"y\":320}],\"text\":\"absolute r(a,b)\"},{\"boundingBox\":[{\"x\":519,\"y\":329},{\"x\":741,\"y\":330},{\"x\":741,\"y\":362},{\"x\":519,\"y\":360}],\"text\":\"A x A ++ [0, 1]\"},{\"boundingBox\":[{\"x\":17,\"y\":362},{\"x\":261,\"y\":363},{\"x\":261,\"y\":396},{\"x\":17,\"y\":395}],\"text\":\"Post-conditions\"},{\"boundingBox\":[{\"x\":1109,\"y\":352},{\"x\":1280,\"y\":352},{\"x\":1280,\"y\":388},{\"x\":1109,\"y\":387}],\"text\":\"Normalizer\"},{\"boundingBox\":[{\"x\":14,\"y\":403},{\"x\":251,\"y\":405},{\"x\":251,\"y\":440},{\"x\":14,\"y\":438}],\"text\":\"Ax A ++ {-1,1}*\"},{\"boundingBox\":[{\"x\":989,\"y\":399},{\"x\":1214,\"y\":399},{\"x\":1214,\"y\":432},{\"x\":989,\"y\":431}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1005,\"y\":441},{\"x\":1229,\"y\":443},{\"x\":1229,\"y\":476},{\"x\":1005,\"y\":473}],\"text\":\"A x A ++ [0, 1]\"},{\"boundingBox\":[{\"x\":990,\"y\":488},{\"x\":1241,\"y\":490},{\"x\":1241,\"y\":523},{\"x\":989,\"y\":520}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":989,\"y\":531},{\"x\":1352,\"y\":533},{\"x\":1351,\"y\":573},{\"x\":989,\"y\":570}],\"text\":\"Fully connected, global,\"},{\"boundingBox\":[{\"x\":196,\"y\":579},{\"x\":282,\"y\":579},{\"x\":282,\"y\":609},{\"x\":196,\"y\":608}],\"text\":\"FHG1\"},{\"boundingBox\":[{\"x\":990,\"y\":577},{\"x\":1208,\"y\":579},{\"x\":1207,\"y\":616},{\"x\":990,\"y\":613}],\"text\":\"relative r(a, b)\"},{\"boundingBox\":[{\"x\":1012,\"y\":628},{\"x\":1236,\"y\":630},{\"x\":1235,\"y\":665},{\"x\":1012,\"y\":661}],\"text\":\"A x A ++ [0, 1]\"},{\"boundingBox\":[{\"x\":156,\"y\":733},{\"x\":321,\"y\":732},{\"x\":322,\"y\":769},{\"x\":156,\"y\":770}],\"text\":\"EigenTrust\"},{\"boundingBox\":[{\"x\":34,\"y\":778},{\"x\":261,\"y\":778},{\"x\":261,\"y\":812},{\"x\":34,\"y\":812}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1130,\"y\":787},{\"x\":1197,\"y\":789},{\"x\":1197,\"y\":821},{\"x\":1131,\"y\":820}],\"text\":\"RG3\"},{\"boundingBox\":[{\"x\":67,\"y\":817},{\"x\":299,\"y\":818},{\"x\":299,\"y\":853},{\"x\":67,\"y\":852}],\"text\":\"Ax A ++ {-1,1}*\"},{\"boundingBox\":[{\"x\":659,\"y\":792},{\"x\":727,\"y\":791},{\"x\":727,\"y\":823},{\"x\":659,\"y\":823}],\"text\":\"RG1\"},{\"boundingBox\":[{\"x\":33,\"y\":867},{\"x\":288,\"y\":868},{\"x\":288,\"y\":902},{\"x\":33,\"y\":901}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":36,\"y\":910},{\"x\":399,\"y\":910},{\"x\":399,\"y\":949},{\"x\":36,\"y\":949}],\"text\":\"Fully connected, global,\"},{\"boundingBox\":[{\"x\":36,\"y\":956},{\"x\":254,\"y\":958},{\"x\":253,\"y\":995},{\"x\":35,\"y\":992}],\"text\":\"relative r(a, b)\"},{\"boundingBox\":[{\"x\":859,\"y\":943},{\"x\":1017,\"y\":942},{\"x\":1017,\"y\":975},{\"x\":859,\"y\":977}],\"text\":\"Spearman\"},{\"boundingBox\":[{\"x\":49,\"y\":1007},{\"x\":280,\"y\":1008},{\"x\":279,\"y\":1042},{\"x\":48,\"y\":1041}],\"text\":\"A x A ++ [0, 1]\"},{\"boundingBox\":[{\"x\":1154,\"y\":999},{\"x\":1330,\"y\":999},{\"x\":1329,\"y\":1033},{\"x\":1154,\"y\":1031}],\"text\":\"Correlation\"},{\"boundingBox\":[{\"x\":1158,\"y\":1045},{\"x\":1324,\"y\":1045},{\"x\":1325,\"y\":1077},{\"x\":1158,\"y\":1078}],\"text\":\"coefficient\"}],\"words\":[{\"boundingBox\":[{\"x\":98,\"y\":36},{\"x\":177,\"y\":37},{\"x\":176,\"y\":67},{\"x\":98,\"y\":65}],\"text\":\"FHGO\"},{\"boundingBox\":[{\"x\":618,\"y\":60},{\"x\":768,\"y\":61},{\"x\":768,\"y\":94},{\"x\":616,\"y\":92}],\"text\":\"PeerTrust\"},{\"boundingBox\":[{\"x\":504,\"y\":105},{\"x\":729,\"y\":106},{\"x\":728,\"y\":139},{\"x\":503,\"y\":138}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":509,\"y\":148},{\"x\":529,\"y\":148},{\"x\":529,\"y\":182},{\"x\":509,\"y\":182}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":537,\"y\":148},{\"x\":557,\"y\":148},{\"x\":556,\"y\":182},{\"x\":536,\"y\":182}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":571,\"y\":148},{\"x\":591,\"y\":148},{\"x\":591,\"y\":183},{\"x\":571,\"y\":182}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":599,\"y\":148},{\"x\":619,\"y\":148},{\"x\":618,\"y\":183},{\"x\":598,\"y\":183}],\"text\":\"+\"},{\"boundingBox\":[{\"x\":643,\"y\":149},{\"x\":719,\"y\":149},{\"x\":718,\"y\":186},{\"x\":642,\"y\":184}],\"text\":\"[0,1]*\"},{\"boundingBox\":[{\"x\":507,\"y\":195},{\"x\":756,\"y\":196},{\"x\":757,\"y\":229},{\"x\":506,\"y\":228}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":1037,\"y\":172},{\"x\":1099,\"y\":174},{\"x\":1098,\"y\":203},{\"x\":1037,\"y\":201}],\"text\":\"RG2\"},{\"boundingBox\":[{\"x\":71,\"y\":231},{\"x\":234,\"y\":231},{\"x\":234,\"y\":260},{\"x\":71,\"y\":260}],\"text\":\"Discretizer\"},{\"boundingBox\":[{\"x\":504,\"y\":239},{\"x\":575,\"y\":239},{\"x\":574,\"y\":276},{\"x\":502,\"y\":276}],\"text\":\"Fully\"},{\"boundingBox\":[{\"x\":583,\"y\":239},{\"x\":754,\"y\":239},{\"x\":754,\"y\":277},{\"x\":582,\"y\":277}],\"text\":\"connected,\"},{\"boundingBox\":[{\"x\":762,\"y\":239},{\"x\":867,\"y\":239},{\"x\":867,\"y\":277},{\"x\":761,\"y\":277}],\"text\":\"global,\"},{\"boundingBox\":[{\"x\":22,\"y\":274},{\"x\":241,\"y\":276},{\"x\":242,\"y\":306},{\"x\":20,\"y\":305}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":26,\"y\":307},{\"x\":45,\"y\":307},{\"x\":45,\"y\":339},{\"x\":25,\"y\":339}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":53,\"y\":307},{\"x\":72,\"y\":307},{\"x\":72,\"y\":340},{\"x\":52,\"y\":339}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":86,\"y\":307},{\"x\":106,\"y\":308},{\"x\":105,\"y\":341},{\"x\":86,\"y\":340}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":115,\"y\":308},{\"x\":135,\"y\":308},{\"x\":134,\"y\":341},{\"x\":115,\"y\":341}],\"text\":\"+\"},{\"boundingBox\":[{\"x\":160,\"y\":308},{\"x\":193,\"y\":308},{\"x\":193,\"y\":344},{\"x\":159,\"y\":342}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":200,\"y\":309},{\"x\":236,\"y\":309},{\"x\":236,\"y\":346},{\"x\":200,\"y\":344}],\"text\":\"1]*\"},{\"boundingBox\":[{\"x\":506,\"y\":283},{\"x\":634,\"y\":283},{\"x\":634,\"y\":321},{\"x\":505,\"y\":321}],\"text\":\"absolute\"},{\"boundingBox\":[{\"x\":642,\"y\":283},{\"x\":731,\"y\":285},{\"x\":732,\"y\":323},{\"x\":642,\"y\":321}],\"text\":\"r(a,b)\"},{\"boundingBox\":[{\"x\":522,\"y\":329},{\"x\":540,\"y\":329},{\"x\":539,\"y\":361},{\"x\":521,\"y\":361}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":553,\"y\":330},{\"x\":572,\"y\":330},{\"x\":571,\"y\":361},{\"x\":552,\"y\":361}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":589,\"y\":330},{\"x\":607,\"y\":330},{\"x\":607,\"y\":362},{\"x\":588,\"y\":362}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":618,\"y\":330},{\"x\":658,\"y\":330},{\"x\":658,\"y\":362},{\"x\":618,\"y\":362}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":665,\"y\":330},{\"x\":704,\"y\":330},{\"x\":704,\"y\":363},{\"x\":664,\"y\":362}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":711,\"y\":330},{\"x\":741,\"y\":330},{\"x\":741,\"y\":363},{\"x\":711,\"y\":363}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":19,\"y\":363},{\"x\":260,\"y\":365},{\"x\":260,\"y\":397},{\"x\":17,\"y\":396}],\"text\":\"Post-conditions\"},{\"boundingBox\":[{\"x\":1110,\"y\":354},{\"x\":1280,\"y\":353},{\"x\":1280,\"y\":389},{\"x\":1110,\"y\":386}],\"text\":\"Normalizer\"},{\"boundingBox\":[{\"x\":19,\"y\":403},{\"x\":65,\"y\":405},{\"x\":65,\"y\":440},{\"x\":19,\"y\":439}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":77,\"y\":405},{\"x\":95,\"y\":405},{\"x\":95,\"y\":440},{\"x\":77,\"y\":440}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":101,\"y\":406},{\"x\":137,\"y\":406},{\"x\":137,\"y\":440},{\"x\":101,\"y\":440}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":144,\"y\":406},{\"x\":252,\"y\":407},{\"x\":251,\"y\":439},{\"x\":143,\"y\":440}],\"text\":\"{-1,1}*\"},{\"boundingBox\":[{\"x\":990,\"y\":400},{\"x\":1213,\"y\":399},{\"x\":1212,\"y\":433},{\"x\":989,\"y\":431}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1010,\"y\":442},{\"x\":1028,\"y\":442},{\"x\":1027,\"y\":473},{\"x\":1010,\"y\":474}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1042,\"y\":442},{\"x\":1060,\"y\":442},{\"x\":1060,\"y\":473},{\"x\":1042,\"y\":473}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1077,\"y\":443},{\"x\":1095,\"y\":443},{\"x\":1095,\"y\":473},{\"x\":1077,\"y\":473}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1108,\"y\":443},{\"x\":1149,\"y\":443},{\"x\":1149,\"y\":474},{\"x\":1108,\"y\":473}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":1155,\"y\":443},{\"x\":1195,\"y\":444},{\"x\":1194,\"y\":475},{\"x\":1155,\"y\":474}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1201,\"y\":444},{\"x\":1230,\"y\":444},{\"x\":1229,\"y\":477},{\"x\":1200,\"y\":476}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":991,\"y\":488},{\"x\":1241,\"y\":491},{\"x\":1240,\"y\":524},{\"x\":990,\"y\":521}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":990,\"y\":532},{\"x\":1058,\"y\":532},{\"x\":1058,\"y\":570},{\"x\":990,\"y\":570}],\"text\":\"Fully\"},{\"boundingBox\":[{\"x\":1066,\"y\":532},{\"x\":1237,\"y\":532},{\"x\":1237,\"y\":572},{\"x\":1066,\"y\":570}],\"text\":\"connected,\"},{\"boundingBox\":[{\"x\":1245,\"y\":533},{\"x\":1350,\"y\":533},{\"x\":1350,\"y\":574},{\"x\":1245,\"y\":572}],\"text\":\"global,\"},{\"boundingBox\":[{\"x\":197,\"y\":580},{\"x\":279,\"y\":579},{\"x\":280,\"y\":610},{\"x\":197,\"y\":608}],\"text\":\"FHG1\"},{\"boundingBox\":[{\"x\":990,\"y\":578},{\"x\":1101,\"y\":578},{\"x\":1101,\"y\":614},{\"x\":990,\"y\":614}],\"text\":\"relative\"},{\"boundingBox\":[{\"x\":1108,\"y\":578},{\"x\":1165,\"y\":579},{\"x\":1164,\"y\":615},{\"x\":1108,\"y\":614}],\"text\":\"r(a,\"},{\"boundingBox\":[{\"x\":1172,\"y\":580},{\"x\":1208,\"y\":581},{\"x\":1207,\"y\":617},{\"x\":1171,\"y\":616}],\"text\":\"b)\"},{\"boundingBox\":[{\"x\":1014,\"y\":628},{\"x\":1033,\"y\":628},{\"x\":1033,\"y\":660},{\"x\":1015,\"y\":660}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1047,\"y\":628},{\"x\":1066,\"y\":628},{\"x\":1066,\"y\":660},{\"x\":1047,\"y\":660}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1081,\"y\":629},{\"x\":1100,\"y\":629},{\"x\":1100,\"y\":661},{\"x\":1081,\"y\":661}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":1112,\"y\":629},{\"x\":1153,\"y\":629},{\"x\":1152,\"y\":662},{\"x\":1111,\"y\":661}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":1159,\"y\":629},{\"x\":1198,\"y\":630},{\"x\":1197,\"y\":664},{\"x\":1159,\"y\":663}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":1205,\"y\":630},{\"x\":1236,\"y\":631},{\"x\":1235,\"y\":666},{\"x\":1204,\"y\":665}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":156,\"y\":734},{\"x\":321,\"y\":733},{\"x\":321,\"y\":770},{\"x\":157,\"y\":771}],\"text\":\"EigenTrust\"},{\"boundingBox\":[{\"x\":34,\"y\":778},{\"x\":262,\"y\":779},{\"x\":261,\"y\":812},{\"x\":34,\"y\":813}],\"text\":\"Preconditions:\"},{\"boundingBox\":[{\"x\":1132,\"y\":787},{\"x\":1193,\"y\":788},{\"x\":1192,\"y\":821},{\"x\":1131,\"y\":819}],\"text\":\"RG3\"},{\"boundingBox\":[{\"x\":69,\"y\":817},{\"x\":113,\"y\":818},{\"x\":112,\"y\":853},{\"x\":68,\"y\":853}],\"text\":\"Ax\"},{\"boundingBox\":[{\"x\":125,\"y\":818},{\"x\":144,\"y\":818},{\"x\":143,\"y\":853},{\"x\":124,\"y\":853}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":151,\"y\":818},{\"x\":185,\"y\":818},{\"x\":184,\"y\":853},{\"x\":149,\"y\":853}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":192,\"y\":819},{\"x\":300,\"y\":820},{\"x\":298,\"y\":854},{\"x\":190,\"y\":853}],\"text\":\"{-1,1}*\"},{\"boundingBox\":[{\"x\":659,\"y\":791},{\"x\":722,\"y\":791},{\"x\":722,\"y\":823},{\"x\":659,\"y\":823}],\"text\":\"RG1\"},{\"boundingBox\":[{\"x\":33,\"y\":867},{\"x\":288,\"y\":870},{\"x\":289,\"y\":902},{\"x\":34,\"y\":902}],\"text\":\"Post-conditions:\"},{\"boundingBox\":[{\"x\":38,\"y\":910},{\"x\":106,\"y\":913},{\"x\":104,\"y\":949},{\"x\":36,\"y\":949}],\"text\":\"Fully\"},{\"boundingBox\":[{\"x\":113,\"y\":913},{\"x\":282,\"y\":914},{\"x\":282,\"y\":950},{\"x\":112,\"y\":949}],\"text\":\"connected,\"},{\"boundingBox\":[{\"x\":289,\"y\":914},{\"x\":397,\"y\":910},{\"x\":398,\"y\":950},{\"x\":289,\"y\":950}],\"text\":\"global,\"},{\"boundingBox\":[{\"x\":38,\"y\":957},{\"x\":149,\"y\":957},{\"x\":148,\"y\":995},{\"x\":36,\"y\":993}],\"text\":\"relative\"},{\"boundingBox\":[{\"x\":156,\"y\":957},{\"x\":209,\"y\":958},{\"x\":209,\"y\":995},{\"x\":155,\"y\":995}],\"text\":\"r(a,\"},{\"boundingBox\":[{\"x\":216,\"y\":958},{\"x\":253,\"y\":959},{\"x\":254,\"y\":996},{\"x\":216,\"y\":996}],\"text\":\"b)\"},{\"boundingBox\":[{\"x\":861,\"y\":944},{\"x\":1012,\"y\":944},{\"x\":1012,\"y\":976},{\"x\":860,\"y\":978}],\"text\":\"Spearman\"},{\"boundingBox\":[{\"x\":60,\"y\":1008},{\"x\":79,\"y\":1008},{\"x\":78,\"y\":1041},{\"x\":59,\"y\":1041}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":92,\"y\":1008},{\"x\":111,\"y\":1008},{\"x\":111,\"y\":1041},{\"x\":92,\"y\":1041}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":129,\"y\":1009},{\"x\":148,\"y\":1009},{\"x\":148,\"y\":1041},{\"x\":129,\"y\":1041}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":159,\"y\":1009},{\"x\":198,\"y\":1009},{\"x\":199,\"y\":1041},{\"x\":159,\"y\":1041}],\"text\":\"++\"},{\"boundingBox\":[{\"x\":205,\"y\":1009},{\"x\":244,\"y\":1009},{\"x\":244,\"y\":1042},{\"x\":205,\"y\":1041}],\"text\":\"[0,\"},{\"boundingBox\":[{\"x\":250,\"y\":1009},{\"x\":279,\"y\":1008},{\"x\":280,\"y\":1043},{\"x\":251,\"y\":1042}],\"text\":\"1]\"},{\"boundingBox\":[{\"x\":1155,\"y\":1000},{\"x\":1323,\"y\":1000},{\"x\":1323,\"y\":1034},{\"x\":1155,\"y\":1031}],\"text\":\"Correlation\"},{\"boundingBox\":[{\"x\":1160,\"y\":1047},{\"x\":1324,\"y\":1046},{\"x\":1324,\"y\":1078},{\"x\":1158,\"y\":1079}],\"text\":\"coefficient\"}]}",
        "{\"language\":\"en\",\"text\":\"0 0 {1.0, 1.0, 1.0} 1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} (1.0, 1.0, 0.0} 2 2 [0.0, 0.0, 0.0} 10.0, 0.0, 0.0} 3 3\",\"lines\":[{\"boundingBox\":[{\"x\":76,\"y\":39},{\"x\":97,\"y\":41},{\"x\":95,\"y\":68},{\"x\":75,\"y\":66}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":448,\"y\":43},{\"x\":469,\"y\":41},{\"x\":470,\"y\":67},{\"x\":448,\"y\":69}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":87,\"y\":177},{\"x\":334,\"y\":177},{\"x\":334,\"y\":216},{\"x\":86,\"y\":214}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":471,\"y\":177},{\"x\":707,\"y\":178},{\"x\":706,\"y\":217},{\"x\":470,\"y\":215}],\"text\":\"1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":73,\"y\":318},{\"x\":94,\"y\":318},{\"x\":95,\"y\":345},{\"x\":75,\"y\":345}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":446,\"y\":317},{\"x\":469,\"y\":317},{\"x\":469,\"y\":346},{\"x\":447,\"y\":346}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":86,\"y\":448},{\"x\":335,\"y\":448},{\"x\":335,\"y\":487},{\"x\":86,\"y\":487}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":462,\"y\":449},{\"x\":707,\"y\":450},{\"x\":706,\"y\":486},{\"x\":462,\"y\":485}],\"text\":\"(1.0, 1.0, 0.0}\"},{\"boundingBox\":[{\"x\":76,\"y\":588},{\"x\":95,\"y\":590},{\"x\":93,\"y\":620},{\"x\":73,\"y\":619}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":448,\"y\":586},{\"x\":470,\"y\":589},{\"x\":466,\"y\":618},{\"x\":444,\"y\":616}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":85,\"y\":718},{\"x\":334,\"y\":718},{\"x\":334,\"y\":763},{\"x\":85,\"y\":762}],\"text\":\"[0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":461,\"y\":719},{\"x\":707,\"y\":718},{\"x\":707,\"y\":763},{\"x\":461,\"y\":764}],\"text\":\"10.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":75,\"y\":860},{\"x\":97,\"y\":861},{\"x\":96,\"y\":890},{\"x\":74,\"y\":888}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":447,\"y\":859},{\"x\":469,\"y\":859},{\"x\":469,\"y\":889},{\"x\":446,\"y\":888}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":76,\"y\":39},{\"x\":94,\"y\":41},{\"x\":91,\"y\":67},{\"x\":75,\"y\":66}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":449,\"y\":42},{\"x\":466,\"y\":41},{\"x\":468,\"y\":66},{\"x\":452,\"y\":68}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":88,\"y\":179},{\"x\":173,\"y\":180},{\"x\":173,\"y\":214},{\"x\":87,\"y\":213}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":180,\"y\":180},{\"x\":250,\"y\":179},{\"x\":250,\"y\":215},{\"x\":180,\"y\":214}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":257,\"y\":179},{\"x\":334,\"y\":178},{\"x\":335,\"y\":217},{\"x\":257,\"y\":215}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":476,\"y\":177},{\"x\":544,\"y\":179},{\"x\":543,\"y\":217},{\"x\":475,\"y\":215}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":551,\"y\":179},{\"x\":619,\"y\":179},{\"x\":619,\"y\":218},{\"x\":551,\"y\":217}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":627,\"y\":179},{\"x\":705,\"y\":179},{\"x\":705,\"y\":218},{\"x\":626,\"y\":218}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":74,\"y\":318},{\"x\":90,\"y\":318},{\"x\":90,\"y\":345},{\"x\":74,\"y\":345}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":447,\"y\":317},{\"x\":464,\"y\":317},{\"x\":464,\"y\":346},{\"x\":447,\"y\":346}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":87,\"y\":449},{\"x\":173,\"y\":451},{\"x\":172,\"y\":487},{\"x\":87,\"y\":487}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":180,\"y\":451},{\"x\":249,\"y\":450},{\"x\":248,\"y\":487},{\"x\":179,\"y\":487}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":256,\"y\":450},{\"x\":334,\"y\":449},{\"x\":333,\"y\":488},{\"x\":255,\"y\":487}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":462,\"y\":450},{\"x\":543,\"y\":450},{\"x\":543,\"y\":486},{\"x\":462,\"y\":486}],\"text\":\"(1.0,\"},{\"boundingBox\":[{\"x\":550,\"y\":450},{\"x\":619,\"y\":450},{\"x\":619,\"y\":486},{\"x\":550,\"y\":486}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":627,\"y\":450},{\"x\":706,\"y\":450},{\"x\":706,\"y\":486},{\"x\":627,\"y\":486}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":75,\"y\":588},{\"x\":92,\"y\":589},{\"x\":89,\"y\":620},{\"x\":73,\"y\":618}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":448,\"y\":586},{\"x\":467,\"y\":587},{\"x\":464,\"y\":617},{\"x\":444,\"y\":615}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":86,\"y\":721},{\"x\":167,\"y\":720},{\"x\":167,\"y\":763},{\"x\":85,\"y\":760}],\"text\":\"[0.0,\"},{\"boundingBox\":[{\"x\":176,\"y\":720},{\"x\":246,\"y\":720},{\"x\":246,\"y\":764},{\"x\":175,\"y\":763}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":255,\"y\":719},{\"x\":334,\"y\":718},{\"x\":335,\"y\":764},{\"x\":255,\"y\":764}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":462,\"y\":721},{\"x\":540,\"y\":720},{\"x\":539,\"y\":764},{\"x\":462,\"y\":762}],\"text\":\"10.0,\"},{\"boundingBox\":[{\"x\":548,\"y\":720},{\"x\":615,\"y\":719},{\"x\":615,\"y\":764},{\"x\":548,\"y\":764}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":624,\"y\":719},{\"x\":705,\"y\":719},{\"x\":705,\"y\":764},{\"x\":623,\"y\":764}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":76,\"y\":860},{\"x\":93,\"y\":861},{\"x\":91,\"y\":890},{\"x\":74,\"y\":888}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":448,\"y\":859},{\"x\":465,\"y\":859},{\"x\":464,\"y\":889},{\"x\":447,\"y\":888}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 0 0 0.22 (1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 0.3 0.22 1 1 1 0.3 0.22 0.12 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} K1.0, 1.0, 1.0} 0.3 0.12 0.36 0.22 2 2 0.36 0.3 3 0.12 K0.0, 0.0, 0.0} {0.0, 0.0, 0.0} 0.36 /0.12 3 3 2 00.36\",\"lines\":[{\"boundingBox\":[{\"x\":45,\"y\":24},{\"x\":60,\"y\":25},{\"x\":60,\"y\":43},{\"x\":45,\"y\":43}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":380,\"y\":37},{\"x\":393,\"y\":37},{\"x\":393,\"y\":53},{\"x\":380,\"y\":53}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1016,\"y\":78},{\"x\":1029,\"y\":79},{\"x\":1027,\"y\":93},{\"x\":1014,\"y\":92}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1100,\"y\":77},{\"x\":1143,\"y\":76},{\"x\":1144,\"y\":94},{\"x\":1101,\"y\":95}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":52,\"y\":107},{\"x\":209,\"y\":106},{\"x\":209,\"y\":134},{\"x\":52,\"y\":135}],\"text\":\"(1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":301,\"y\":117},{\"x\":456,\"y\":117},{\"x\":456,\"y\":141},{\"x\":301,\"y\":142}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":822,\"y\":154},{\"x\":854,\"y\":154},{\"x\":854,\"y\":171},{\"x\":821,\"y\":171}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":900,\"y\":154},{\"x\":944,\"y\":153},{\"x\":944,\"y\":172},{\"x\":900,\"y\":173}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":45,\"y\":196},{\"x\":59,\"y\":196},{\"x\":60,\"y\":215},{\"x\":45,\"y\":215}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":281,\"y\":203},{\"x\":295,\"y\":203},{\"x\":295,\"y\":224},{\"x\":282,\"y\":224}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":789,\"y\":232},{\"x\":803,\"y\":233},{\"x\":803,\"y\":250},{\"x\":790,\"y\":250}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":841,\"y\":229},{\"x\":907,\"y\":229},{\"x\":907,\"y\":251},{\"x\":842,\"y\":254}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":968,\"y\":230},{\"x\":1071,\"y\":230},{\"x\":1071,\"y\":249},{\"x\":968,\"y\":249}],\"text\":\"0.22 0.12\"},{\"boundingBox\":[{\"x\":52,\"y\":277},{\"x\":209,\"y\":276},{\"x\":209,\"y\":302},{\"x\":52,\"y\":302}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":286,\"y\":282},{\"x\":439,\"y\":283},{\"x\":439,\"y\":311},{\"x\":286,\"y\":310}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":497,\"y\":284},{\"x\":652,\"y\":284},{\"x\":652,\"y\":309},{\"x\":497,\"y\":310}],\"text\":\"K1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":822,\"y\":309},{\"x\":856,\"y\":306},{\"x\":857,\"y\":327},{\"x\":822,\"y\":329}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":858,\"y\":307},{\"x\":927,\"y\":307},{\"x\":927,\"y\":329},{\"x\":858,\"y\":328}],\"text\":\"0.12\"},{\"boundingBox\":[{\"x\":1086,\"y\":308},{\"x\":1182,\"y\":309},{\"x\":1181,\"y\":328},{\"x\":1086,\"y\":327}],\"text\":\"0.36 0.22\"},{\"boundingBox\":[{\"x\":44,\"y\":362},{\"x\":60,\"y\":364},{\"x\":61,\"y\":386},{\"x\":44,\"y\":385}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":279,\"y\":368},{\"x\":296,\"y\":368},{\"x\":296,\"y\":392},{\"x\":279,\"y\":391}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":686,\"y\":385},{\"x\":730,\"y\":386},{\"x\":730,\"y\":405},{\"x\":686,\"y\":404}],\"text\":\"0.36\"},{\"boundingBox\":[{\"x\":820,\"y\":386},{\"x\":853,\"y\":385},{\"x\":856,\"y\":405},{\"x\":821,\"y\":404}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":929,\"y\":385},{\"x\":944,\"y\":385},{\"x\":945,\"y\":405},{\"x\":930,\"y\":404}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1016,\"y\":379},{\"x\":1060,\"y\":381},{\"x\":1060,\"y\":408},{\"x\":1016,\"y\":406}],\"text\":\"0.12\"},{\"boundingBox\":[{\"x\":51,\"y\":448},{\"x\":209,\"y\":447},{\"x\":209,\"y\":475},{\"x\":51,\"y\":476}],\"text\":\"K0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":301,\"y\":451},{\"x\":455,\"y\":450},{\"x\":455,\"y\":476},{\"x\":301,\"y\":477}],\"text\":\"{0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":864,\"y\":460},{\"x\":907,\"y\":461},{\"x\":908,\"y\":484},{\"x\":864,\"y\":484}],\"text\":\"0.36\"},{\"boundingBox\":[{\"x\":914,\"y\":464},{\"x\":965,\"y\":462},{\"x\":965,\"y\":480},{\"x\":914,\"y\":482}],\"text\":\"/0.12\"},{\"boundingBox\":[{\"x\":44,\"y\":533},{\"x\":60,\"y\":533},{\"x\":61,\"y\":554},{\"x\":45,\"y\":554}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":380,\"y\":535},{\"x\":395,\"y\":535},{\"x\":395,\"y\":555},{\"x\":379,\"y\":555}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":893,\"y\":540},{\"x\":907,\"y\":541},{\"x\":907,\"y\":558},{\"x\":893,\"y\":556}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":972,\"y\":540},{\"x\":1023,\"y\":539},{\"x\":1025,\"y\":558},{\"x\":973,\"y\":561}],\"text\":\"00.36\"}],\"words\":[{\"boundingBox\":[{\"x\":46,\"y\":24},{\"x\":57,\"y\":24},{\"x\":56,\"y\":43},{\"x\":45,\"y\":42}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":380,\"y\":37},{\"x\":389,\"y\":37},{\"x\":389,\"y\":53},{\"x\":380,\"y\":53}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1019,\"y\":78},{\"x\":1028,\"y\":79},{\"x\":1027,\"y\":93},{\"x\":1018,\"y\":92}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":1102,\"y\":77},{\"x\":1141,\"y\":76},{\"x\":1141,\"y\":94},{\"x\":1102,\"y\":95}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":53,\"y\":108},{\"x\":104,\"y\":107},{\"x\":103,\"y\":135},{\"x\":53,\"y\":135}],\"text\":\"(1.0,\"},{\"boundingBox\":[{\"x\":109,\"y\":107},{\"x\":151,\"y\":107},{\"x\":151,\"y\":135},{\"x\":109,\"y\":135}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":157,\"y\":107},{\"x\":209,\"y\":106},{\"x\":208,\"y\":135},{\"x\":157,\"y\":135}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":302,\"y\":119},{\"x\":355,\"y\":119},{\"x\":356,\"y\":142},{\"x\":302,\"y\":141}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":360,\"y\":119},{\"x\":401,\"y\":119},{\"x\":402,\"y\":142},{\"x\":360,\"y\":142}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":405,\"y\":119},{\"x\":454,\"y\":117},{\"x\":455,\"y\":142},{\"x\":406,\"y\":142}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":822,\"y\":154},{\"x\":851,\"y\":154},{\"x\":851,\"y\":171},{\"x\":822,\"y\":171}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":900,\"y\":154},{\"x\":942,\"y\":153},{\"x\":942,\"y\":172},{\"x\":901,\"y\":173}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":46,\"y\":196},{\"x\":57,\"y\":196},{\"x\":57,\"y\":215},{\"x\":46,\"y\":215}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":282,\"y\":203},{\"x\":294,\"y\":203},{\"x\":294,\"y\":224},{\"x\":282,\"y\":224}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":790,\"y\":232},{\"x\":800,\"y\":232},{\"x\":800,\"y\":250},{\"x\":789,\"y\":249}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":871,\"y\":229},{\"x\":904,\"y\":229},{\"x\":904,\"y\":253},{\"x\":871,\"y\":253}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":969,\"y\":230},{\"x\":1010,\"y\":231},{\"x\":1010,\"y\":250},{\"x\":969,\"y\":250}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":1027,\"y\":231},{\"x\":1068,\"y\":231},{\"x\":1068,\"y\":249},{\"x\":1026,\"y\":250}],\"text\":\"0.12\"},{\"boundingBox\":[{\"x\":53,\"y\":278},{\"x\":107,\"y\":279},{\"x\":107,\"y\":303},{\"x\":53,\"y\":302}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":112,\"y\":279},{\"x\":155,\"y\":279},{\"x\":154,\"y\":303},{\"x\":111,\"y\":303}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":160,\"y\":278},{\"x\":209,\"y\":277},{\"x\":208,\"y\":303},{\"x\":159,\"y\":303}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":287,\"y\":283},{\"x\":336,\"y\":283},{\"x\":337,\"y\":311},{\"x\":287,\"y\":311}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":342,\"y\":283},{\"x\":385,\"y\":283},{\"x\":385,\"y\":311},{\"x\":342,\"y\":311}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":390,\"y\":283},{\"x\":439,\"y\":283},{\"x\":439,\"y\":312},{\"x\":391,\"y\":311}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":498,\"y\":285},{\"x\":553,\"y\":285},{\"x\":553,\"y\":310},{\"x\":499,\"y\":311}],\"text\":\"K1.0,\"},{\"boundingBox\":[{\"x\":558,\"y\":285},{\"x\":600,\"y\":285},{\"x\":601,\"y\":310},{\"x\":558,\"y\":310}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":605,\"y\":285},{\"x\":653,\"y\":284},{\"x\":653,\"y\":310},{\"x\":606,\"y\":310}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":822,\"y\":308},{\"x\":852,\"y\":306},{\"x\":853,\"y\":327},{\"x\":823,\"y\":329}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":882,\"y\":308},{\"x\":924,\"y\":308},{\"x\":925,\"y\":328},{\"x\":882,\"y\":329}],\"text\":\"0.12\"},{\"boundingBox\":[{\"x\":1087,\"y\":308},{\"x\":1125,\"y\":309},{\"x\":1125,\"y\":329},{\"x\":1086,\"y\":328}],\"text\":\"0.36\"},{\"boundingBox\":[{\"x\":1137,\"y\":309},{\"x\":1179,\"y\":310},{\"x\":1179,\"y\":328},{\"x\":1136,\"y\":329}],\"text\":\"0.22\"},{\"boundingBox\":[{\"x\":44,\"y\":362},{\"x\":62,\"y\":363},{\"x\":60,\"y\":386},{\"x\":44,\"y\":384}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":279,\"y\":368},{\"x\":295,\"y\":368},{\"x\":294,\"y\":392},{\"x\":279,\"y\":391}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":686,\"y\":385},{\"x\":727,\"y\":386},{\"x\":727,\"y\":405},{\"x\":686,\"y\":404}],\"text\":\"0.36\"},{\"boundingBox\":[{\"x\":821,\"y\":385},{\"x\":851,\"y\":385},{\"x\":851,\"y\":405},{\"x\":821,\"y\":405}],\"text\":\"0.3\"},{\"boundingBox\":[{\"x\":930,\"y\":385},{\"x\":942,\"y\":385},{\"x\":941,\"y\":405},{\"x\":929,\"y\":404}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1017,\"y\":379},{\"x\":1061,\"y\":381},{\"x\":1060,\"y\":408},{\"x\":1016,\"y\":406}],\"text\":\"0.12\"},{\"boundingBox\":[{\"x\":52,\"y\":450},{\"x\":105,\"y\":450},{\"x\":105,\"y\":474},{\"x\":52,\"y\":475}],\"text\":\"K0.0,\"},{\"boundingBox\":[{\"x\":110,\"y\":450},{\"x\":152,\"y\":449},{\"x\":152,\"y\":474},{\"x\":110,\"y\":474}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":157,\"y\":449},{\"x\":208,\"y\":448},{\"x\":207,\"y\":476},{\"x\":157,\"y\":474}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":301,\"y\":452},{\"x\":353,\"y\":452},{\"x\":352,\"y\":478},{\"x\":301,\"y\":478}],\"text\":\"{0.0,\"},{\"boundingBox\":[{\"x\":358,\"y\":452},{\"x\":399,\"y\":451},{\"x\":399,\"y\":478},{\"x\":357,\"y\":478}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":404,\"y\":451},{\"x\":455,\"y\":451},{\"x\":454,\"y\":477},{\"x\":404,\"y\":477}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":864,\"y\":460},{\"x\":906,\"y\":460},{\"x\":906,\"y\":484},{\"x\":864,\"y\":483}],\"text\":\"0.36\"},{\"boundingBox\":[{\"x\":914,\"y\":464},{\"x\":962,\"y\":462},{\"x\":963,\"y\":480},{\"x\":915,\"y\":482}],\"text\":\"/0.12\"},{\"boundingBox\":[{\"x\":45,\"y\":533},{\"x\":57,\"y\":533},{\"x\":57,\"y\":554},{\"x\":45,\"y\":554}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":380,\"y\":535},{\"x\":392,\"y\":535},{\"x\":392,\"y\":555},{\"x\":380,\"y\":555}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":896,\"y\":540},{\"x\":907,\"y\":541},{\"x\":905,\"y\":558},{\"x\":894,\"y\":557}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":972,\"y\":540},{\"x\":1021,\"y\":539},{\"x\":1022,\"y\":559},{\"x\":972,\"y\":561}],\"text\":\"00.36\"}]}",
        "{\"language\":\"en\",\"text\":\"0 0 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} K0.0, 0.0, 0.0} 2 2 (0.0, 0.0, 0.0} {0.0, 0.0, 0.0} 3 3\",\"lines\":[{\"boundingBox\":[{\"x\":62,\"y\":50},{\"x\":80,\"y\":48},{\"x\":83,\"y\":73},{\"x\":65,\"y\":76}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":515,\"y\":35},{\"x\":536,\"y\":36},{\"x\":536,\"y\":60},{\"x\":515,\"y\":59}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":78,\"y\":160},{\"x\":279,\"y\":160},{\"x\":279,\"y\":199},{\"x\":78,\"y\":199}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":408,\"y\":149},{\"x\":620,\"y\":147},{\"x\":620,\"y\":183},{\"x\":408,\"y\":185}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":66,\"y\":279},{\"x\":83,\"y\":280},{\"x\":82,\"y\":305},{\"x\":67,\"y\":304}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":378,\"y\":269},{\"x\":400,\"y\":270},{\"x\":401,\"y\":296},{\"x\":379,\"y\":295}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":74,\"y\":388},{\"x\":282,\"y\":388},{\"x\":282,\"y\":422},{\"x\":74,\"y\":422}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":392,\"y\":379},{\"x\":598,\"y\":378},{\"x\":598,\"y\":416},{\"x\":392,\"y\":417}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":683,\"y\":378},{\"x\":894,\"y\":378},{\"x\":895,\"y\":415},{\"x\":683,\"y\":416}],\"text\":\"K0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":64,\"y\":503},{\"x\":84,\"y\":504},{\"x\":85,\"y\":532},{\"x\":65,\"y\":531}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":376,\"y\":496},{\"x\":399,\"y\":497},{\"x\":400,\"y\":527},{\"x\":376,\"y\":526}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":75,\"y\":617},{\"x\":282,\"y\":616},{\"x\":282,\"y\":651},{\"x\":75,\"y\":652}],\"text\":\"(0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":407,\"y\":613},{\"x\":620,\"y\":612},{\"x\":620,\"y\":647},{\"x\":407,\"y\":648}],\"text\":\"{0.0, 0.0, 0.0}\"},{\"boundingBox\":[{\"x\":65,\"y\":727},{\"x\":83,\"y\":728},{\"x\":84,\"y\":757},{\"x\":66,\"y\":756}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":516,\"y\":728},{\"x\":537,\"y\":728},{\"x\":537,\"y\":756},{\"x\":516,\"y\":755}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":63,\"y\":50},{\"x\":78,\"y\":48},{\"x\":81,\"y\":74},{\"x\":66,\"y\":76}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":516,\"y\":35},{\"x\":530,\"y\":36},{\"x\":529,\"y\":60},{\"x\":515,\"y\":59}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":79,\"y\":161},{\"x\":144,\"y\":161},{\"x\":143,\"y\":198},{\"x\":78,\"y\":199}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":151,\"y\":161},{\"x\":208,\"y\":161},{\"x\":208,\"y\":199},{\"x\":150,\"y\":198}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":216,\"y\":161},{\"x\":278,\"y\":161},{\"x\":278,\"y\":200},{\"x\":215,\"y\":199}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":408,\"y\":150},{\"x\":480,\"y\":149},{\"x\":481,\"y\":185},{\"x\":409,\"y\":186}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":487,\"y\":149},{\"x\":546,\"y\":148},{\"x\":547,\"y\":184},{\"x\":488,\"y\":185}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":553,\"y\":148},{\"x\":620,\"y\":148},{\"x\":620,\"y\":184},{\"x\":554,\"y\":184}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":67,\"y\":279},{\"x\":82,\"y\":280},{\"x\":80,\"y\":305},{\"x\":66,\"y\":304}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":381,\"y\":269},{\"x\":396,\"y\":270},{\"x\":395,\"y\":296},{\"x\":380,\"y\":295}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":75,\"y\":389},{\"x\":144,\"y\":390},{\"x\":144,\"y\":422},{\"x\":75,\"y\":422}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":150,\"y\":390},{\"x\":209,\"y\":390},{\"x\":210,\"y\":423},{\"x\":151,\"y\":422}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":216,\"y\":389},{\"x\":281,\"y\":388},{\"x\":281,\"y\":423},{\"x\":216,\"y\":423}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":394,\"y\":380},{\"x\":457,\"y\":380},{\"x\":456,\"y\":417},{\"x\":393,\"y\":417}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":465,\"y\":380},{\"x\":524,\"y\":379},{\"x\":523,\"y\":417},{\"x\":464,\"y\":417}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":531,\"y\":379},{\"x\":598,\"y\":379},{\"x\":598,\"y\":417},{\"x\":531,\"y\":417}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":683,\"y\":379},{\"x\":753,\"y\":379},{\"x\":754,\"y\":416},{\"x\":685,\"y\":417}],\"text\":\"K0.0,\"},{\"boundingBox\":[{\"x\":760,\"y\":379},{\"x\":819,\"y\":380},{\"x\":820,\"y\":415},{\"x\":761,\"y\":416}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":826,\"y\":380},{\"x\":895,\"y\":380},{\"x\":895,\"y\":414},{\"x\":827,\"y\":415}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":65,\"y\":503},{\"x\":84,\"y\":504},{\"x\":82,\"y\":532},{\"x\":64,\"y\":531}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":379,\"y\":496},{\"x\":397,\"y\":497},{\"x\":396,\"y\":527},{\"x\":378,\"y\":526}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":75,\"y\":617},{\"x\":144,\"y\":617},{\"x\":144,\"y\":651},{\"x\":75,\"y\":653}],\"text\":\"(0.0,\"},{\"boundingBox\":[{\"x\":151,\"y\":617},{\"x\":208,\"y\":617},{\"x\":208,\"y\":651},{\"x\":151,\"y\":651}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":215,\"y\":617},{\"x\":282,\"y\":616},{\"x\":283,\"y\":652},{\"x\":215,\"y\":651}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":410,\"y\":614},{\"x\":481,\"y\":614},{\"x\":480,\"y\":647},{\"x\":409,\"y\":648}],\"text\":\"{0.0,\"},{\"boundingBox\":[{\"x\":487,\"y\":614},{\"x\":547,\"y\":614},{\"x\":546,\"y\":647},{\"x\":487,\"y\":647}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":553,\"y\":614},{\"x\":620,\"y\":613},{\"x\":620,\"y\":648},{\"x\":553,\"y\":647}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":65,\"y\":727},{\"x\":84,\"y\":728},{\"x\":83,\"y\":757},{\"x\":65,\"y\":756}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":517,\"y\":728},{\"x\":534,\"y\":728},{\"x\":533,\"y\":756},{\"x\":517,\"y\":755}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 0 (1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 1 1 {1.0, 1.0, 1.0} {1.0, 1.0, 1.0} 2 2 [0.0, 0.0, 0.0} {0.0, 0.0, 0.0}[0.0, 0.0. 0.0} 3 3\",\"lines\":[{\"boundingBox\":[{\"x\":534,\"y\":41},{\"x\":556,\"y\":41},{\"x\":554,\"y\":67},{\"x\":532,\"y\":68}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":71,\"y\":69},{\"x\":94,\"y\":69},{\"x\":94,\"y\":98},{\"x\":71,\"y\":97}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":85,\"y\":200},{\"x\":327,\"y\":200},{\"x\":327,\"y\":243},{\"x\":85,\"y\":244}],\"text\":\"(1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":545,\"y\":177},{\"x\":799,\"y\":179},{\"x\":799,\"y\":217},{\"x\":545,\"y\":215}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":74,\"y\":341},{\"x\":96,\"y\":341},{\"x\":95,\"y\":372},{\"x\":74,\"y\":372}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":532,\"y\":317},{\"x\":557,\"y\":319},{\"x\":556,\"y\":355},{\"x\":531,\"y\":352}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":85,\"y\":469},{\"x\":330,\"y\":469},{\"x\":330,\"y\":507},{\"x\":85,\"y\":506}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":547,\"y\":454},{\"x\":801,\"y\":454},{\"x\":801,\"y\":493},{\"x\":547,\"y\":493}],\"text\":\"{1.0, 1.0, 1.0}\"},{\"boundingBox\":[{\"x\":72,\"y\":605},{\"x\":97,\"y\":608},{\"x\":96,\"y\":640},{\"x\":71,\"y\":637}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":535,\"y\":592},{\"x\":558,\"y\":594},{\"x\":554,\"y\":631},{\"x\":531,\"y\":628}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":81,\"y\":736},{\"x\":893,\"y\":731},{\"x\":894,\"y\":773},{\"x\":81,\"y\":779}],\"text\":\"[0.0, 0.0, 0.0} {0.0, 0.0, 0.0}[0.0, 0.0. 0.0}\"},{\"boundingBox\":[{\"x\":72,\"y\":875},{\"x\":96,\"y\":876},{\"x\":97,\"y\":905},{\"x\":73,\"y\":904}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":532,\"y\":870},{\"x\":558,\"y\":872},{\"x\":557,\"y\":904},{\"x\":530,\"y\":901}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":532,\"y\":41},{\"x\":547,\"y\":41},{\"x\":547,\"y\":68},{\"x\":532,\"y\":68}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":72,\"y\":69},{\"x\":89,\"y\":69},{\"x\":89,\"y\":98},{\"x\":72,\"y\":97}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":85,\"y\":201},{\"x\":167,\"y\":201},{\"x\":167,\"y\":244},{\"x\":85,\"y\":245}],\"text\":\"(1.0,\"},{\"boundingBox\":[{\"x\":176,\"y\":201},{\"x\":244,\"y\":201},{\"x\":244,\"y\":244},{\"x\":176,\"y\":244}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":252,\"y\":201},{\"x\":327,\"y\":200},{\"x\":328,\"y\":245},{\"x\":253,\"y\":244}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":546,\"y\":178},{\"x\":634,\"y\":180},{\"x\":634,\"y\":217},{\"x\":546,\"y\":214}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":641,\"y\":180},{\"x\":711,\"y\":181},{\"x\":711,\"y\":217},{\"x\":641,\"y\":217}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":718,\"y\":181},{\"x\":798,\"y\":179},{\"x\":798,\"y\":217},{\"x\":718,\"y\":217}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":76,\"y\":341},{\"x\":94,\"y\":341},{\"x\":94,\"y\":372},{\"x\":76,\"y\":372}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":534,\"y\":317},{\"x\":557,\"y\":319},{\"x\":554,\"y\":355},{\"x\":531,\"y\":352}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":86,\"y\":470},{\"x\":169,\"y\":472},{\"x\":169,\"y\":507},{\"x\":85,\"y\":506}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":176,\"y\":472},{\"x\":245,\"y\":472},{\"x\":245,\"y\":508},{\"x\":176,\"y\":507}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":252,\"y\":472},{\"x\":328,\"y\":470},{\"x\":329,\"y\":508},{\"x\":252,\"y\":508}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":548,\"y\":455},{\"x\":633,\"y\":455},{\"x\":634,\"y\":494},{\"x\":549,\"y\":494}],\"text\":\"{1.0,\"},{\"boundingBox\":[{\"x\":641,\"y\":455},{\"x\":712,\"y\":455},{\"x\":712,\"y\":494},{\"x\":641,\"y\":494}],\"text\":\"1.0,\"},{\"boundingBox\":[{\"x\":720,\"y\":455},{\"x\":800,\"y\":455},{\"x\":800,\"y\":493},{\"x\":720,\"y\":494}],\"text\":\"1.0}\"},{\"boundingBox\":[{\"x\":72,\"y\":605},{\"x\":97,\"y\":608},{\"x\":94,\"y\":640},{\"x\":71,\"y\":637}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":536,\"y\":592},{\"x\":556,\"y\":594},{\"x\":552,\"y\":630},{\"x\":532,\"y\":628}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":82,\"y\":740},{\"x\":165,\"y\":738},{\"x\":165,\"y\":779},{\"x\":82,\"y\":779}],\"text\":\"[0.0,\"},{\"boundingBox\":[{\"x\":174,\"y\":738},{\"x\":241,\"y\":736},{\"x\":241,\"y\":779},{\"x\":174,\"y\":779}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":249,\"y\":736},{\"x\":330,\"y\":735},{\"x\":330,\"y\":779},{\"x\":249,\"y\":779}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":384,\"y\":734},{\"x\":468,\"y\":733},{\"x\":468,\"y\":778},{\"x\":384,\"y\":778}],\"text\":\"{0.0,\"},{\"boundingBox\":[{\"x\":477,\"y\":733},{\"x\":547,\"y\":732},{\"x\":547,\"y\":777},{\"x\":477,\"y\":778}],\"text\":\"0.0,\"},{\"boundingBox\":[{\"x\":556,\"y\":732},{\"x\":730,\"y\":731},{\"x\":730,\"y\":775},{\"x\":556,\"y\":777}],\"text\":\"0.0}[0.0,\"},{\"boundingBox\":[{\"x\":738,\"y\":731},{\"x\":805,\"y\":732},{\"x\":805,\"y\":774},{\"x\":738,\"y\":775}],\"text\":\"0.0.\"},{\"boundingBox\":[{\"x\":814,\"y\":732},{\"x\":893,\"y\":732},{\"x\":893,\"y\":773},{\"x\":814,\"y\":774}],\"text\":\"0.0}\"},{\"boundingBox\":[{\"x\":73,\"y\":875},{\"x\":90,\"y\":876},{\"x\":89,\"y\":905},{\"x\":72,\"y\":904}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":537,\"y\":870},{\"x\":558,\"y\":872},{\"x\":555,\"y\":904},{\"x\":534,\"y\":902}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"1.0 1.0 2 0.0 1.0 3\",\"lines\":[{\"boundingBox\":[{\"x\":501,\"y\":385},{\"x\":600,\"y\":382},{\"x\":601,\"y\":447},{\"x\":503,\"y\":450}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":377,\"y\":961},{\"x\":488,\"y\":963},{\"x\":486,\"y\":1028},{\"x\":376,\"y\":1024}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":181,\"y\":1253},{\"x\":207,\"y\":1252},{\"x\":209,\"y\":1309},{\"x\":183,\"y\":1310}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":510,\"y\":1244},{\"x\":631,\"y\":1250},{\"x\":622,\"y\":1320},{\"x\":507,\"y\":1314}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":343,\"y\":1550},{\"x\":452,\"y\":1542},{\"x\":455,\"y\":1609},{\"x\":345,\"y\":1609}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":421,\"y\":1835},{\"x\":467,\"y\":1832},{\"x\":468,\"y\":1892},{\"x\":421,\"y\":1895}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":501,\"y\":385},{\"x\":590,\"y\":382},{\"x\":592,\"y\":447},{\"x\":503,\"y\":450}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":377,\"y\":961},{\"x\":474,\"y\":963},{\"x\":473,\"y\":1028},{\"x\":376,\"y\":1025}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":181,\"y\":1253},{\"x\":204,\"y\":1252},{\"x\":206,\"y\":1309},{\"x\":183,\"y\":1310}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":513,\"y\":1244},{\"x\":615,\"y\":1249},{\"x\":611,\"y\":1319},{\"x\":510,\"y\":1314}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":343,\"y\":1546},{\"x\":439,\"y\":1542},{\"x\":441,\"y\":1609},{\"x\":345,\"y\":1610}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":421,\"y\":1834},{\"x\":455,\"y\":1832},{\"x\":459,\"y\":1892},{\"x\":424,\"y\":1894}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 1.0 1 1.0 0.0 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 6 4 3 11 5 12 9 13 8 10\",\"lines\":[{\"boundingBox\":[{\"x\":658,\"y\":25},{\"x\":673,\"y\":26},{\"x\":672,\"y\":43},{\"x\":658,\"y\":42}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":664,\"y\":104},{\"x\":698,\"y\":103},{\"x\":698,\"y\":123},{\"x\":664,\"y\":124}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":657,\"y\":184},{\"x\":672,\"y\":185},{\"x\":673,\"y\":203},{\"x\":658,\"y\":203}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":709,\"y\":262},{\"x\":744,\"y\":261},{\"x\":745,\"y\":280},{\"x\":710,\"y\":281}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":179,\"y\":340},{\"x\":211,\"y\":341},{\"x\":212,\"y\":360},{\"x\":179,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":317,\"y\":340},{\"x\":348,\"y\":341},{\"x\":348,\"y\":361},{\"x\":316,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":432,\"y\":340},{\"x\":464,\"y\":340},{\"x\":464,\"y\":361},{\"x\":431,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":512,\"y\":340},{\"x\":545,\"y\":340},{\"x\":545,\"y\":360},{\"x\":512,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":588,\"y\":340},{\"x\":666,\"y\":340},{\"x\":666,\"y\":359},{\"x\":588,\"y\":360}],\"text\":\"0.0 0.0\"},{\"boundingBox\":[{\"x\":733,\"y\":341},{\"x\":749,\"y\":340},{\"x\":750,\"y\":361},{\"x\":734,\"y\":361}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":910,\"y\":340},{\"x\":943,\"y\":341},{\"x\":944,\"y\":360},{\"x\":910,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1023,\"y\":340},{\"x\":1056,\"y\":341},{\"x\":1056,\"y\":360},{\"x\":1023,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1144,\"y\":340},{\"x\":1177,\"y\":340},{\"x\":1177,\"y\":360},{\"x\":1144,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1270,\"y\":341},{\"x\":1301,\"y\":341},{\"x\":1301,\"y\":361},{\"x\":1269,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1359,\"y\":340},{\"x\":1391,\"y\":340},{\"x\":1392,\"y\":361},{\"x\":1358,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":303,\"y\":421},{\"x\":334,\"y\":419},{\"x\":336,\"y\":439},{\"x\":305,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":426,\"y\":421},{\"x\":457,\"y\":419},{\"x\":459,\"y\":439},{\"x\":427,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":530,\"y\":421},{\"x\":563,\"y\":418},{\"x\":564,\"y\":438},{\"x\":532,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":618,\"y\":420},{\"x\":669,\"y\":420},{\"x\":668,\"y\":441},{\"x\":619,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":679,\"y\":419},{\"x\":771,\"y\":419},{\"x\":771,\"y\":441},{\"x\":679,\"y\":441}],\"text\":\"1.0 1.0\"},{\"boundingBox\":[{\"x\":803,\"y\":419},{\"x\":838,\"y\":419},{\"x\":839,\"y\":441},{\"x\":803,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":893,\"y\":419},{\"x\":926,\"y\":421},{\"x\":925,\"y\":440},{\"x\":891,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1001,\"y\":420},{\"x\":1032,\"y\":420},{\"x\":1032,\"y\":439},{\"x\":1001,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1124,\"y\":419},{\"x\":1157,\"y\":420},{\"x\":1157,\"y\":440},{\"x\":1124,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1257,\"y\":421},{\"x\":1290,\"y\":419},{\"x\":1290,\"y\":439},{\"x\":1256,\"y\":440}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":45,\"y\":500},{\"x\":57,\"y\":501},{\"x\":57,\"y\":518},{\"x\":44,\"y\":517}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":325,\"y\":497},{\"x\":340,\"y\":498},{\"x\":339,\"y\":518},{\"x\":324,\"y\":517}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":454,\"y\":498},{\"x\":469,\"y\":498},{\"x\":470,\"y\":519},{\"x\":454,\"y\":518}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":581,\"y\":498},{\"x\":607,\"y\":497},{\"x\":608,\"y\":517},{\"x\":581,\"y\":518}],\"text\":\"11\"},{\"boundingBox\":[{\"x\":715,\"y\":497},{\"x\":731,\"y\":497},{\"x\":730,\"y\":519},{\"x\":715,\"y\":518}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":839,\"y\":497},{\"x\":865,\"y\":497},{\"x\":866,\"y\":517},{\"x\":840,\"y\":517}],\"text\":\"12\"},{\"boundingBox\":[{\"x\":991,\"y\":497},{\"x\":1007,\"y\":498},{\"x\":1006,\"y\":517},{\"x\":990,\"y\":517}],\"text\":\"9\"},{\"boundingBox\":[{\"x\":1128,\"y\":498},{\"x\":1155,\"y\":498},{\"x\":1155,\"y\":518},{\"x\":1128,\"y\":518}],\"text\":\"13\"},{\"boundingBox\":[{\"x\":1267,\"y\":497},{\"x\":1282,\"y\":498},{\"x\":1282,\"y\":518},{\"x\":1267,\"y\":517}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":1391,\"y\":498},{\"x\":1419,\"y\":497},{\"x\":1420,\"y\":517},{\"x\":1392,\"y\":518}],\"text\":\"10\"}],\"words\":[{\"boundingBox\":[{\"x\":659,\"y\":25},{\"x\":669,\"y\":26},{\"x\":668,\"y\":43},{\"x\":658,\"y\":42}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":666,\"y\":104},{\"x\":695,\"y\":103},{\"x\":695,\"y\":123},{\"x\":666,\"y\":124}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":658,\"y\":184},{\"x\":670,\"y\":184},{\"x\":670,\"y\":203},{\"x\":657,\"y\":202}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":709,\"y\":262},{\"x\":739,\"y\":261},{\"x\":740,\"y\":280},{\"x\":710,\"y\":281}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":179,\"y\":340},{\"x\":209,\"y\":340},{\"x\":209,\"y\":360},{\"x\":179,\"y\":359}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":317,\"y\":340},{\"x\":346,\"y\":341},{\"x\":345,\"y\":361},{\"x\":316,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":431,\"y\":340},{\"x\":462,\"y\":340},{\"x\":462,\"y\":361},{\"x\":431,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":512,\"y\":340},{\"x\":542,\"y\":340},{\"x\":542,\"y\":360},{\"x\":512,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":590,\"y\":341},{\"x\":620,\"y\":341},{\"x\":620,\"y\":360},{\"x\":590,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":633,\"y\":341},{\"x\":662,\"y\":341},{\"x\":662,\"y\":360},{\"x\":633,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":733,\"y\":340},{\"x\":747,\"y\":340},{\"x\":748,\"y\":361},{\"x\":734,\"y\":361}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":910,\"y\":340},{\"x\":940,\"y\":340},{\"x\":940,\"y\":361},{\"x\":910,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1023,\"y\":340},{\"x\":1053,\"y\":340},{\"x\":1053,\"y\":360},{\"x\":1023,\"y\":359}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1145,\"y\":340},{\"x\":1174,\"y\":340},{\"x\":1174,\"y\":361},{\"x\":1145,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1269,\"y\":341},{\"x\":1299,\"y\":341},{\"x\":1299,\"y\":361},{\"x\":1269,\"y\":360}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1358,\"y\":340},{\"x\":1390,\"y\":340},{\"x\":1390,\"y\":361},{\"x\":1358,\"y\":361}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":304,\"y\":421},{\"x\":331,\"y\":419},{\"x\":333,\"y\":439},{\"x\":305,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":427,\"y\":420},{\"x\":454,\"y\":419},{\"x\":456,\"y\":439},{\"x\":428,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":530,\"y\":421},{\"x\":559,\"y\":418},{\"x\":561,\"y\":438},{\"x\":532,\"y\":441}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":618,\"y\":420},{\"x\":652,\"y\":420},{\"x\":651,\"y\":441},{\"x\":618,\"y\":440}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":682,\"y\":420},{\"x\":711,\"y\":420},{\"x\":711,\"y\":442},{\"x\":681,\"y\":440}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":735,\"y\":420},{\"x\":766,\"y\":419},{\"x\":767,\"y\":442},{\"x\":735,\"y\":442}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":806,\"y\":419},{\"x\":836,\"y\":419},{\"x\":835,\"y\":441},{\"x\":805,\"y\":440}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":892,\"y\":419},{\"x\":924,\"y\":420},{\"x\":923,\"y\":440},{\"x\":891,\"y\":438}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1001,\"y\":420},{\"x\":1030,\"y\":420},{\"x\":1030,\"y\":439},{\"x\":1001,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1124,\"y\":419},{\"x\":1153,\"y\":420},{\"x\":1152,\"y\":440},{\"x\":1124,\"y\":439}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":1256,\"y\":420},{\"x\":1286,\"y\":419},{\"x\":1287,\"y\":439},{\"x\":1257,\"y\":440}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":45,\"y\":500},{\"x\":55,\"y\":501},{\"x\":53,\"y\":518},{\"x\":44,\"y\":517}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":326,\"y\":497},{\"x\":338,\"y\":498},{\"x\":336,\"y\":518},{\"x\":325,\"y\":517}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":455,\"y\":498},{\"x\":467,\"y\":498},{\"x\":467,\"y\":519},{\"x\":454,\"y\":518}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":581,\"y\":497},{\"x\":605,\"y\":497},{\"x\":605,\"y\":517},{\"x\":582,\"y\":518}],\"text\":\"11\"},{\"boundingBox\":[{\"x\":716,\"y\":497},{\"x\":730,\"y\":497},{\"x\":729,\"y\":519},{\"x\":715,\"y\":518}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":840,\"y\":497},{\"x\":862,\"y\":497},{\"x\":862,\"y\":517},{\"x\":840,\"y\":517}],\"text\":\"12\"},{\"boundingBox\":[{\"x\":992,\"y\":497},{\"x\":1004,\"y\":497},{\"x\":1003,\"y\":517},{\"x\":992,\"y\":516}],\"text\":\"9\"},{\"boundingBox\":[{\"x\":1129,\"y\":498},{\"x\":1151,\"y\":498},{\"x\":1151,\"y\":518},{\"x\":1129,\"y\":518}],\"text\":\"13\"},{\"boundingBox\":[{\"x\":1268,\"y\":497},{\"x\":1280,\"y\":498},{\"x\":1278,\"y\":518},{\"x\":1267,\"y\":517}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":1391,\"y\":498},{\"x\":1415,\"y\":497},{\"x\":1416,\"y\":517},{\"x\":1392,\"y\":518}],\"text\":\"10\"}]}",
        "{\"language\":\"en\",\"text\":\"1.0 1 1.0 1.0 2 3\",\"lines\":[{\"boundingBox\":[{\"x\":70,\"y\":63},{\"x\":91,\"y\":63},{\"x\":91,\"y\":76},{\"x\":70,\"y\":75}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":67,\"y\":114},{\"x\":77,\"y\":114},{\"x\":77,\"y\":126},{\"x\":67,\"y\":126}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":55,\"y\":162},{\"x\":77,\"y\":162},{\"x\":78,\"y\":175},{\"x\":55,\"y\":175}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":92,\"y\":162},{\"x\":115,\"y\":162},{\"x\":115,\"y\":174},{\"x\":93,\"y\":175}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":27,\"y\":211},{\"x\":35,\"y\":212},{\"x\":36,\"y\":224},{\"x\":27,\"y\":224}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":106,\"y\":211},{\"x\":117,\"y\":211},{\"x\":117,\"y\":224},{\"x\":106,\"y\":224}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":72,\"y\":63},{\"x\":89,\"y\":63},{\"x\":89,\"y\":76},{\"x\":71,\"y\":75}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":68,\"y\":114},{\"x\":75,\"y\":114},{\"x\":75,\"y\":126},{\"x\":68,\"y\":126}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":56,\"y\":162},{\"x\":74,\"y\":162},{\"x\":74,\"y\":175},{\"x\":56,\"y\":175}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":94,\"y\":162},{\"x\":113,\"y\":162},{\"x\":113,\"y\":175},{\"x\":94,\"y\":175}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":27,\"y\":211},{\"x\":36,\"y\":211},{\"x\":35,\"y\":224},{\"x\":27,\"y\":223}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":107,\"y\":211},{\"x\":114,\"y\":211},{\"x\":114,\"y\":224},{\"x\":107,\"y\":224}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"? 1 2 3\",\"lines\":[{\"boundingBox\":[{\"x\":78,\"y\":72},{\"x\":89,\"y\":73},{\"x\":89,\"y\":86},{\"x\":78,\"y\":86}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":73,\"y\":129},{\"x\":84,\"y\":128},{\"x\":85,\"y\":142},{\"x\":75,\"y\":142}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":30,\"y\":222},{\"x\":39,\"y\":223},{\"x\":40,\"y\":238},{\"x\":30,\"y\":237}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":138,\"y\":223},{\"x\":147,\"y\":223},{\"x\":148,\"y\":236},{\"x\":138,\"y\":236}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":78,\"y\":72},{\"x\":87,\"y\":72},{\"x\":87,\"y\":86},{\"x\":78,\"y\":85}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":73,\"y\":128},{\"x\":81,\"y\":128},{\"x\":82,\"y\":142},{\"x\":74,\"y\":142}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":30,\"y\":222},{\"x\":40,\"y\":223},{\"x\":38,\"y\":238},{\"x\":30,\"y\":237}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":140,\"y\":223},{\"x\":147,\"y\":223},{\"x\":147,\"y\":236},{\"x\":140,\"y\":236}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 1.0 1.0 1 4 1.0 1.0 1.0 2 3\",\"lines\":[{\"boundingBox\":[{\"x\":145,\"y\":14},{\"x\":152,\"y\":14},{\"x\":152,\"y\":24},{\"x\":145,\"y\":24}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":130,\"y\":61},{\"x\":151,\"y\":60},{\"x\":151,\"y\":73},{\"x\":130,\"y\":74}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":169,\"y\":62},{\"x\":188,\"y\":61},{\"x\":189,\"y\":72},{\"x\":169,\"y\":73}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":103,\"y\":109},{\"x\":111,\"y\":109},{\"x\":111,\"y\":121},{\"x\":103,\"y\":121}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":179,\"y\":108},{\"x\":189,\"y\":108},{\"x\":190,\"y\":121},{\"x\":179,\"y\":120}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":76,\"y\":155},{\"x\":97,\"y\":155},{\"x\":97,\"y\":167},{\"x\":76,\"y\":168}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":103,\"y\":155},{\"x\":125,\"y\":156},{\"x\":125,\"y\":168},{\"x\":103,\"y\":167}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":183,\"y\":156},{\"x\":203,\"y\":155},{\"x\":203,\"y\":168},{\"x\":183,\"y\":168}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":25,\"y\":204},{\"x\":34,\"y\":204},{\"x\":34,\"y\":216},{\"x\":25,\"y\":215}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":102,\"y\":203},{\"x\":112,\"y\":204},{\"x\":112,\"y\":216},{\"x\":102,\"y\":215}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":145,\"y\":14},{\"x\":151,\"y\":14},{\"x\":151,\"y\":24},{\"x\":145,\"y\":24}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":130,\"y\":61},{\"x\":148,\"y\":60},{\"x\":149,\"y\":73},{\"x\":131,\"y\":74}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":169,\"y\":61},{\"x\":186,\"y\":61},{\"x\":187,\"y\":72},{\"x\":170,\"y\":73}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":103,\"y\":109},{\"x\":111,\"y\":109},{\"x\":111,\"y\":121},{\"x\":103,\"y\":121}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":181,\"y\":108},{\"x\":189,\"y\":108},{\"x\":189,\"y\":121},{\"x\":180,\"y\":121}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":76,\"y\":155},{\"x\":93,\"y\":155},{\"x\":94,\"y\":168},{\"x\":77,\"y\":168}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":108,\"y\":155},{\"x\":124,\"y\":156},{\"x\":124,\"y\":168},{\"x\":107,\"y\":167}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":184,\"y\":155},{\"x\":201,\"y\":155},{\"x\":202,\"y\":168},{\"x\":184,\"y\":168}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":25,\"y\":204},{\"x\":33,\"y\":204},{\"x\":32,\"y\":216},{\"x\":25,\"y\":215}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":103,\"y\":203},{\"x\":110,\"y\":204},{\"x\":108,\"y\":216},{\"x\":102,\"y\":215}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 1 2 3 5\",\"lines\":[{\"boundingBox\":[{\"x\":142,\"y\":14},{\"x\":150,\"y\":14},{\"x\":149,\"y\":24},{\"x\":141,\"y\":24}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":74,\"y\":93},{\"x\":83,\"y\":93},{\"x\":84,\"y\":103},{\"x\":75,\"y\":103}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":25,\"y\":171},{\"x\":33,\"y\":172},{\"x\":33,\"y\":183},{\"x\":25,\"y\":183}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":111,\"y\":171},{\"x\":120,\"y\":171},{\"x\":121,\"y\":183},{\"x\":111,\"y\":183}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":193,\"y\":171},{\"x\":202,\"y\":171},{\"x\":202,\"y\":183},{\"x\":193,\"y\":183}],\"text\":\"5\"}],\"words\":[{\"boundingBox\":[{\"x\":141,\"y\":14},{\"x\":147,\"y\":14},{\"x\":147,\"y\":24},{\"x\":141,\"y\":24}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":74,\"y\":93},{\"x\":80,\"y\":93},{\"x\":80,\"y\":103},{\"x\":74,\"y\":103}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":27,\"y\":171},{\"x\":33,\"y\":171},{\"x\":32,\"y\":183},{\"x\":26,\"y\":182}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":112,\"y\":171},{\"x\":119,\"y\":171},{\"x\":119,\"y\":183},{\"x\":112,\"y\":183}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":194,\"y\":171},{\"x\":201,\"y\":171},{\"x\":201,\"y\":183},{\"x\":194,\"y\":183}],\"text\":\"5\"}]}",
        "{\"language\":\"en\",\"text\":\"1.0 1 1.0 2 1.0 1.0 3\",\"lines\":[{\"boundingBox\":[{\"x\":78,\"y\":60},{\"x\":98,\"y\":60},{\"x\":98,\"y\":73},{\"x\":78,\"y\":73}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":74,\"y\":108},{\"x\":83,\"y\":108},{\"x\":83,\"y\":120},{\"x\":75,\"y\":120}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":60,\"y\":154},{\"x\":78,\"y\":154},{\"x\":78,\"y\":166},{\"x\":59,\"y\":167}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":25,\"y\":202},{\"x\":33,\"y\":201},{\"x\":34,\"y\":214},{\"x\":25,\"y\":214}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":81,\"y\":202},{\"x\":102,\"y\":201},{\"x\":102,\"y\":214},{\"x\":81,\"y\":214}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":53,\"y\":250},{\"x\":72,\"y\":249},{\"x\":73,\"y\":260},{\"x\":54,\"y\":262}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":66,\"y\":296},{\"x\":76,\"y\":295},{\"x\":76,\"y\":308},{\"x\":66,\"y\":308}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":80,\"y\":60},{\"x\":97,\"y\":60},{\"x\":97,\"y\":73},{\"x\":80,\"y\":73}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":75,\"y\":108},{\"x\":82,\"y\":108},{\"x\":82,\"y\":120},{\"x\":75,\"y\":120}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":59,\"y\":154},{\"x\":77,\"y\":154},{\"x\":77,\"y\":167},{\"x\":60,\"y\":167}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":25,\"y\":201},{\"x\":32,\"y\":201},{\"x\":33,\"y\":213},{\"x\":26,\"y\":214}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":82,\"y\":201},{\"x\":99,\"y\":201},{\"x\":100,\"y\":214},{\"x\":82,\"y\":214}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":54,\"y\":250},{\"x\":70,\"y\":249},{\"x\":71,\"y\":261},{\"x\":54,\"y\":262}],\"text\":\"1.0\"},{\"boundingBox\":[{\"x\":66,\"y\":295},{\"x\":74,\"y\":295},{\"x\":74,\"y\":308},{\"x\":67,\"y\":308}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"0 1 2 3\",\"lines\":[{\"boundingBox\":[{\"x\":105,\"y\":16},{\"x\":114,\"y\":16},{\"x\":114,\"y\":27},{\"x\":105,\"y\":27}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":30,\"y\":107},{\"x\":37,\"y\":106},{\"x\":37,\"y\":118},{\"x\":30,\"y\":118}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":89,\"y\":212},{\"x\":98,\"y\":212},{\"x\":98,\"y\":224},{\"x\":89,\"y\":224}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":87,\"y\":302},{\"x\":97,\"y\":302},{\"x\":98,\"y\":314},{\"x\":88,\"y\":314}],\"text\":\"3\"}],\"words\":[{\"boundingBox\":[{\"x\":106,\"y\":16},{\"x\":112,\"y\":16},{\"x\":112,\"y\":27},{\"x\":106,\"y\":27}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":30,\"y\":106},{\"x\":35,\"y\":106},{\"x\":36,\"y\":118},{\"x\":30,\"y\":118}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":89,\"y\":212},{\"x\":97,\"y\":212},{\"x\":97,\"y\":224},{\"x\":89,\"y\":224}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":89,\"y\":302},{\"x\":97,\"y\":302},{\"x\":97,\"y\":314},{\"x\":89,\"y\":314}],\"text\":\"3\"}]}",
        "{\"language\":\"en\",\"text\":\"4 2 r 1.5 1 0.5 Feedback value,f & Reputation,r 0 2 3 4 15 1 Feedback, i\",\"lines\":[{\"boundingBox\":[{\"x\":1020,\"y\":27},{\"x\":1020,\"y\":46},{\"x\":1010,\"y\":47},{\"x\":1010,\"y\":28}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":100,\"y\":1},{\"x\":121,\"y\":4},{\"x\":119,\"y\":28},{\"x\":99,\"y\":26}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":1010,\"y\":69},{\"x\":1023,\"y\":69},{\"x\":1022,\"y\":89},{\"x\":1009,\"y\":88}],\"text\":\"r\"},{\"boundingBox\":[{\"x\":74,\"y\":196},{\"x\":117,\"y\":196},{\"x\":117,\"y\":225},{\"x\":74,\"y\":225}],\"text\":\"1.5\"},{\"boundingBox\":[{\"x\":104,\"y\":391},{\"x\":119,\"y\":393},{\"x\":118,\"y\":416},{\"x\":103,\"y\":414}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":73,\"y\":585},{\"x\":116,\"y\":584},{\"x\":116,\"y\":613},{\"x\":72,\"y\":613}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":2,\"y\":644},{\"x\":2,\"y\":183},{\"x\":33,\"y\":183},{\"x\":31,\"y\":644}],\"text\":\"Feedback value,f & Reputation,r\"},{\"boundingBox\":[{\"x\":99,\"y\":783},{\"x\":118,\"y\":782},{\"x\":120,\"y\":805},{\"x\":101,\"y\":805}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":390,\"y\":817},{\"x\":411,\"y\":818},{\"x\":412,\"y\":846},{\"x\":390,\"y\":845}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":647,\"y\":817},{\"x\":667,\"y\":817},{\"x\":669,\"y\":843},{\"x\":648,\"y\":843}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":906,\"y\":817},{\"x\":925,\"y\":819},{\"x\":925,\"y\":843},{\"x\":905,\"y\":841}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":1159,\"y\":818},{\"x\":1179,\"y\":818},{\"x\":1179,\"y\":841},{\"x\":1160,\"y\":840}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":137,\"y\":820},{\"x\":151,\"y\":820},{\"x\":152,\"y\":839},{\"x\":138,\"y\":838}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":569,\"y\":872},{\"x\":734,\"y\":872},{\"x\":734,\"y\":900},{\"x\":569,\"y\":900}],\"text\":\"Feedback, i\"}],\"words\":[{\"boundingBox\":[{\"x\":1020,\"y\":27},{\"x\":1020,\"y\":33},{\"x\":1010,\"y\":33},{\"x\":1010,\"y\":27}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":101,\"y\":1},{\"x\":117,\"y\":2},{\"x\":114,\"y\":27},{\"x\":99,\"y\":25}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":1009,\"y\":69},{\"x\":1020,\"y\":69},{\"x\":1020,\"y\":89},{\"x\":1009,\"y\":88}],\"text\":\"r\"},{\"boundingBox\":[{\"x\":75,\"y\":196},{\"x\":114,\"y\":196},{\"x\":114,\"y\":225},{\"x\":75,\"y\":225}],\"text\":\"1.5\"},{\"boundingBox\":[{\"x\":104,\"y\":391},{\"x\":119,\"y\":393},{\"x\":116,\"y\":416},{\"x\":103,\"y\":414}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":72,\"y\":584},{\"x\":113,\"y\":584},{\"x\":114,\"y\":613},{\"x\":72,\"y\":613}],\"text\":\"0.5\"},{\"boundingBox\":[{\"x\":2,\"y\":641},{\"x\":2,\"y\":501},{\"x\":31,\"y\":501},{\"x\":30,\"y\":641}],\"text\":\"Feedback\"},{\"boundingBox\":[{\"x\":2,\"y\":492},{\"x\":2,\"y\":394},{\"x\":32,\"y\":394},{\"x\":31,\"y\":492}],\"text\":\"value,f\"},{\"boundingBox\":[{\"x\":2,\"y\":388},{\"x\":2,\"y\":371},{\"x\":32,\"y\":371},{\"x\":32,\"y\":387}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":2,\"y\":360},{\"x\":2,\"y\":184},{\"x\":33,\"y\":184},{\"x\":32,\"y\":360}],\"text\":\"Reputation,r\"},{\"boundingBox\":[{\"x\":100,\"y\":782},{\"x\":113,\"y\":782},{\"x\":114,\"y\":805},{\"x\":100,\"y\":805}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":391,\"y\":817},{\"x\":410,\"y\":818},{\"x\":408,\"y\":846},{\"x\":390,\"y\":845}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":648,\"y\":817},{\"x\":663,\"y\":817},{\"x\":663,\"y\":843},{\"x\":648,\"y\":843}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":910,\"y\":817},{\"x\":925,\"y\":819},{\"x\":923,\"y\":843},{\"x\":907,\"y\":841}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":1159,\"y\":818},{\"x\":1177,\"y\":818},{\"x\":1177,\"y\":841},{\"x\":1159,\"y\":840}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":139,\"y\":820},{\"x\":150,\"y\":820},{\"x\":150,\"y\":839},{\"x\":139,\"y\":838}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":569,\"y\":873},{\"x\":716,\"y\":874},{\"x\":716,\"y\":900},{\"x\":569,\"y\":901}],\"text\":\"Feedback,\"},{\"boundingBox\":[{\"x\":721,\"y\":874},{\"x\":734,\"y\":874},{\"x\":734,\"y\":899},{\"x\":721,\"y\":900}],\"text\":\"i\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 07 September 2015\",\"lines\":[{\"boundingBox\":[{\"x\":5,\"y\":15},{\"x\":1065,\"y\":17},{\"x\":1065,\"y\":72},{\"x\":5,\"y\":71}],\"text\":\"Published online: 07 September 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":6,\"y\":16},{\"x\":272,\"y\":17},{\"x\":272,\"y\":71},{\"x\":6,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":290,\"y\":17},{\"x\":494,\"y\":18},{\"x\":494,\"y\":73},{\"x\":290,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":505,\"y\":18},{\"x\":578,\"y\":18},{\"x\":578,\"y\":73},{\"x\":505,\"y\":73}],\"text\":\"07\"},{\"boundingBox\":[{\"x\":597,\"y\":18},{\"x\":906,\"y\":18},{\"x\":905,\"y\":72},{\"x\":596,\"y\":73}],\"text\":\"September\"},{\"boundingBox\":[{\"x\":921,\"y\":18},{\"x\":1057,\"y\":17},{\"x\":1056,\"y\":70},{\"x\":920,\"y\":72}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 0.8949991,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "metadata_storage_path": "aHR0cHM6Ly9zdG9yYWdldWRjZGV2ZWF0dXMwMS5ibG9iLmNvcmUud2luZG93cy5uZXQvcGFwZXJzL0xpYnJhcnkvczQwNTM3LTAxOS0wMTg0LTUucGRm0",
      "metadata_author": "Tu Nguyen Thi Ngoc ",
      "metadata_title": "Mining aspects of customer’s review on the social network",
      "people": [
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Viet Anh Nguyen2",
        "Naive",
        "Nguyen Thi Ngoc",
        "Thi Ngoc",
        "Trung Nguyen",
        "Soft",
        "Hu",
        "Liu",
        "eling",
        "Lin",
        "Pang",
        "Lee",
        "Moghaddam",
        "Ester",
        "Xiaowen Ding",
        "Minqing Hu",
        "Yan",
        "Peñalver-Martinez",
        "Naïve Bayes",
        "Asha",
        "Gini",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "Bayes",
        "fk"
      ],
      "keyphrases": [
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "iveco mmons",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "reviews",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "org",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "main challenge",
        "many reviews",
        "big fan",
        "Thi Ngoc",
        "thorough understanding",
        "general impression",
        "Three tasks",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "two important tasks",
        "many researches work",
        "Different approach",
        "earliest researches",
        "top pot",
        "supervised approach",
        "sophisticated state",
        "Related work",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "frequency threshold",
        "frequency-based approach",
        "classification method",
        "Method” sections",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "aspect identification",
        "aspect terms",
        "regression methods",
        "interesting methods",
        "frequency-based method",
        "words frequency",
        "chocolate",
        "note",
        "milk",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "rule-based approaches",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "item",
        "work",
        "fication",
        "sampling",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "| k",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "R K",
        "K-dimensional vector",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "vector ri",
        "aspect aj",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1,Q",
        "yi",
        "corpus",
        "Sj",
        "Tj",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "frequency",
        "subset",
        "initial aspect core terms",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "new words",
        "aspect weight",
        "Aspect ratings",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "flavor taste",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "mouthfeel",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "Laplace transformation",
        "JJ tags",
        "RB tags",
        "VB tags",
        "class c",
        "one word",
        "first word",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "P(rij",
        "POS tags",
        "class label",
        "second word",
        "rating label",
        "rating rij",
        "third rules",
        "word features",
        "probability rij",
        "feature fk",
        "αij",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "POS labeled rules",
        "JJ NN",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "third word",
        "NNS JJ",
        "Fq",
        "∏q",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method"
      ],
      "merged_content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often",
      "text": [
        "Published online: 28 February 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2019\",\"lines\":[{\"boundingBox\":[{\"x\":6,\"y\":15},{\"x\":1014,\"y\":16},{\"x\":1014,\"y\":72},{\"x\":6,\"y\":70}],\"text\":\"Published online: 28 February 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":6,\"y\":16},{\"x\":270,\"y\":15},{\"x\":270,\"y\":71},{\"x\":6,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":289,\"y\":15},{\"x\":495,\"y\":15},{\"x\":496,\"y\":72},{\"x\":289,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":506,\"y\":15},{\"x\":575,\"y\":16},{\"x\":576,\"y\":72},{\"x\":507,\"y\":72}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":595,\"y\":16},{\"x\":855,\"y\":16},{\"x\":857,\"y\":73},{\"x\":596,\"y\":72}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":871,\"y\":16},{\"x\":1010,\"y\":17},{\"x\":1012,\"y\":73},{\"x\":872,\"y\":73}],\"text\":\"2019\"}]}"
      ]
    }
  ]
}